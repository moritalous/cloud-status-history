{"archive":[{"service_name":"AWS Internet Connectivity (Ohio)","summary":"[RESOLVED] Internet Connectivity","date":"1624564393","status":"1","details":"","description":"<div><span class=\"yellowfg\">12:53 PM PDT</span>&nbsp;Between 12:15 PM and 12:21 PM PDT we observed connectivity issues between some customer networks and the US-EAST-2 Region. Connectivity within the Region was not impacted. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-us-east-2"},{"service_name":"Amazon Elastic Compute Cloud (Ohio)","summary":"[RESOLVED] Network Connectivity Issues","date":"1624992057","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:40 AM PDT</span>&nbsp;Between 11:07 AM and 11:15 AM PDT we experienced a network connectivity issue that impacted Amazon VPC Inter-Region Peering in the US-EAST-2 Region. Connectivity to instances and services within the Region was not impacted by the event. The issue has been resolved and connectivity has been restored. </div>","service":"ec2-us-east-2"},{"service_name":"AWS Certificate Manager (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates and Latency","date":"1624993969","status":"3","details":"","description":"<div><span class=\"yellowfg\">12:12 PM PDT</span>&nbsp;We are investigating increased API latency and error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">12:36 PM PDT</span>&nbsp;We can confirm increased API latency and increased API error rates for the ACM APIs in the US-EAST-1 Region. During this time, you may be unable to Request new certificates, and may also observe errors when attempting to List and/or nodify existing certificates. This issue impacts both the AWS Management Console, and the ACM APIs. Additionally, you may also receive API errors when attempting to associate new resources. Existing associated resources are unaffected, and continue to operate as normal. We have identified the root cause of the issue and are working toward mitigation and resolution. We will provide further updates as we have more information to share.</div><div><span class=\"yellowfg\"> 1:14 PM PDT</span>&nbsp;We continue to work toward mitigating the affected subsystem responsible for the increase in API Errors and Latencies for the ACM APIs. Other AWS Services (such as ClientVPN) who attempt to create or associate new certificates may also be impacted by this issue. Existing resources remain unaffected by this issue and continue to operate normally. </div><div><span class=\"yellowfg\"> 2:13 PM PDT</span>&nbsp;We are continuing to drive to root cause and work toward mitigating the affected subsystem responsible for the increase in API Errors and Latencies for the ACM APIs. Other AWS Services (such as ClientVPN) who attempt to create or associate new certificates may also be impacted by this issue. Existing resources remain unaffected by this issue and continue to operate normally.</div><div><span class=\"yellowfg\"> 2:56 PM PDT</span>&nbsp;We have identified some workloads on the affected subsystem of the ACM API that may be causing the increase in API errors and latency, and we are reviewing and testing procedures to mitigate their impact.  We do not have an ETA at this time.  This issue does affect services like CloudFront and ElasticSearch that rely on ACM for their certificate needs. It would also impact CloudFormation workflows that either directly or indirectly need to manipulate ACM certificates.\n\nAll workflows that depend on ACM certificates that are already created are not impacted by this event, and continue to operate normally.</div><div><span class=\"yellowfg\"> 3:33 PM PDT</span>&nbsp;We continue to work toward mitigating the increased latencies and error rates affecting the ACM APIs. Until this point, some requests and retries have been succeeding. At this time, we are temporarily not accepting additional API requests, in order to help accelerate mitigation and recovery. Once we begin accepting new API requests, requests will be throttled. We will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 4:49 PM PDT</span>&nbsp;We are starting to see some ACM API calls succeed for CloudFront and ELB and we are starting to propagate changes for CloudFront distributions to our edge locations.  Customer facing APIs are still throttled.  ACM is continuing to make progress towards recovery.  </div><div><span class=\"yellowfg\"> 5:47 PM PDT</span>&nbsp;We are seeing recovery for customers and throttling has been removed from most APIs.  We are working through the final changes to unblock the following APIs: RequestCertificate, ListCertificates, and ImportCertificate and expect to have those final changes in-place shortly. We will update as we make progress towards full recovery. </div><div><span class=\"yellowfg\"> 7:05 PM PDT</span>&nbsp;We are seeing recovery for customers and throttling has been removed from most APIs.  We have unblocked RequestCertificate for most use cases and are working to have the final changes in-place shortly. We will update as we make progress towards full recovery.</div><div><span class=\"yellowfg\"> 7:46 PM PDT</span>&nbsp;Between 11:45 AM and 7:42 PM PDT, customers experienced increased ACM API errors and latency in the US-EAST-1 Region that impacted the ability to issue new certificates, import certificates and retrieve information about certificates from ACM. Existing certificates that were already vended to services such as CloudFront and ELB continued to operate and were unaffected. This issue also impacted provisioning and scaling workflows for services that depend on ACM for certificate management needs, such as CloudFront and ELB, as well as CloudFormation operations that involve mutating ACM certificates. This issue was caused by a previously unknown limit in an ACM storage subsystem. We have identified the limit issue and have mitigated it.  The issue has been fully resolved and all ACM API requests are being answered normally.  During this time, all existing resources that had a configured ACM certificate (such as ELB load balancers and CloudFront distributions) continued to operate normally, and were not impaired by this issue.</div>","service":"certificatemanager-us-east-1"},{"service_name":"Amazon CloudFront","summary":"[RESOLVED] Propagation Delays","date":"1625000294","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 1:58 PM PDT</span>&nbsp;We are investigating delays in propagating changes to CloudFront distributions to our edge locations. This is related to the ACM issue in the US-EAST-1 Region that we have posted to the Service Health Dashboard. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 4:08 PM PDT</span>&nbsp;CloudFront distribution changes continue to be affected by the AWS Certificate Manager issue in US-EAST-1. ACM is continuing to make progress towards recovery, but change propagation for CloudFront distribution changes will continue to be affected until the ACM issue is fully mitigated. At that point, we will begin to process the backlog of distribution changes. The backlog of changes may take additional time to complete. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 5:06 PM PDT</span>&nbsp;We are starting to see some ACM API calls succeed for CloudFront and we are starting to propagate changes for CloudFront distributions to our edge locations. ACM is continuing to make progress towards recovery. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 6:11 PM PDT</span>&nbsp;We continue to process the backlog of distribution changes and propagate the updates to our edge locations. Existing distributions continue to operate normally and there is no impact to serving content from our edge locations.</div><div><span class=\"yellowfg\"> 7:38 PM PDT</span>&nbsp;Between 11:45 AM and 7:25 PM PDT, we experienced delays in propagating changes to CloudFront distributions to our edge locations due to the ACM issue in US-EAST-1. This issue has been resolved and the service is operating normally. During this time, previous configured distributions continued to operate without any issues and there were no issues with serving content from our CloudFront edge locations.</div>","service":"cloudfront"},{"service_name":"Amazon Elastic Compute Cloud (Frankfurt)","summary":"[RESOLVED] Instance connectivity","date":"1626179354","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:29 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for the EC2 APIs and connectivity issues for some instances in a single Availability Zone in the EU-CENTRAL-1 Region</div><div><span class=\"yellowfg\"> 6:05 AM PDT</span>&nbsp;We are seeing increased error rates and latencies for the RunInstances and CreateSnapshot APIs, and increased connectivity issues for some instances in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. We have resolved the networking issues that affected the majority of instances within the affected Availability Zone, but continue to work on some instances that are experiencing degraded performance for some EBS volumes. Other Availability Zones are not affected by this issue. We would recommend failing away from the affected Availability Zone until this issue has been resolved.</div><div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We continue to make progress in resolving the connectivity issues affecting some instances in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. The increased error rates and latencies for the RunInstance and CreateSnapshot APIs have been resolved, as well as the degraded performance for some EC2 volumes within the affected Availability Zone. We continue to work on the remaining EC2 instances that are still impaired as a result of this event, some of which may have experienced a power cycle. While we do not expect any further impact at this stage, we would recommend continuing to utilize other Availability Zones in the EU-CENTRAL-1 region until this issue has been resolved.</div><div><span class=\"yellowfg\"> 7:24 AM PDT</span>&nbsp;Starting at 5:07 AM PDT we experienced increase connectivity issues for some instances, degraded performance for some EBS volumes and increased error rates and latencies for the EC2 APIs in a single Availability Zone (euc1-az3) in the EU-CENTRAL-1 Region. By 6:03 AM PDT, API error rates had returned to normal levels, but some Auto Scaling workflows continued to see delays until 6:35 AM PDT. By 6:10 AM PDT, the vast majority of EBS volumes with degraded performance had been resolved as well, and by 7:05 AM PDT, the vast majority of affected instances had been recovered, some of which may have experienced a power cycle. A small number of remaining instances are hosted on hardware which was adversely affected by this event and require additional attention. We continue to work to recover all affected instances and have opened notifications for the remaining impacted customers via the Personal Health Dashboard. For immediate recovery, we recommend replacing any remaining affected instances if possible.</div>","service":"ec2-eu-central-1"},{"service_name":"Amazon CloudWatch (Tokyo)","summary":"[RESOLVED] エラー率およびレイテンシーの上昇 | Increased Error rates and Latencies","date":"1626280770","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 9:39 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの増加を調査しています。現在この問題の解決に取り組んでいます。 | We are investigating increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:04 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける API エラー率の上昇およびログイベントの遅延を確認しています。現在この問題の解決に取り組んでいます。| We can confirm elevated API error rates and some delayed log events in AP-NORTHEAST-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける API エラー率の上昇およびログイベントの遅延の原因を特定しました。復旧の兆候を確認しており、解決に向け引き続き対応を行っています。 | We have identified the root cause of the elevated API error rates and some delayed log events in the AP-NORTHEAST-1 Region. We are beginning to see signs of recovery and continue working towards resolution.</div><div><span class=\"yellowfg\">12:07 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの増加について引き続き調査および解決に取り組んでいます。| We continue to work on investigating and resolving increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region.</div><div><span class=\"yellowfg\"> 2:28 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの上昇を解消すると予測される緩和策の実施を完了しました。エラー率の改善が見られる一方で引き続き高い水準のレイテンシーを確認しており、完全な解決に向け引き続き対応を行っています。 | We have completed a mitigation strategy that we anticipated would resolve the increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. While we have seen some recovery in error rates, we continue to see elevated levels of latency and continue working towards full resolution.</div><div><span class=\"yellowfg\"> 3:42 PM PDT</span>&nbsp;AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のレイテンシー上昇と残存しているエラーの解消のため、第二の緩和策の展開を行なっております。解決に向け引き続き対応を行っております。 | We are deploying a second mitigation strategy to resolve elevated latency and the remaining level of errors for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. We continue working towards recovery.</div><div><span class=\"yellowfg\"> 5:05 PM PDT</span>&nbsp;日本時間 2021/07/14 23:01 から 2021/07/15 08:20 の間、AP-NORTHEAST-1 リージョンにおける CloudWatch Logs API のエラー率とレイテンシーの上昇が発生しておりました。この問題は解決し、現在サービスは正常に動作しています。 | Between 7:01 AM and 4:20 PM PDT we experienced increased error rates and latencies for CloudWatch Logs APIs in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"cloudwatch-ap-northeast-1"},{"service_name":"Amazon Elastic Compute Cloud (Milan)","summary":"[RESOLVED] Increased Error rates","date":"1626910891","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 4:41 PM PDT</span>&nbsp;We are investigating increased API error rates for the RunInstances API in the EU-SOUTH-1 Region. </div><div><span class=\"yellowfg\"> 5:04 PM PDT</span>&nbsp;We can confirm increased API error rates for the RunInstances API in the EU-SOUTH-1 Region. This is also affecting services that depend on EC2 such as Auto Scaling, and launches of service instances that are built on EC2, such as RDS and ElastiCache. Instances that are already launched are operating normally. We have identified the root cause and are actively testing a mitigation plan. We expect to have an update on the success of this mitigation effort in the next 30 minutes.</div><div><span class=\"yellowfg\"> 5:19 PM PDT</span>&nbsp;Between 3:59 PM and 5:07 PM PDT customers experienced increased error rates for the EC2 RunInstances API in the EU-SOUTH-1 Region. This is also affected services that depend on EC2 such as Auto Scaling, and launches of service instances that are built on EC2, such as RDS and ElastiCache. The issue has been resolved and the RunInstances API is now operating normally.  This issue only affected new instance launches, instances that were already running were not affected. </div>","service":"ec2-eu-south-1"},{"service_name":"AWS Lambda (Milan)","summary":"[RESOLVED] Increased Invoke Error Rate","date":"1628028846","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 3:14 PM PDT</span>&nbsp;We are investigating increased invoke error rates in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 3:45 PM PDT</span>&nbsp;Between 2:39 PM and 3:16 PM PDT, we experienced increased invoke error rates in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"lambda-eu-south-1"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Increased Management Console Error Rates","date":"1628029136","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 3:18 PM PDT</span>&nbsp;We are investigating increased errors for AWS Management Console in the EU-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 3:46 PM PDT</span>&nbsp;Between 2:38 PM and 3:16 PM PDT, we experienced increased errors for AWS Management Console in the EU-SOUTH-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"management-console"},{"service_name":"Amazon ElastiCache (Ireland)","summary":"[RESOLVED] Increased Error Rates","date":"1628216388","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:19 PM PDT</span>&nbsp;We are experiencing increased latencies while provisioning new ElastiCache nodes and elevated API error rates in the EU-WEST-1 Region. Existing ElastiCache clusters are not impacted and are continuing to serve traffic. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;Between 6:22 PM and 7:31 PM PDT, Amazon ElastiCache experienced increased latencies while provisioning new ElastiCache nodes and elevated API error rates in the EU-WEST-1 Region. Existing ElastiCache clusters were not impacted and continued to serve traffic. The issue has been resolved and the service is operating normally.</div>","service":"elasticache-eu-west-1"},{"service_name":"Amazon DynamoDB (N. Virginia)","summary":"[RESOLVED] Increased error rates","date":"1628298493","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 6:08 PM PDT</span>&nbsp;We are investigating increased error rate and latencies for DynamoDB in US-EAST-1 region.</div><div><span class=\"yellowfg\"> 6:32 PM PDT</span>&nbsp;We can confirm increased error rates for DynamoDB in the US-EAST-1 Region. We have identified the root cause of the issue and are working towards resolution.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have seen some improvement to the error rates for DynamoDB in the US-EAST-1 Region and continue to work towards full resolution. For customers that are experiencing 503 errors, retries may resolve the issue in some cases. In other cases, recreating the connection to DynamoDB may address the error rates. We continue to take steps towards full resolution for all affected tables.</div><div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;We continue to make progress in addressing the increased error rates for DynamoDB in the US-EAST-1 Region. The root cause of the issue is a problem with the metadata subsystem used by DynamoDB, where several nodes are in an unhealthy state. We continue to work towards restoring the health of these nodes. The issue affected a subset of DynamoDB tables that are associated with the unhealthy nodes in the metadata subsystem. For these tables, customers will experience increased error rates until we have resolved the issue. DynamoDB tables that are not associated with the affected metadata nodes, are not affected by this issue. We continue to work towards full resolution.</div><div><span class=\"yellowfg\"> 9:00 PM PDT</span>&nbsp;We continue to see an improvement in the error rates for affected DynamoDB tables in the US-EAST-1 Region. Since the start of the event, we have seen a 75% reduction in error rates and are now working on resolving the errors for the remaining DynamoDB tables. </div><div><span class=\"yellowfg\">10:20 PM PDT</span>&nbsp;We have resolved the error rates for the majority of the affected DynamoDB tables and now have a small number of DynamoDB tables that are still experiencing error rates and a small number of global secondary indexes that are experiencing propagation delays. While all the nodes in the metadata store are now healthy, some are not yet able to process incoming requests, which we are working to resolve.</div><div><span class=\"yellowfg\">11:11 PM PDT</span>&nbsp;We have now resolved the error rates affecting DynamoDB tables in the US-EAST-1 Region. A small number of DynamoDB tables continue to experience delayed propagation for global secondary indexes, but these are moving towards full recovery as well. We’re continuing to monitor the service, but customers should be seeing recovery for their DynamoDB tables at this stage.</div><div><span class=\"yellowfg\">Aug 7,  1:50 AM PDT</span>&nbsp;We have now resolved the Global Secondary index propagation delays for most customers in the US-EAST-1 region. Our mitigation efforts are working as expected and we continue to work towards full recovery.</div><div><span class=\"yellowfg\">Aug 7,  2:53 AM PDT</span>&nbsp;Between August 6 5:23 PM and August 7 2:48 AM PDT, DynamoDB customers experienced API errors and delayed propagation for global secondary indexes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally. </div>","service":"dynamodb-us-east-1"},{"service_name":"Amazon Elastic Kubernetes Service (N. Virginia)","summary":"[RESOLVED] Increased error rates","date":"1628306280","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:18 PM PDT</span>&nbsp;We are investigating increased error rates for Cluster create and update operations in the US-EAST-1 Region. Some existing clusters are also impacted.</div><div><span class=\"yellowfg\"> 9:30 PM PDT</span>&nbsp;We are continuing to investigate increased error rates for Cluster create and update operations in the US-EAST-1 Region. We are seeing recovery for some clusters, and continue to work on recovery for all clusters.</div><div><span class=\"yellowfg\">10:34 PM PDT</span>&nbsp;We can confirm errors creating and updating Clusters in the US-EAST-1 Region, and are continuing to work towards resolution. We are starting to see recovery for existing clusters, and continue to work on recovery for all clusters.</div><div><span class=\"yellowfg\">11:33 PM PDT</span>&nbsp;We continue to see improvement in the error rates for creating and upgrading clusters in the US-EAST-1 Region. We are starting to see recovery in availability of existing clusters. We are continuing to work towards a full resolution.</div><div><span class=\"yellowfg\">Aug 7, 12:27 AM PDT</span>&nbsp;Between August 6 5:50 PM PDT and 11:27 PM PDT, we experienced errors for creating and upgrading clusters in the US-EAST-1 Region. Some existing clusters were also impacted during this event. The issue has been resolved and the service is operating normally.</div>","service":"eks-us-east-1"},{"service_name":"Amazon Elastic Load Balancing (N. Virginia)","summary":"[RESOLVED] Increased error rates","date":"1628306461","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:21 PM PDT</span>&nbsp;We are investigating increased provisioning latencies for new load balancers, delayed propagation of load balancer CloudWatch metrics, and inconsistent health check statuses from the DescribeTargetHealth API for Network Load Balancers in the US-EAST-1 Region.</div><div><span class=\"yellowfg\">10:08 PM PDT</span>&nbsp;We can confirm delayed propagation of load balancer CloudWatch metrics and provisioning latencies for Network Load Balancers in the US-EAST-1 Region, and are continuing to work towards resolution. Connectivity to existing load balancers is not affected.</div><div><span class=\"yellowfg\">11:22 PM PDT</span>&nbsp;Between 6:08 PM and 11:12 PM PDT, we experienced increased provisioning latencies for Network Load Balancers in the US-EAST-1 Region, which has now fully recovered. We are continuing to experience delays in the propagation of CloudWatch metrics for load balancers. The system used to generate these metrics has begun to recover, and we anticipate full recovery in the next few hours after we process the backlog of metrics.</div>","service":"elb-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] DescribeImages API","date":"1628699197","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 9:26 AM PDT</span>&nbsp;We are investigating an issue where the ‘virtualization type’ field is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. The virtualization type for some images is being returned as “paravirt” instead of “hvm”. This can impact the instance selection in the console and also cause some EMR jobs to fail to start as EMR does not support “paravirt” images. We have identified the root cause of the issue and are working to resolve it. Note that the underlying images (AMI) are not affected by this issue, which has only affected the returning of the metadata for the ‘virtualization type’ field. Once resolved, images will continue to operate as they did before. </div><div><span class=\"yellowfg\">10:25 AM PDT</span>&nbsp;We continue to make progress in resolving the issue where the virtualization type is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. The issue will prevent some images from launching on the current generation (e.g. c5, m5, r5, t3, etc.) instance types and some previous generation instances (e.g t2) as they do not support “paravirt” virtualization. Customers could choose to launch on a previous generation instance type that does support ‘paravirt’ virtualization, or attempt launching via the EC2 command line tools, which in many cases when allow for the instance to be launched. We continue to work on resolving the issue.</div><div><span class=\"yellowfg\">11:01 AM PDT</span>&nbsp;We have begun to see recovery for some of the affected images (AMI), which are now returning the correct virtualization type. We expect to see full recovery within the next hour, at which time all operations affected by this issue should be operating normally. We will provide an update once we have fully recovered.</div><div><span class=\"yellowfg\">11:24 AM PDT</span>&nbsp;We have resolved the issue where the virtualization type is incorrectly set in the DescribeImages API call for the EC2 APIs in the US-EAST-1 Region. Starting at 2:43 AM PDT, some images (AMI) returned the incorrect virtualization type (‘paravirt’ instead of ‘hvm’) in a DescribeImages API call. Since some instance types do not support ‘paravirt’ images, this prevented the EC2 Management Console and some EMR jobs from being able to launch new EC2 instances. As of 11:17 AM PDT, the issue has been fully resolved. Instances launched during this time do not need to be relaunched. The issue has been resolved and the service is operating normally.\n</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Container Service (N. Virginia)","summary":"[RESOLVED] Elevated API Error Rates","date":"1629201355","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 4:55 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:24 AM PDT</span>&nbsp;We can confirm increased API error rates for ECS in the US-EAST-1 Region. We have identified the issue and are working towards mitigation. In most cases, a retry of the API operation will succeed.</div><div><span class=\"yellowfg\"> 6:15 AM PDT</span>&nbsp;We can confirm increased API error rates for ECS in the US-EAST-1 Region. We have identified the root cause and continue to work towards recovery. In most cases, a retry of the API operation will succeed.\n</div><div><span class=\"yellowfg\"> 7:12 AM PDT</span>&nbsp;Between 3:40 AM and 6:45 AM PDT we experienced increased API error rates for ECS in the US-EAST-1 Region. In most cases, API calls succeeded when retried.  We continue to work to recover residual impact and have opened notifications for the remaining impacted customers via the Personal Health Dashboard.</div>","service":"ecs-us-east-1"},{"service_name":"Amazon EventBridge (N. Virginia)","summary":"[RESOLVED] Elevated Event Delivery Latency","date":"1629229180","status":"1","details":"","description":"<div><span class=\"yellowfg\">12:39 PM PDT</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the US-EAST-1 Region. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency. </div><div><span class=\"yellowfg\"> 1:53 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We have taken initial mitigation actions, which have reduced the delivery latency for some requests, and continue to work toward full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We are seeing further reduction of the delivery latency for some requests. We are exploring further mitigation steps as we continue to work toward full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 4:39 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We are seeing further reduction of the delivery latency for more requests. We have identified the component that is contributing to the event delivery latency and are mitigating impact within this component as we work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 5:36 PM PDT</span>&nbsp;We have identified the root cause of the issue resulting in elevated event delivery latency for EventBridge events in the US-EAST-1 Region. We continue to observe further reduction of the delivery latency for more requests. We have completed additional mitigation steps that have led to the reduction in delivery latency and continue to work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 6:17 PM PDT</span>&nbsp;We continue to take mitigation actions to improve elevated event delivery latency for EventBridge events in the US-EAST-1 Region. While we observe improvement in event delivery latency, we are still experiencing elevated latency in the component responsible for matching events and continue to work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 7:07 PM PDT</span>&nbsp;We previously took steps towards mitigating event delivery latency based upon the suspected root cause and observed improvements in event delivery latency in the US-EAST-1 Region. We have now identified that we are continuing to see event delivery latency due to the component responsible for matching events with EventBridge rules and are taking further steps to mitigate the issue as we work towards full resolution. Other AWS Services that make use of EventBridge may also observe elevated event delivery latency.</div><div><span class=\"yellowfg\"> 8:06 PM PDT</span>&nbsp;We have completed our actions towards mitigating event delivery latency for EventBridge events in the US-EAST-1 Region. Event delivery latency for new events has fully recovered and we are processing the backlog of previously published events. Other AWS Services that make use of EventBridge are observing improvements in elevated event delivery latency as well.\n</div><div><span class=\"yellowfg\"> 9:05 PM PDT</span>&nbsp;Between 8:31 AM and 9:00 PM PDT, we experienced elevated event delivery latency for EventBridge events in the US-EAST-1 Region. Other AWS services that make use of EventBridge are no longer experiencing elevated event delivery latency. The issue has been resolved and the service is operating normally. </div>","service":"events-us-east-1"},{"service_name":"AWS Identity and Access Management","summary":"[RESOLVED] IAM errors and propagation latency","date":"1629996723","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 9:52 AM PDT</span>&nbsp;We are investigating elevated latencies and errors on the IAM APIs. In addition, we are investigating propagation delays for recently created or recently updated IAM users, credentials, roles, policies. Authentication and authorization of existing users, credentials, roles, policies are not impacted. Other AWS services like AWS CloudFormation that use IAM roles were also impacted.</div><div><span class=\"yellowfg\">10:39 AM PDT</span>&nbsp;Between 6:44 AM and 9:12 AM PDT, customers experienced elevated latency and error rates in response to IAM API requests, as well as delays in describing recently created or modified IAM resources. In addition, between 6:44 AM and 10:02 AM, propagation of IAM API updates was delayed in the ME-SOUTH-1, EU-SOUTH-1, AP-EAST-1, and AF-SOUTH-1 regions, and newly created or recently updated IAM users, credentials, roles, and policies may not have been available for authentication and authorization in those regions. Other AWS Services that rely on IAM changes to provision resources, such as CloudFormation, were also impacted. Authentication and authorization for existing users, credentials, roles, policies were not impacted. The issue has been resolved and the service is operating normally.</div>","service":"iam"},{"service_name":"AWS Internet Connectivity (Oregon)","summary":"[RESOLVED] Connectivity Issues","date":"1630434320","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:25 AM PDT</span>&nbsp;We are investigating an issue which is affecting network traffic for some customers using AWS services in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. While we continue to work towards root cause, we believe that the issue is affecting connectivity to Network Load Balancers from EC2 instances, connectivity from Lambda to EC2 instances and other AWS services, as well as connectivity between EC2 and some AWS services using PrivateLink. In an effort to further mitigate the impact, we are shifting some services and network flows away from the affected Availability Zone to mitigate the impact.</div><div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. We have narrowed down the issue to an increase in packet loss within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services. The issue continues to only affect the single Availability Zone (usw2-az2) within the US-WEST-2 region, so shifting traffic away from Networking Load Balancer and NAT Gateway within the affected Availability Zone can mitigate the impact. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, are seeing impact as a result of this issue. </div><div><span class=\"yellowfg\"> 2:26 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region and are actively working on mitigation. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in usw2-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in usw2-az2.</div><div><span class=\"yellowfg\"> 3:23 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:02 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. Beginning at 10:58 AM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the US-WEST-2 Region. At 2:45 PM, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:35 PM, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-us-west-2"},{"service_name":"Amazon Elastic Load Balancing (Oregon)","summary":"[RESOLVED] Connectivity Issues","date":"1630435612","status":"2","details":"","description":"<div><span class=\"yellowfg\">11:46 AM PDT</span>&nbsp;We are investigating connectivity, provisioning, and target registration issues for load balancers in the US-WEST-2 Region.</div><div><span class=\"yellowfg\">12:10 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. While we continue to work towards root cause, we believe that the issue is affecting connectivity to Network Load Balancers from EC2 instances, connectivity from Lambda to EC2 instances and other AWS services, as well as connectivity between EC2 and some AWS services using PrivateLink. In an effort to further mitigate the impact, we are shifting some services and network flows away from the affected Availability Zone to mitigate the impact.</div><div><span class=\"yellowfg\"> 1:00 PM PDT</span>&nbsp;We continue to investigate the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. We have narrowed down the issue to an increase in packet loss within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services. The issue continues to only affect the single Availability Zone (usw2-az2) within the US-WEST-2 region, so shifting traffic away from Networking Load Balancer and NAT Gateway within the affected Availability Zone can mitigate the impact. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, are seeing impact as a result of this issue. </div><div><span class=\"yellowfg\"> 2:24 PM PDT</span>&nbsp;We have identified the root cause of the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region and are actively working on mitigation. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in usw2-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in usw2-az2. </div><div><span class=\"yellowfg\"> 3:22 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:02 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (usw2-az2) in the US-WEST-2 Region. Beginning at 10:58 AM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the US-WEST-2 Region. At 2:45 PM, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:35 PM, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>","service":"elb-us-west-2"},{"service_name":"Amazon Elastic Load Balancing (Ireland)","summary":"[RESOLVED] Connectivity Issues","date":"1630445941","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 2:39 PM PDT</span>&nbsp;We are investigating connectivity issues for load balancers in a single Availability Zone (euw1-az2) in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 2:58 PM PDT</span>&nbsp;We can confirm network connectivity issues affecting a single Availability Zone (euw1-az2) in the EU-WEST-1 Region and are actively working on mitigation. Some other AWS services, including Lambda, ELB, Kinesis, SQS, RDS, CloudWatch and ECS, may also see impact as a result of this issue. A component within the subsystem responsible for the processing of network packets for Network Load Balancer, NAT Gateway and PrivateLink services became impaired and was no longer processing health checks successfully. This resulted in other components no longer accepting new connection requests, as well as elevated packet loss for Network Load Balancer, NAT Gateway and PrivateLink endpoints. For immediate mitigation for NLB, customers should (1) disable ‘cross zone load balancing’ on Network Load Balancer, and then (2) deregister any targets that are in euw1-az2. For NAT Gateway/PrivateLink, you may modify your route tables to direct traffic to NAT Gateways in other Availability Zones or you may disable PrivateLink endpoints in euw1-az2.</div><div><span class=\"yellowfg\"> 3:35 PM PDT</span>&nbsp;We are beginning to see signs of recovery, and continue to work toward full resolution. </div><div><span class=\"yellowfg\"> 4:04 PM PDT</span>&nbsp;We have resolved the issue affecting network connectivity within a single Availability Zone (euw1-az2) in the EU-WEST-1 Region. Beginning at 2:19 PM PDT, we experienced network connectivity issues for Network Load Balancer, NAT Gateway and PrivateLink endpoints within the EU-WEST-1 Region. At 3:25 PM PDT, some Network Load Balancers, NAT Gateways and PrivateLink endpoints began to see recovery and by 3:40 PM PDT, all affected Network Load Balancers, NAT Gateways and PrivateLink endpoints had fully recovered. The issue has been resolved and the service is operating normally.</div>","service":"elb-eu-west-1"},{"service_name":"AWS Direct Connect (Tokyo)","summary":"[RESOLVED] ネットワーク接続性 | Network Connectivity","date":"1630543140","status":"3","details":"","description":"<div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;日本時間 2021/09/02 07:30 から一部の AWS Direct Connect 接続と AP-NORTHEAST-1 リージョン間にネットワーク接続性の問題が発生していることを確認しております。この問題について調査を行っております。| Starting at 3:30 PM PDT, we began to experience network connectivity issues, impacting AWS Direct Connect connectivity between some AWS Direct Connections and the AP-NORTHEAST-1 Region. We are actively investigating the issue. </div><div><span class=\"yellowfg\"> 6:02 PM PDT</span>&nbsp;一部の AWS Direct Connect 接続と AP-NORTHEAST-1 リージョン間にネットワーク接続性の問題について追加の情報をご案内いたします。日本時間 2021/09/02 07:30 からコアネットワークデバイスに複数の問題が発生していることを確認しております。現在、問題が発生したデバイスについて復旧を進めており、デバイスがオンラインの状態に戻ることで接続性の問題が解消することが期待されます。現状では復旧の目途に関する情報はございません。進展がございましたら、随時更新致します。| We wanted to provide some more information for the event affecting some Direct Connect network connectivity in the AP-NORTHEAST-1 Region. Starting at 3:30 PM PDT, we began to experience network connectivity issues due to some failures in core networking devices. We are currently working on restoring these devices and we expect some restoration of connectivity as these devices come back online. We currently do not have an ETA on full recovery and will update further as information comes to hand. </div><div><span class=\"yellowfg\"> 6:43 PM PDT</span>&nbsp;現在引き続き故障したデバイスの復旧を試みており、完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。| We are still trying to recover the failed devices and do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover are recommended to do so to achieve recovery.</div><div><span class=\"yellowfg\"> 7:33 PM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョン内の故障したデバイスの復旧に取り組んでおりますが、現時点において完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。| We are continuing to work on recovering a number of failed devices within the AP-NORTHEAST-1 Region, but do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover to VPN are recommended to do so to achieve recovery.</div><div><span class=\"yellowfg\"> 8:20 PM PDT</span>&nbsp;現在 AP-NORTHEAST-1 リージョン内の故障したデバイスの復旧に取り組んでおりますが、現時点において完全な復旧の目途に関する情報はございません。今回の問題によりサイト間 VPN の接続性への影響はなく、VPN へのフェイルオーバーのオプションがあるお客様に関しては、VPN にフェイルオーバーいただくことをお勧めいたします。Direct Connect Gateway と Transit Gateway をご利用のお客様に関しては、AWS Site-to-Site VPN をご作成いただき Transit Gateway にアタッチしてご利用いただくことをお勧めいたします。こちらの VPN へのフェイルオーバーの設定手順に関しては次の記事をご参照ください: <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/\">https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/</a> | We are continuing to work on recovering a number of failed devices within the AP-NORTHEAST-1 Region, but do not have an ETA on full recovery. VPN connectivity is not impacted by this failure, and those customers that have that option available for failover to VPN are recommended to do so to achieve recovery. For customers using Direct Connect gateway and Transit Gateway, we recommend creating an AWS Site-to-Site VPN and attach it to your Transit Gateway. Instructions for how to do this failover can be found here: <a href=\"https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/\">https://aws.amazon.com/premiumsupport/knowledge-center/dx-configure-dx-and-vpn-failover-tgw/</a></div><div><span class=\"yellowfg\"> 9:06 PM PDT</span>&nbsp;復旧の兆しが確認できておりますが、引き続き事象の完全な解消に取り組んでおります。VPN を使用するワークアラウンドを実施いただいているお客様につきましては、完全な復旧のご連絡まではワークアラウンドを継続してご利用いただくことをお勧めいたします。| We are beginning to see signs of recovery, and continue to work toward full resolution. We suggest that customers that may have implemented the suggested workaround via VPN continue to use this workaround until we advise of full recovery.</div><div><span class=\"yellowfg\"> 9:51 PM PDT</span>&nbsp;日本時間 2021/09/02 07:30 から 13:42 の間、Direct Connect 接続を利用した AP-NORTHEAST-1 リージョン内の AWS サービスへの通信においてパケットロスの増加が発生しました。今回の事象は、 Direct Connect を利用したネットワークトラフィックを AP-NORTHEAST-1 リージョン内の全てのアベイラビリティーゾーンに接続するのに使用される複数のコアネットワークデバイスの問題に起因しておりました。現在問題は解消し、サービスは正常に稼働しています。\n| Between 3:30 PM and 9:42 PM PDT we experienced elevated packet loss for customers connecting to AWS services within AP-NORTHEAST-1 Region through their Direct Connect connections. This was caused by the loss of serveral core networking devices that are used to connect Direct Connect network traffic to all Availability Zones in the AP-NORTHEAST-1 Region. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">Sep 6,  8:25 PM PDT</span>&nbsp;日本時間 9月2日 の Direct Connect のイベントに関する詳細情報を提供いたします。詳細については、以下をご参照ください。ご質問がある場合は、AWS サポートにお問い合わせください。<a href=\"https://aws.amazon.com/message/17908\">https://aws.amazon.com/message/17908</a> | We'd like to share more information about the Direct Connect event on Wednesday September 2nd. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/17908\">https://aws.amazon.com/message/17908</a></div>","service":"directconnect-ap-northeast-1"},{"service_name":"Amazon Elastic Compute Cloud (Hong Kong)","summary":"[RESOLVED] API错误率的上升 | API的錯誤率上升 | Increased API error rates","date":"1631281173","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 6:39 AM PDT</span>&nbsp;我们正在调查AP-EAST-1地区EC2 API错误率的上升，若有新的进展时会立即更新。| 我們正在調查AP-EAST-1地區EC2 API的錯誤率上升，若有新的進展時會立即更新。| We are investigating increased EC2 API error rates in the AP-EAST-1 Region. We will provide more information as it becomes available.</div><div><span class=\"yellowfg\"> 7:30 AM PDT</span>&nbsp;在 20:23 至 21:56 (UTC+8) 之间，我们在 AP-EAST-1 区域遇到了 API 和新实例启动错误率增加的情况。发生原因是子系统中的少数主机无法处理 API 请求。 在多数情况下，重试调用 API 之后可成功执行。 目前问题已解决，服务已恢复正常运行。| 在 20:23 至 21:56 (UTC+8) 之間，我們在 AP-EAST-1 區域遇到了 API 和新實例啟動錯誤率增加的情況。發生原因是子系統中的少數主機無法處理 API 請求。 在多數情況下，重試調用 API 之後可成功執行。 目前問題已解決，服務已恢復正常運行。| Between 5:23 AM and 6:56 AM PDT we experienced increased error rates for APIs and new instance launches in the AP-EAST-1 Region. The root cause was a small number of hosts in a subsystem that failed to process API requests. Retries of API calls would have succeeded in many cases. The issue has been resolved and the service is operating normally.</div>","service":"ec2-ap-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Degraded EBS Volume Performance","date":"1632712292","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 8:11 PM PDT</span>&nbsp;We are investigating degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. Some new EC2 instance launches, within the affected Availability Zone, are also impacted by this issue. We are working to resolve the issue.</div><div><span class=\"yellowfg\"> 8:41 PM PDT</span>&nbsp;We can confirm degraded performance for some EBS volumes within a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. Existing EC2 instances within the affected Availability Zone that use EBS volumes may also experience impairment due to stuck IO to the attached EBS volume(s). Newly launched EC2 instances within the affected Availability Zone may fail to launch due to the degraded volume performance. We continue to work toward determining root cause and mitigating impact but recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:17 PM PDT</span>&nbsp;We are making progress in determining the root cause and have isolated it to a subsystem within the EBS service. We are working through multiple steps to mitigate the issue and will continue to provide updates as we make progress. Other Availability Zones remain unaffected by this issue and affected EBS volumes and EC2 instances within the affected Availability Zone have plateaued at this stage. We continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\"> 9:47 PM PDT</span>&nbsp;We continue to make progress in determining the root cause of the issue causing degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. A subsystem within the larger EBS service that is responsible for coordinating storage hosts is currently degraded due to increased resource contention. We continue to work to understand the root cause of the elevated resource contention, but are actively working to mitigate the issue. Once mitigated, we expect performance for the affected EBS volumes to return to normal levels. We will continue to provide you with updates on our progress. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">10:23 PM PDT</span>&nbsp;We continue to make progress in determining the root cause of the issue causing degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. We have made several changes to address the increased resource contention within the subsystem responsible for coordinating storage hosts with the EBS service. While these changes have led to some improvement, we have not yet seen full recovery for the affected EBS volumes. We continue to expect full recovery of the affected EBS volumes once the subsystem issue has been addressed. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">11:19 PM PDT</span>&nbsp;We continue to work to address the increased resource contention within the EBS service subsystem responsible for coordinating EBS storage hosts. We have applied several mitigations, and while we have seen some improvements, we have not yet seen performance for affected volumes return to normal levels. We are currently rolling out another mitigation that we believe addresses the root cause and has shown promising signs in testing. We will know within the next 30-45 minutes as to whether this restores normal operations for current affected volumes. For immediate recovery, we continue to recommend that you fail out of the affected Availability Zone (USE1-AZ2) if you are able to do so.</div><div><span class=\"yellowfg\">11:43 PM PDT</span>&nbsp;We can confirm that the deployed mitigation has worked and we have started to see recovery for some affected EBS volumes within the affected Availability Zone (USE1-AZ2). We are still finishing the deployment of the mitigation, but expect performance of affected EBS volumes in this single Availability Zone to return to normal levels over the next 60 minutes.</div><div><span class=\"yellowfg\">Sep 27, 12:30 AM PDT</span>&nbsp;We can confirm that the performance has returned to normal levels for the majority of the affected EBS volumes within the affected Availability Zone (USE1-AZ2). We continue to work towards full recovery for the remaining EBS volumes.</div><div><span class=\"yellowfg\">Sep 27,  1:15 AM PDT</span>&nbsp;We can confirm that the performance has returned to normal levels for the majority of the affected EBS volumes within the affected Availability Zone (USE1-AZ2). Starting at 12:12 AM PDT, we saw recovery slow down some affected EBS volumes as well as some degraded performance for a small number of additional volumes in the affected Availability Zone. We have investigated the root cause and mitigations are underway to complete the performance recovery of the affected EBS volumes.  We continue to work towards full resolution for all affected EBS volumes. In some cases, customers may be experiencing volume state transition delays, which we expect to clear up once volumes have fully recovered.</div><div><span class=\"yellowfg\">Sep 27,  2:26 AM PDT</span>&nbsp;We continue to apply mitigations to address the degraded performance for the smaller set of remaining EBS volumes affected by this issue. While the vast majority of affected EBS volumes were operating normally by 12:05 AM PDT, we have been working to recover a smaller set of affected volumes since 12:12 AM PDT. We continue to make progress on restoring performance for affected volumes, and expect full recovery to take another 2 hours. In some cases, customers may also be experiencing volume state transition delays, which will resolve when the underlying volume has fully recovered.</div><div><span class=\"yellowfg\">Sep 27,  3:36 AM PDT</span>&nbsp;We had restored performance for the vast majority of affected EBS volumes within the affected Availability Zone in the US-EAST-1 Region at 12:05 AM PDT and have been working to restore a remaining smaller set of EBS volumes. EC2 instances affected by this issue have now also recovered and new EC2 instance launches with attached EBS volumes have been succeeding since 1:30 AM PDT. Other services - including Redshift, OpenSearch, and Elasticache - are seeing recovery. Some RDS databases are still experiencing connectivity issues, but we’re working towards full recovery. We are in the process of restoring performance for the remaining small number of EBS volumes and EC2 instances that are still affected by this issue. </div><div><span class=\"yellowfg\">Sep 27,  4:21 AM PDT</span>&nbsp;Starting at 6:41 PM PDT on September 26th, we experienced degraded performance for some EBS volumes in a single Availability Zone (USE1-AZ2) in the US-EAST-1 Region. The issue was caused by increased resource contention within the EBS subsystem responsible for coordinating EBS storage hosts. Engineering worked to identify the root cause and resolve the issue within the affected subsystem. At 11:20 PM PDT, after deploying an update to the affected subsystem, IO performance for the affected EBS volumes began to return to normal levels. By 12:05 AM on September 27th, IO performance for the vast majority of affected EBS volumes in the USE1-AZ2 Availability Zone were operating normally. However, starting at 12:12 AM PDT, we saw recovery slow down for a smaller set of affected EBS volumes as well as seeing degraded performance for a small number of additional volumes in the USE1-AZ2 Availability Zone. Engineering investigated the root cause and put in place mitigations to restore performance for the smaller set of remaining affected EBS volumes. These mitigations slowly improved the performance for the remaining smaller set of affected EBS volumes, with full operations restored by 3:45 AM PDT. While almost all of EBS volumes have fully recovered, we continue to work on recovering a remaining small set of EBS volumes. We will communicate the recovery status of these volumes via the Personal Health Dashboard. While the majority of affected services have fully recovered, we continue to recover some services, including RDS databases and Elasticache clusters. We will also communicate the recovery status of these services via the Personal Health Dashboard. The issue has been fully resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Chime","summary":"[RESOLVED] Increased contact search failures","date":"1632832788","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:39 AM PDT</span>&nbsp;We are investigating increased latency in being able to search for contacts and start chime messages. We will provide more information as it becomes available</div><div><span class=\"yellowfg\"> 6:31 AM PDT</span>&nbsp;Between 2:55 AM and 6:10 AM PDT, we experienced increased error rates when searching for contacts and starting Chime messages. The issue is resolved and the service is operating normally.  </div>","service":"chime"},{"service_name":"AWS Lambda (Ireland)","summary":"[RESOLVED] Increased invoke timeouts","date":"1633447955","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:32 AM PDT</span>&nbsp;We are investigating increased invoke timeout rates for AWS Lambda functions in the EU-WEST-1 Region.</div><div><span class=\"yellowfg\"> 9:15 AM PDT</span>&nbsp;We are still investigating increased invoke timeout rates for AWS Lambda functions in the EU-WEST-1 Region. A subset of functions making API calls to other AWS services within the region are affected.</div><div><span class=\"yellowfg\">10:04 AM PDT</span>&nbsp;We are still investigating increased invoke timeout rates and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. We have isolated the timeouts to network connectivity issues in a section of the AWS Lambda compute subsystem in the region, and are working through resolution and root cause. The issue affects a subset of functions making API calls to other AWS services within the region.</div><div><span class=\"yellowfg\">10:43 AM PDT</span>&nbsp;We can confirm increased invoke timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. The issue affects a subset of functions making API calls to other AWS services within the region. There are no identified workarounds for affected Lambda functions at this time. We continue to investigate a network connectivity issue in a section of the AWS Lambda compute subsystem. We will formulate a mitigation plan based on these findings as we work towards narrowing root cause and work towards resolution.</div><div><span class=\"yellowfg\">11:49 AM PDT</span>&nbsp;We continue to experience increased invoke timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. The issue affects a subset of functions making API calls to other AWS services within the region. At this point we have not identified any workarounds for affected Lambda functions, and we continue to investigate possible networking issues in a section of the AWS Lambda compute subsystem. We will make more information about our mitigation plan available as soon as we have more information to share.</div><div><span class=\"yellowfg\"> 2:03 PM PDT</span>&nbsp;We continue to investigate our subsystems to identify the root cause of timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. Impairment is limited to a small proportion of non-VPC Lambda functions that make outbound network connections. We will update information about workarounds and mitigation plans once we have more information to share.</div><div><span class=\"yellowfg\"> 3:12 PM PDT</span>&nbsp;Between October 4 9:46 AM and October 5 2:47 PM PDT, we experienced timeouts and elevated latencies for a small proportion of AWS Lambda functions in the EU-WEST-1 Region. Impairments were limited to a small proportion of non-VPC Lambda functions that made outbound network connections during this time. The issue has been resolved and the service is operating normally.   </div>","service":"lambda-eu-west-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Network Connectivity Issue","date":"1633484779","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 6:46 PM PDT</span>&nbsp;We are investigating network connectivity for some instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have identified the root cause and are working to restore connectivity for the affected instances. Some EBS volumes are also experiencing degraded performance due to this issue.</div><div><span class=\"yellowfg\"> 7:13 PM PDT</span>&nbsp;We continue to work to mitigate the issue affecting connectivity to some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is a loss of power to a small number of EC2 instances and some networking devices within the affected Availability Zone. A small number of EC2 instances are also experiencing network connectivity issue as a result of the loss of power to the affected networking devices. We are working to restore power to the affected instances. If you are able to launch new EC2 instances, all APIs are operating normally with in the affected Availability Zone. We do not expect the issue to affect any other power line-ups within the affected Availability Zone or other Availability Zones within the Region.</div><div><span class=\"yellowfg\"> 7:34 PM PDT</span>&nbsp;We have restored power to the affected line-up and are seeing recovery for the majority of the affected EC2 instances and EBS volumes. We continue to work on the remaining EC2 instances and EBS volumes.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;Starting at 6:05 PM PDT some EC2 instances within a single Availability Zone experienced a loss of power. Other instances within the affected Availability Zone experienced connectivity issues and some EBS volumes experienced degraded performance. The root cause of the event was a loss of power to a single line-up within the affected Availability Zone. Power was restored to the affected line-up at 7:20 PM PDT and at 7:35 PM PDT, the affected EC2 instances and EBS volumes had recovered. The issue has been resolved and the service is operating normally. </div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Seoul)","summary":"[RESOLVED] Increased API Error Rates","date":"1633637388","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 1:09 PM PDT</span>&nbsp;PDT 시간으로 12:24 PM과 12:34 PM 사이에, AP-NORTHEAST-2 리전에서 API 에러율 증가가 발생했습니다. 이 이슈는 해결되었고 서비스는 현재 정상 동작 중입니다. 이슈가 영향을 미치는 중에는 API 실패가 발생했을 수 있고, 고객들이 API 호출들을 재시도 할 것을 권고합니다. 고객들이 서비스의 가용성을 복구하기 위한 추가적인 조치를 취할 필요는 없습니다. 이 기간 동안 다른 AWS 서비스들에서도 API 실패가 발생할 수 있었습니다. 만일 궁금한 점이나 서비스와 관련된 운영상의 이슈를 겪으신다면, AWS Support Center를 통해 AWS 기술지원팀에 문의해주십시오. https://console.aws.amazon.com/support  |  Between 12:24 PM and 12:34 PM PDT we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally.  During the period of impact, API's may have failed and we recommend that customers retry these API calls.  Customers do not need to take additional measures to restore the availability of this service.  Additional AWS services may have seen API failures during this period.  If you have any questions or are experiencing any operational issues with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>","service":"ec2-ap-northeast-2"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Elevated Error Rates for AWS Management Console ","date":"1634052623","status":"3","details":"","description":"<div><span class=\"yellowfg\"> 8:30 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:38 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:47 AM PDT</span>&nbsp;Between 8:10 AM and 8:39 AM PDT we experienced increased error rates and latency for the AWS Management Console. The issue has been resolved and the console is operating normally.  This issue did not affect any AWS service APIs, they were operating correctly during this time.</div>","service":"management-console"},{"service_name":"AWS Directory Service (Sydney)","summary":"[RESOLVED] Increased Authentication Error Rates ","date":"1634086680","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:58 PM PDT</span>&nbsp;We are investigating authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Other AWS services, including Workspaces and Client VPN, are also affected by this event if they are configured to use AD Connector directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 7:04 PM PDT</span>&nbsp;We have identified the root cause of the authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. We are currently working on a fix to mitigate the issue. Other AWS services, including Workspaces, Client VPN and SSO, are also affected by this event if they are configured to use AD Connector directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 7:22 PM PDT</span>&nbsp;We have identified the root cause of the authentication errors on AD Connector Directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Our mitigation efforts are working as expected and customers should see resolution over the next 3 hours. We are currently working on steps to speed this up and will update resolution timings as we progress. Other AWS services, including Workspaces, Client VPN and SSO, are also affected by this event if they are configured to use AD Connector Directories with MFA authentication enabled.</div><div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;Our actions to expedite recovery have been successful and we now expect the fix to be completely deployed within the hour.</div><div><span class=\"yellowfg\"> 9:19 PM PDT</span>&nbsp;Between 1:00 PM and 9:15 PM PDT we experienced authentication errors on AD Connector directories that have MFA authentication enabled in the AP-SOUTHEAST-2 Region. Other AWS services, including Workspaces, Client VPN and SSO were also affected by this event if they were configured to use AD Connector directories with MFA authentication enabled. The issue has been resolved and the service is operating normally.</div>","service":"directoryservice-ap-southeast-2"},{"service_name":"AWS Internet Connectivity (US-West)","summary":"[RESOLVED] Network Connectivity in US-GOV-WEST-1","date":"1635874351","status":"2","details":"","description":"<div><span class=\"yellowfg\">10:32 AM PDT</span>&nbsp;We are investigating connectivity issues in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">11:45 AM PDT</span>&nbsp;The issue is affecting network connectivity from the Internet to EC2 instances in a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region, between instances within this Availability Zone and between instances within this Availability Zone and other Availability Zones.</div><div><span class=\"yellowfg\">12:54 PM PDT</span>&nbsp;We have resolved the issue affecting Internet connectivity to a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-us-gov-west-1"},{"service_name":"Amazon Elastic Compute Cloud (US-West)","summary":"[RESOLVED] Network Connectivity","date":"1635875642","status":"2","details":"","description":"<div><span class=\"yellowfg\">10:54 AM PDT</span>&nbsp;Network Connectivity\nWe're investigating network connectivity issues for instances within a single Availability Zone (USGW1-AZ3) in the US-GOV-WEST-1 Region. We are also seeing increased error rates and latencies for the EC2 APIs within the region and are working to solve the issue.</div><div><span class=\"yellowfg\">11:41 AM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The issue is affecting network connectivity from the Internet to instances in the affected Availability Zone, between instances within the affected Availability Zone and between instances within the affected Availability Zone and other Availability Zones. The EC2 APIs are also experiencing increased error rates and latencies within the US-GOV-WEST-1 Region, which is also affecting the AWS Management Console. We have made some progress in mitigating the impact for other AWS services, such as connectivity to Amazon S3, but continue to work on resolving the issue.</div><div><span class=\"yellowfg\">12:09 PM PDT</span>&nbsp;We continue to work towards resolving the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. We have identified the root cause of the issue and are now focused on mitigating the issue. We are seeing some recovery in the error rates and latencies for the EC2 APIs and launches of new instances are once again working within the region. For recovery at this stage, we recommend focusing on shifting workloads and traffic away from the affected Availability Zone (USGW-AZ3).</div><div><span class=\"yellowfg\">12:46 PM PDT</span>&nbsp;We are seeing recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. Once networking was restored to the affected Availability Zone, affected AWS services are also recovering. While the majority are seeing full recovery, we continue to work on the networking-related EC2 APIs, which are still seeing errors at this stage.</div><div><span class=\"yellowfg\"> 1:10 PM PDT</span>&nbsp;We have seen recovery for the issue affecting connectivity for instances within a single Availability Zone (USGW-AZ3) in the US-GOV-WEST-1 Region. The increased error rates and latencies for the EC2 APIs have also recovered. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-gov-west-1"},{"service_name":"Amazon Kinesis Data Streams (US-West)","summary":"[RESOLVED] Increased API Error Rates","date":"1635879208","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:53 AM PDT</span>&nbsp;We are investigating increased API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 1:01 PM PDT</span>&nbsp;Between 10:15 AM and 12:53 PM PDT we experienced increased API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"kinesis-us-gov-west-1"},{"service_name":"AWS Internet Connectivity (Sao Paulo)","summary":"[RESOLVED] Internet Connectivity","date":"1636421771","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:36 PM PST</span>&nbsp;We are investigating internet connectivity issues within the SA-EAST-1 Region. This is affecting connectivity into the SA-EAST-1 Region but also causing increased error rates and latencies for AWS service APIs within the region.</div><div><span class=\"yellowfg\"> 5:51 PM PST</span>&nbsp;Between 5:15 PM and 5:41 PM PST we experienced Internet connectivity issues for a single Availability Zone (sae1-az1) within the SA-EAST-1 Region. Some AWS services also experienced increased error rates and latencies during this time. The issue has been resolved and the service is operating normally.</div>","service":"internetconnectivity-sa-east-1"},{"service_name":"Amazon Simple Storage Service (N. Virginia)","summary":"[RESOLVED] Elevated API Error Rates","date":"1636495106","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 1:58 PM PST</span>&nbsp;We are investigating increased 5xx error rates and latencies for requests to the Amazon S3 APIs in the US-EAST-1 Region. Where possible, we recommend that requests that fail with a 5xx error be retried.</div><div><span class=\"yellowfg\"> 2:48 PM PST</span>&nbsp;We have identified the S3 subsystem responsible for increased 5xx error rates for the S3 PUT APIs, and are working to isolate the root cause within this subsystem. Customers may also be experiencing increased latency when performing PUT operations. During this time, we recommend customers retry any failed requests.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;We are continuing to see increased 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. We have narrowed down the root cause to a specific sub-system within S3 and continue to make progress in mitigating the impact to this service but have not yet seen significant improvement. S3 API error rates and latencies have stayed a consistent low level with the vast majority of request retries succeeding. While the vast majority of requests are being processed within the normal latency levels, request tail latencies are exceeding 1 second in some cases. In some applications, increasing client timeouts may also help to mitigate the issue. </div><div><span class=\"yellowfg\"> 4:52 PM PST</span>&nbsp;We are starting to see some improvement in the 5xx error rates and latencies for S3 API requests, in particular S3 PUT API calls. The issue affected a subsystem that stores routing metadata used by Amazon S3 to map API requests to the storage nodes. A recent update caused increased load within this subsystem, which led to increased error rates and latencies for the S3 APIs. We have now successfully mitigated this increased load within this subsystem and are seeing early signs of recovery. As the sub-system processes the backlog of requests, S3 API error rates and latencies will continue to improve.</div><div><span class=\"yellowfg\"> 5:53 PM PST</span>&nbsp;We continue to see a gradual improvement in error rates as we process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We are currently working on mitigations to speed up the processing of the backlog during this event. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally. </div><div><span class=\"yellowfg\"> 6:58 PM PST</span>&nbsp;We continue to process the backlog of mappings between request metadata and data storage in the sub-system affected by the increased load. We have implemented two parallel mitigations to improve the speed of processing. Both mitigations are in process of deployment. Once the backlog is resolved, we expect that the error rate will fully recover. The vast majority of requests to S3 APIs continue to operate normally.</div><div><span class=\"yellowfg\"> 7:51 PM PST</span>&nbsp;We have completed the mitigation to accelerate processing of the mappings between S3 API request metadata and storage. The backlog has been fully processed and S3 API errors and latencies have returned to normal levels. The issue has been resolved and the service is operating normally.</div>","service":"s3-us-standard"},{"service_name":"Amazon Redshift (N. Virginia)","summary":"[RESOLVED] Redshift cluster reboot and degraded performance","date":"1636496591","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 2:23 PM PST</span>&nbsp;We are investigating cluster reboots and degraded performance for Redshift Clusters in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:47 PM PST</span>&nbsp;We continue to investigate cluster reboots and degraded performance for Redshift RA3 Clusters in the US-EAST-1 Region. The cluster reboots are being triggered by writes that are being impacted by the elevated S3 put latencies. Redshift attempts to retry these writes automatically but if they are unsuccessful after extended attempts, the cluster may restart. If you are able to pause write workloads while we work towards resolving the S3 put latency issue, your clusters will no longer restart and can serve read queries normally.</div><div><span class=\"yellowfg\"> 4:53 PM PST</span>&nbsp;We have identified and continue to work on mitigating the root cause of the Redshift RA3 Clusters reboots and degraded performance in the US-EAST-1 Region. The cluster reboots are triggered by Redshift writes to S3 that have been impacted by the elevated S3 PUT API latencies. If you are impacted and able to pause Redshift write workloads on your RA3 clusters while we work towards recovery, your clusters will no longer restart and can serve read queries normally. As S3 API error rates and latencies continue to improve, we expect Redshift RA3 cluster restarts to decline as well.</div><div><span class=\"yellowfg\"> 7:19 PM PST</span>&nbsp;While the S3 API error rates and latencies continue to hold steady, we continue to see a low rate of Redshift RA3 Cluster restarts. While we continue to take steps to mitigate and reduce the risk of a restart for affected Redshift clusters, we expect to see full recovery when the S3 error rates and latencies have fully recovered. Please refer to the S3 Service Health Dashboard updates for progress towards recovery.</div><div><span class=\"yellowfg\"> 7:47 PM PST</span>&nbsp;As S3 has completed their mitigation and is operating normally, Redshift RA3 clusters are seeing writes succeed and clusters are no longer rebooting. For the vast majority of customers the issue has been resolved and the service is operating normally. We continue to work with a small number of impacted customers individually to recover their clusters and will reach out via the AWS Personal Health Dashboard and AWS Support.</div>","service":"redshift-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Stockholm)","summary":"[RESOLVED] Increased EC2 Console Error Rates","date":"1637596278","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:51 AM PST</span>&nbsp;We are investigating increased error rates within the EC2 Management Console in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are starting to see recovery for the issue causing increased error rates in the EC2 Console in the EU-NORTH-1 Region. We expect to see full recovery shortly. In the meantime, the EC2 APIs and EC2 CLI remain unaffected by the issue.</div><div><span class=\"yellowfg\"> 8:38 AM PST</span>&nbsp;We have resolved the issue causing increased error rates for the EC2 Management Console in the EU-NORTH-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"ec2-eu-north-1"},{"service_name":"Amazon EventBridge (Cape Town)","summary":"[RESOLVED] Event Delivery Delays","date":"1637641154","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating increased event delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:13 PM PST</span>&nbsp;We continue to investigate increased event delivery delays in AF-SOUTH-1 Region. We are actively working to resolve the issue.</div><div><span class=\"yellowfg\">10:26 PM PST</span>&nbsp;We can confirm elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken initial mitigation actions based on our investigations and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23, 12:04 AM PST</span>&nbsp;We continue working to identify the root cause of elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. We have taken further mitigation actions which have reduced the delivery latency and continue to work toward full resolution.</div><div><span class=\"yellowfg\">Nov 23,  1:59 AM PST</span>&nbsp;Between November 22 7:30 PM and November 23 1:32 AM PST, we experienced elevated event delivery latency for EventBridge events in the AF-SOUTH-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"events-af-south-1"},{"service_name":"Amazon CloudWatch (Cape Town)","summary":"[RESOLVED] Metric Stream Delivery Delays ","date":"1637641175","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:19 PM PST</span>&nbsp;We are investigating CloudWatch Metric Streams delivery delays in the AF-SOUTH-1 Region.</div><div><span class=\"yellowfg\"> 9:09 PM PST</span>&nbsp;We can confirm elevated API error rates for CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. We are actively working to resolve the issue. Customers may experience issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams is not affected.</div><div><span class=\"yellowfg\"> 9:42 PM PST</span>&nbsp;Between 7:30 PM and 9:25 PM PST, we experienced elevated error rates when calling CloudWatch Metric Streams APIs in the AF-SOUTH-1 Region. Customers may have experienced issues creating or updating their metric streams. Delivery of metric updates for the existing metric streams was not affected. The issue has been resolved and the service is operating normally.</div>","service":"cloudwatch-af-south-1"},{"service_name":"AWS Lambda (Ohio)","summary":"[RESOLVED] Increased Error Rates","date":"1637779569","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:46 AM PST</span>&nbsp;We are investigating increased invoke error rates in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:10 AM PST</span>&nbsp;We continue to investigate increased function invoke error rates within the US-EAST-2 Region. We are working to determine the root cause of the issue and will continue to provide updates as we make progress in resolving the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">11:34 AM PST</span>&nbsp;We have identified the root cause within Lambda that is causing the increased error rates for function invocations within the US-EAST-2 Region. The subsystem responsible for executing functions is currently impaired and we are working to resolve it. We have seen some improvement in Lambda function invocation error rates and believe that this will continue as we take steps to resolve the issue. Other AWS services, including AWS Management Console, API Gateway, and Batch, are experiencing elevated error rates as a result of this issue.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:27 AM and 12:03 PM PST we experienced increased Lambda invoke error rates in the US-EAST-2 Region. It may take some additional time to process backlogged async invoke traffic that accumulated during this period. The issue has been resolved and the service is operating normally.</div>","service":"lambda-us-east-2"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Increased Error Rates","date":"1637780142","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:55 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console is the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:46 AM PST</span>&nbsp;We are seeing some recovery in error rates and latencies for the AWS Management Console in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:13 PM PST</span>&nbsp;Between 10:26 AM and 12:05 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"management-console"},{"service_name":"Amazon API Gateway (Ohio)","summary":"[RESOLVED] Increased API Error Rates","date":"1637780385","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We are investigating increased API errors for API Gateway in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">11:24 AM PST</span>&nbsp;We have confirmed that the elevated API error rate is restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs).</div><div><span class=\"yellowfg\">12:20 PM PST</span>&nbsp;Between 10:27 AM and 12:06 PM PST, we experienced elevated API error rate in the US-EAST-2 Region, restricted to control plane operations for API Gateway Version 2 API (WebSocket and HTTP APIs). The issue has been resolved, and the service is operating normally. </div>","service":"apigateway-us-east-2"},{"service_name":"AWS Batch (Ohio)","summary":"[RESOLVED] Increased API Error Rates and Scaling Delays","date":"1637781298","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:14 AM PST</span>&nbsp;We are investigating increased API error rates and delays in scaling of some AWS Batch Compute Environments in the US-EAST-2 Region.</div><div><span class=\"yellowfg\">12:12 PM PST</span>&nbsp;Between 10:25 AM and 12:00 PM PST we experienced increased error rates for all AWS Batch APIs, as well as some Compute Environment scaling delays, in the US-EAST-2 Region. Compute Resource connectivity and running jobs were not affected. The issue has been resolved and the service is operating normally.</div>","service":"batch-us-east-2"},{"service_name":"Amazon Simple Notification Service (Oregon)","summary":"[RESOLVED] Increased Latency and Error Rates","date":"1638021360","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:56 AM PST</span>&nbsp;We are investigating increased latency and error rates for API calls in the US-WEST-2 Region. We will provide more information as we continue to investigate.</div><div><span class=\"yellowfg\"> 6:12 AM PST</span>&nbsp;We are starting to see improved SNS API success rates and latency in the US-WEST-2 Region, and are working towards full recovery.</div><div><span class=\"yellowfg\"> 6:42 AM PST</span>&nbsp;Between 5:05 AM and 5:55 AM PST, we experienced increased SNS API latency and error rates in the US-WEST-2 Region. The issue has been resolved and the service is operating normally.</div>","service":"sns-us-west-2"},{"service_name":"Amazon CloudWatch (Ireland)","summary":"[RESOLVED] Delayed CloudWatch Metrics","date":"1638042899","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:55 AM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters.  We are are working towards resolution.</div><div><span class=\"yellowfg\">12:49 PM PST</span>&nbsp;We can confirm increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. CloudWatch alarms may transition into \"INSUFFICIENT_DATA\" state if set on metrics extracted using log filters. We have isolated the likely root cause to a subsystem that saw an unexpected jump in resource consumption.  We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 1:52 PM PST</span>&nbsp;We have implemented a fix to address the CloudWatch log event processing delays in the EU-WEST-1 Region and are starting to see signs of recovery. We will provide an update once full recovery has been observed.</div><div><span class=\"yellowfg\"> 2:56 PM PST</span>&nbsp;Between 10:26 AM and 02:40 PM PST, we experienced increased delays for CloudWatch log event processing for metric filter extraction and log subscriptions in the EU-WEST-1 Region. This was due to a subsystem that saw an unexpected jump in resource consumption. The issue has been resolved and the service is operating normally. New events are processing as normal, while we work through the message backlog. We expect to completely drain the backlog over the next 1 hour.</div>","service":"cloudwatch-eu-west-1"},{"service_name":"AWS CloudShell (N. Virginia)","summary":"[RESOLVED] Increased Latencies and Failure Rates","date":"1638507226","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:53 PM PST</span>&nbsp;We are experiencing increased latencies and failures in launching CloudShell environments in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:26 PM PST</span>&nbsp;Between 7:05 PM and 9:15 PM PST we experienced increased latencies and failures launching CloudShell environments in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"cloudshell-us-east-1"},{"service_name":"AWS Management Console","summary":"[RESOLVED] Increased Error Rates","date":"1638894176","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 8:22 AM PST</span>&nbsp;We are investigating increased error rates for the AWS Management Console.</div><div><span class=\"yellowfg\"> 8:26 AM PST</span>&nbsp;We are experiencing API and console issues in the US-EAST-1 Region.  We have identified root cause and we are actively working towards recovery.  This issue is affecting the global console landing page, which is also hosted in US-EAST-1.  Customers may be able to access region-specific consoles going to https://console.aws.amazon.com/.  So, to access the US-WEST-2 console, try https://us-west-2.console.aws.amazon.com/</div><div><span class=\"yellowfg\"> 4:25 PM PST</span>&nbsp;We are seeing improvements in the error rates and latencies in the AWS Management Console in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:14 PM PST</span>&nbsp;Between 7:32 AM to 4:56 PM PST we experienced increased error rates and latencies for the AWS Management Console in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>","service":"management-console"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1638895771","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 8:49 AM PST</span>&nbsp;We are experiencing elevated error rates for EC2 APIs in the US-EAST-1 region.  We have identified root cause and we are actively working towards recovery.</div><div><span class=\"yellowfg\"> 3:31 PM PST</span>&nbsp;Between 7:32 AM and 3:10 PM PST we experienced increased API error rates in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>","service":"ec2-us-east-1"},{"service_name":"Amazon Connect (N. Virginia)","summary":"[RESOLVED] Degraded Contact Handling","date":"1638896019","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 8:53 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 9:08 AM PST</span>&nbsp;We are experiencing degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 9:18 AM PST</span>&nbsp;We can confirm degraded Contact handling by agents in the US-EAST-1 Region. Agents may experience issues logging in or being connected with end-customers.</div><div><span class=\"yellowfg\"> 4:47 PM PST</span>&nbsp;We are seeing improvements to contact handling in the US-EAST-1 Region. We are continuing to work towards resolution</div><div><span class=\"yellowfg\"> 5:10 PM PST</span>&nbsp;Between 7:25 AM PST and 4:47 PM PST we experienced degraded Contact handling, increased user login errors, and increased API error rates in the US-EAST-1 Region. During this time, end-customers may have experienced delays or errors when placing a call or starting a chat, and agents may have experienced issues logging in or being connected with end-customers. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>","service":"connect-us-east-1"},{"service_name":"Amazon DynamoDB (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1638896271","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 8:57 AM PST</span>&nbsp;We are currently investigating increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 3:40 PM PST</span>&nbsp;Between 7:40 AM and 2:25 PM PST, we experienced increased error rates with DynamoDB Control Plane APIs, including the Backup and Restore APIs in US-EAST-1 Region. Data plane operations were not impacted. The issue has been resolved and the service is operating normally.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>","service":"dynamodb-us-east-1"},{"service_name":"AWS Support Center","summary":"[RESOLVED] Increased Error Rates","date":"1638896490","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 9:01 AM PST</span>&nbsp;We are investigating increased error rates for the Support Center console and Support API in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 1:55 PM PST</span>&nbsp;We continue to see increased error rates for the Support Center console and Support API in the US-EAST-1 Region. Support Cases successfully created via the console or the API may not be successfully routed to Support Engineers. We continue to work toward full resolution.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Between 7:33 AM and 2:25 PM PST, we experienced increased error rates for the Support Center console and Support API in the US-EAST-1 Region. This resulted in errors in creating support cases and delays in routing cases to Support Engineers. The issue has been resolved and our Support Engineering team is responding to cases. The service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>","service":"supportcenter"},{"service_name":"Amazon EventBridge (N. Virginia)","summary":"[RESOLVED] Event Delivery Delays","date":"1638917670","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 2:54 PM PST</span>&nbsp;We have temporarily disabled event deliveries in the US-EAST-1 Region. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will not trigger targets. These events will still be received by EventBridge and will deliver once we recover.</div><div><span class=\"yellowfg\"> 3:00 PM PST</span>&nbsp;We have re-enabled event deliveries in the US-EAST-1 Region, but are experiencing event delivery latencies. Customers who have EventBridge rules that trigger from 1st party AWS events (including CloudTrail), scheduled events via CloudWatch, events from 3rd parties, and events they post themselves via the PutEvents API action will be delayed.</div><div><span class=\"yellowfg\"> 4:31 PM PST</span>&nbsp;We continue to see event delivery latencies in the US-EAST-1 region. We have identified the root cause and are working toward recovery.</div><div><span class=\"yellowfg\"> 6:00 PM PST</span>&nbsp;Event delivery latency for new events in the US-EAST-1 Region have returned to normal levels. We continue to process a backlog of events.</div><div><span class=\"yellowfg\"> 9:21 PM PST</span>&nbsp;Between 7:30 AM and 8:40 PM PST we experienced elevated event delivery latency in the US-EAST-1 Region. Event delivery latencies have returned to normal levels. Some CloudTrail events for API calls between 7:35 AM and 6:05 PM PST may be delayed but will be delivered in the coming hours.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>","service":"events-us-east-1"},{"service_name":"Amazon API Gateway (N. Virginia)","summary":"[RESOLVED] Elevated Errors and Latencies","date":"1638919396","status":"2","details":"","description":"<div><span class=\"yellowfg\"> 3:23 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are working towards resolution.</div><div><span class=\"yellowfg\"> 4:05 PM PST</span>&nbsp;We continue to see increased error rates and latencies for invokes in the US-EAST-1 region. We have identified the root cause and are continuing to work towards resolution.</div><div><span class=\"yellowfg\"> 4:41 PM PST</span>&nbsp;We have seen improvement in error rates and latencies for invokes in the US-EAST-1 region. We continue to drive towards full recovery.</div><div><span class=\"yellowfg\"> 5:23 PM PST</span>&nbsp;Between 9:02 AM and 5:01 PM PST we experienced increased error rates and latencies for invokes in the US-EAST-1 Region. The issue has been resolved and the service is operating normally.<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>","service":"apigateway-us-east-1"},{"service_name":"Amazon Elastic Container Service (N. Virginia)","summary":"[RESOLVED] Elevated Fargate task launch failures","date":"1638919976","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 3:32 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day, but we are still investigating task launch failures using the Fargate launch type. Task launches using the EC2 launch type are not impacted.</div><div><span class=\"yellowfg\"> 4:44 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are working towards recovery.</div><div><span class=\"yellowfg\"> 5:31 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. We have identified the root cause for the increased Fargate launch failures and are starting to see recovery. As we work towards full recovery, customers may experience insufficient capacity errors and these are being addressed as well.</div><div><span class=\"yellowfg\"> 7:30 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">11:01 PM PST</span>&nbsp;ECS has recovered from the issue earlier in the day. Task launches using the EC2 launch type are fully recovered. Fargate task launches are currently experiencing increased insufficient capacity errors. We are working on addressing this and have recently seen a decrease in these errors while continuing to work towards full recovery. In the interim, tasks sizes smaller than 4vCPU are less likely to see insufficient capacity errors.</div><div><span class=\"yellowfg\">Dec 8,  2:29 AM PST</span>&nbsp;Between 7:31 AM PST on December 7 and 2:20 AM PST on December 8, ECS experienced increased API error rates, latencies, and task launch failures. API error rates and latencies recovered by 6:10 PM PST on December 7. After this point, ECS customers using the EC2 launch type were fully recovered. ECS customers using the Fargate launch type along with EKS customers using Fargate continued to see decreasing impact in the form of insufficient capacity errors between 4:40 PM PST on December 7 and 2:20 AM on December 8. The service is now operating normally. A small set of customers may still experience low levels of insufficient capacity errors and will be notified using the Personal Health Dashboard in that case. There was no impact to running tasks during the event although any ECS task that failed health checks would have been stopped because of that failing health check.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a></div>","service":"ecs-us-east-1"},{"service_name":"AWS Batch (N. Virginia)","summary":"[RESOLVED] Increased Job Processing Delays","date":"1638922058","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 4:07 PM PST</span>&nbsp;We have identified the root cause of increased delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 5:20 PM PST</span>&nbsp;We have seen improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region and continue to work toward resolution.</div><div><span class=\"yellowfg\"> 8:02 PM PST</span>&nbsp;Improvement from the delay in job state transitions of AWS Batch Jobs in the US-EAST-1 Region is accelerating, we continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:29 PM PST</span>&nbsp;Between 7:35 AM and 8:13 PM PST, we experienced increase job state transition delays of AWS Batch Jobs in the US-EAST-1 Region.  The issue has been resolved and the service is now operating normally for new job submissions.  Jobs that were delayed from earlier in the event will be processed in order until we clear the queue.\n<br><br>\n\nWe'd like to share more information about the service event on Tuesday December 7th. Additional details are available below. Should you have any questions, please contact AWS Support. <a href=\"https://aws.amazon.com/message/12721/\">https://aws.amazon.com/message/12721/</a> </div>","service":"batch-us-east-1"},{"service_name":"Amazon Redshift (N. Virginia)","summary":"[RESOLVED] Redshift Management Console Errors","date":"1639055699","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:15 AM PST</span>&nbsp;We are investigating elevated error rates for the Redshift Management Console in the US-EAST-1 region and are actively working to resolve the issue.</div><div><span class=\"yellowfg\"> 5:34 AM PST</span>&nbsp;We continue to investigate elevated error rates for the Redshift Management Console in the US-EAST-1 region. We have identified the issue causing elevated error rates and are actively working to resolve the issue. Customer clusters remain available and are operating normally. Customers can use API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries. </div><div><span class=\"yellowfg\"> 6:30 AM PST</span>&nbsp;We have identified the root cause of the issue causing elevated error rates and are in the process of deploying a fix that will resolve the issue. We do not have a precise ETA for the deployment to complete that we can share at this time. Customer clusters remain available and are operating normally. Customers can use the API and CLI to manage their clusters and can use SQL clients using ODBC/JDBC and DATA API to run queries.</div><div><span class=\"yellowfg\"> 6:58 AM PST</span>&nbsp;Between 12:00 AM and 6:25 AM PST we saw elevated error rates for the Amazon Redshift Management Console in the US-EAST-1 region. This initial error rate increase was observed at 12:00 AM PST and at 4:05 AM PST this error rate increased further. Customer clusters were operating normally throughout and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. We have completed the deployment of a fix. The issue has been resolved and the Management Console and the Amazon Redshift service are operating normally.</div>","service":"redshift-us-east-1"},{"service_name":"Amazon Redshift (N. Virginia)","summary":"[RESOLVED] Redshift Management Console Errors","date":"1639067113","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:25 AM PST</span>&nbsp;Earlier today we reported elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. At 6:58 AM PST we reported the issue was resolved but we have since detected that some errors persist despite much lower rates. Redshift clusters remain available and customers can manage their clusters using the API and CLI. Customers can also execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API. We continue to work towards resolution.</div><div><span class=\"yellowfg\"> 9:05 AM PST</span>&nbsp;We continue to investigate intermittent elevated error rates when accessing the Amazon Redshift Management Console in the US-EAST-1 Region. Customers will be able to get the console to load if they refresh their browser tab several times. Once the console has loaded, the console will work as expected and customers will be able to execute queries normally. Redshift clusters remain available and customers can manage their clusters using the API and CLI and can execute queries from their applications or SQL clients using JDBC/ODBC connections and the Data API.</div><div><span class=\"yellowfg\">10:45 AM PST</span>&nbsp;Between 7:25 AM and 10:05 AM PST we experienced increased error rates for the Amazon Redshift Management Console in the US-EAST-1 Region. During this time, clusters were operating normally  and customers were able to manage their clusters using the API and the CLI, and execute SQL queries using JDBC/ODBC connections and the Data API. The issue is resolved and the service is operating normally.</div>","service":"redshift-us-east-1"},{"service_name":"AWS Internet Connectivity (Oregon)","summary":"[RESOLVED] Internet Connectivity","date":"1639582979","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:43 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-2 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:14 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-2 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.\n</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>","service":"internetconnectivity-us-west-2"},{"service_name":"AWS Internet Connectivity (N. California)","summary":"[RESOLVED] Internet Connectivity","date":"1639583545","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:52 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:01 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:14 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>","service":"internetconnectivity-us-west-1"},{"service_name":"AWS Internet Connectivity (US-West)","summary":"[RESOLVED] Internet Connectivity","date":"1639583722","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:55 AM PST</span>&nbsp;We are investigating Internet connectivity issues to the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\"> 8:00 AM PST</span>&nbsp;We have identified the root cause of the Internet connectivity to the US-GOV-WEST-1 Region and have taken steps to restore connectivity. We have seen some improvement to Internet connectivity in the last few minutes but continue to work towards full recovery.</div><div><span class=\"yellowfg\"> 8:10 AM PST</span>&nbsp;We have resolved the issue affecting Internet connectivity to the US-GOV-WEST-1 Region. Connectivity within the region was not affected by this event. The issue has been resolved and the service is operating normally.</div><div><span class=\"yellowfg\">12:16 PM PST</span>&nbsp;Between 7:14 AM PST and 7:59 AM PST, customers experienced elevated network packet loss that impacted connectivity to a subset of Internet destinations. Traffic within AWS Regions, between AWS Regions, and to other destinations on the Internet was not impacted. The issue was caused by network congestion between parts of the AWS Backbone and a subset of Internet Service Providers, which was triggered by AWS traffic engineering, executed in response to congestion outside of our network. This traffic engineering incorrectly moved more traffic than expected to parts of the AWS Backbone that affected connectivity to a subset of Internet destinations. The issue has been resolved, and we do not expect a recurrence.</div>","service":"internetconnectivity-us-gov-west-1"},{"service_name":"AWS Elastic Beanstalk (N. Virginia)","summary":"[RESOLVED] Console Application Upload Errors","date":"1640158400","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:33 PM PST</span>&nbsp;We are investigating an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. Customers who need to update or deploy a new application version should do so using the AWS CLI. Existing applications are not impacted by this issue</div><div><span class=\"yellowfg\">Dec 22, 12:34 AM PST</span>&nbsp;We continue to investigate an issue where customers are unable to upload and deploy new application versions through the Elastic Beanstalk console in multiple Regions. We are determining the root causes and working through steps to mitigate the issue. Customers who need to update or deploy a new application version should do so using the AWS CLI while we work towards resolving the issue. Existing applications are not impacted by this issue.</div><div><span class=\"yellowfg\">Dec 22,  1:20 AM PST</span>&nbsp;We have identified the root cause and prepared a fix to address the issue that prevents customers from uploading new application versions through the Elastic Beanstalk console in multiple Regions. The service team is testing this fix and preparing for deployment to the Regions that are affected by this issue. We expect to see full recovery by 3:00 AM PST and will continue to keep you updated if this ETA changes. Customers who need to update or deploy a new application version should do so using the AWS CLI until the issue is fully resolved.</div><div><span class=\"yellowfg\">Dec 22,  3:21 AM PST</span>&nbsp;Between December 21, 2021 at 6:37 PM  and December 22, 2021 at 03:17 AM PST, customers were unable to upload their code through the Elastic Beanstalk console due to a Content Security Policy (CSP) error. Customers were impacted when they attempted to upload a new application version for existing environments or upload their code when creating a new environment in multiple regions. The issue has been resolved and the service is operating normally.</div>","service":"elasticbeanstalk-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] API Error Rates","date":"1640176551","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 4:35 AM PST</span>&nbsp;We are investigating increased EC2 launch failures and networking connectivity issues for some instances in a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Other Availability Zones within the US-EAST-1 Region are not affected by this issue.</div><div><span class=\"yellowfg\"> 5:01 AM PST</span>&nbsp;We can confirm a loss of power within a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. This is affecting availability and connectivity to EC2 instances that are part of the affected data center within the affected Availability Zone. We are also experiencing elevated RunInstance API error rates for launches within the affected Availability Zone. Connectivity and power to other data centers within the affected Availability Zone, or other Availability Zones within the US-EAST-1 Region are not affected by this issue, but we would recommend failing away from the affected Availability Zone (USE1-AZ4) if you are able to do so. We continue to work to address the issue and restore power within the affected data center.</div><div><span class=\"yellowfg\"> 5:18 AM PST</span>&nbsp;We continue to make progress in restoring power to the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have now restored power to the majority of instances and networking devices within the affected data center and are starting to see some early signs of recovery. Customers experiencing connectivity or instance availability issues within the affected Availability Zone, should start to see some recovery as power is restored to the affected data center. RunInstances API error rates are returning to normal levels and we are working to recover affected EC2 instances and EBS volumes. While we would expect continued improvement over the coming hour, we would still recommend failing away from the Availability Zone if you are able to do so to mitigate this issue.</div><div><span class=\"yellowfg\"> 5:39 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. Network connectivity within the affected Availability Zone has also returned to normal levels. While all services are starting to see meaningful recovery, services which were hosting endpoints within the affected data center - such as single-AZ RDS databases, ElastiCache, etc. - would have seen impact during the event, but are starting to see recovery now. Given the level of recovery, if you have not yet failed away from the affected Availability Zone, you should be starting to see recovery at this stage. </div><div><span class=\"yellowfg\"> 6:13 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. We continue to make progress in recovering the remaining EC2 instances and EBS volumes within the affected Availability Zone. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 6:51 AM PST</span>&nbsp;We have now restored power to all instances and network devices within the affected data center and are seeing recovery for the majority of EC2 instances and EBS volumes within the affected Availability Zone. For the remaining EC2 instances, we are experiencing some network connectivity issues, which is slowing down full recovery. We believe we understand why this is the case and are working on a resolution. Once resolved, we expect to see faster recovery for the remaining EC2 instances and EBS volumes. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We have a small number of affected EBS volumes that are still experiencing degraded IO performance that we are working to recover. The majority of AWS services have also recovered, but services which host endpoints within the customer’s VPCs - such as single-AZ RDS databases, ElasticCache, Redshift, etc. - continue to see some impact as we work towards full recovery. </div><div><span class=\"yellowfg\"> 8:02 AM PST</span>&nbsp;Power continues to be stable within the affected data center within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have been working to resolve the connectivity issues that the remaining EC2 instances and EBS volumes are experiencing in the affected data center, which is part of a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We have addressed the connectivity issue for the affected EBS volumes, which are now starting to see further recovery. We continue to work on mitigating the networking impact for EC2 instances within the affected data center, and expect to see further recovery there starting in the next 30 minutes. Since the EC2 APIs have been healthy for some time within the affected Availability Zone, the fastest path to recovery now would be to relaunch affected EC2 instances within the affected Availability Zone or other Availability Zones within the region.</div><div><span class=\"yellowfg\"> 9:28 AM PST</span>&nbsp;We continue to make progress in restoring connectivity to the remaining EC2 instances and EBS volumes. In the last hour, we have restored underlying connectivity to the majority of the remaining EC2 instance and EBS volumes, but are now working through full recovery at the host level. The majority of affected AWS services remain in recovery and we have seen recovery for the majority of single-AZ RDS databases that were affected by the event. If you are able to relaunch affected EC2 instances within the affected Availability Zone, that may help to speed up recovery. Note that restarting an instance at this stage will not help as a restart does not change the underlying hardware. We continue to work towards full recovery.</div><div><span class=\"yellowfg\">11:08 AM PST</span>&nbsp;We continue to make progress in restoring power and connectivity to the remaining EC2 instances and EBS volumes, although recovery of the remaining instances and volumes is taking longer than expected. We believe this is related to the way in which the data center lost power, which has led to failures in the underlying hardware that we are working to recover. While EC2 instances and EBS volumes that have recovered continue to operate normally within the affected data center, we are working to replace hardware components for the recovery of the remaining EC2 instances and EBS volumes. We have multiple engineers working on the underlying hardware failures and expect to see recovery over the next few hours. As is often the case with a loss of power, there may be some hardware that is not recoverable, and so we continue to recommend that you relaunch your EC2 instance, or recreate you EBS volume from a snapshot, if you are able to do so.</div><div><span class=\"yellowfg\">12:03 PM PST</span>&nbsp;Over the last hour, after addressing many of the underlying hardware failures, we have seen an accelerated rate of recovery for the affected EC2 instances and EBS volumes. We continue to work on addressing the underlying hardware failures that are preventing the remaining EC2 instances and EBS volumes. For customers that continue to have EC2 instance or EBS volume impairments, relaunching affected EC2 instances or recreating affecting EBS volumes within the affected Availability Zone, continues to be a faster path to full recovery. </div><div><span class=\"yellowfg\"> 1:39 PM PST</span>&nbsp;We continue to make progress in addressing the hardware failures that are delaying recovery of the remaining EC2 instances and EBS volumes. At this stage, if you are still waiting for an EC2 instance or EBS volume to fully recover, we would strongly recommend that you consider relaunching the EC2 instance or recreating the EBS volume from a snapshot. As is often the case with a loss of power, there may be some hardware that is not recoverable, which will prevent us from fully recovering the affected EC2 instances and EBS volumes. We are not quite at that point yet in terms of recovery, but it is unlikely that we will recover all of the small number of remaining EC2 instances and EBS volumes. If you need help in launching new EC2 instances or recreating EBS volumes, please reach out to AWS Support.</div><div><span class=\"yellowfg\"> 3:13 PM PST</span>&nbsp;Since the last update, we have more than halved the number of affected EC2 instances and EBS volumes and continue to work on the remaining EC2 instances and EBS volumes. The remaining EC2 instances and EBS volumes have all experienced underlying hardware failures due to the nature of the initial power event, which we are working to resolve. We expect to make further progress on this list within the next hour, but some of the remaining EC2 instances and EBS volumes may not be recoverable due to hardware failures. If you have the ability to relaunch an affected EC2 instance or recreate an affected EBS volume from snapshot, we continue to strongly recommend that you take that path.</div><div><span class=\"yellowfg\"> 4:22 PM PST</span>&nbsp;Starting at 4:11 AM PST some EC2 instances and EBS volumes experienced a loss of power in a single data center within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Instances in other data centers within the affected Availability Zone, and other Availability Zones within the US-EAST-1 Region were not affected by this event. At 4:55 AM PST, power was restored to EC2 instances and EBS volumes in the affected data center, which allowed the majority of EC2 instances and EBS volumes to recover. However, due to the nature of the power event, some of the underlying hardware experienced failures, which needed to be resolved by engineers within the facility. Engineers worked to recover the remaining EC2 instances and EBS volumes affected by the issue. By 2:30 PM PST, we recovered the vast majority of EC2 instances and EBS volumes. However, some of the affected EC2 instances and EBS volumes were running on hardware that has been affected by the loss of power and is not recoverable. For customers still waiting for recovery of a specific EC2 instance or EBS volume, we recommend that you relaunch the instance or recreate the volume from a snapshot for full recovery. If you need further assistance, please contact AWS Support.</div>","service":"ec2-us-east-1"},{"service_name":"AWS Single Sign-On (N. Virginia)","summary":"[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD","date":"1640193970","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 9:26 AM PST</span>&nbsp;We are investigating increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">10:49 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region.  Some customers may begin to see signs of recovery.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">11:56 AM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\">12:10 PM PST</span>&nbsp;As the root cause of this impact is related to Directory Services, we will continue to provide updates on the new post we have just created for Directory Service in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 5:56 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. The issue has been resolved and the service is operating normally.  If you experience any issues with this service or need further assistance, please contact AWS Support.</div>","service":"sso-us-east-1"},{"service_name":"AWS Directory Service (N. Virginia)","summary":"[RESOLVED] Increased Error Rates with Directory Services AD Connector or Managed AD","date":"1640203590","status":"1","details":"","description":"<div><span class=\"yellowfg\">12:06 PM PST</span>&nbsp;We continue to investigate increased error rates for some customers using Directory Services AD Connector or Managed AD with Amazon SSO in US-EAST-1 Region. This is also impacting some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. Some customers may begin to see signs of recovery. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 2:29 PM PST</span>&nbsp;We continue to resolve increased error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication.  We are prioritizing the most impacted directories to expedite resolution.  Additional customers will see recovery as resolution takes place.  Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 4:09 PM PST</span>&nbsp;Our mitigation efforts are working as expected and we are making steady progress toward recovery of error rates for Directory Services AD or Managed AD, impacting some services like Amazon WorkSpaces that can be configured to use Directory Services for user authentication. We continue to prioritize the most impacted directories to expedite resolution. Additional customers will see recovery as resolution takes place. Customers using other Active Directory functionality are not impacted at this time.</div><div><span class=\"yellowfg\"> 5:57 PM PST</span>&nbsp;Between 4:09 AM and 5:00 PM PST we experienced increased error rates for some customers using Directory Services AD Connector or Managed AD with Directory Services in US-EAST-1 Region. This also impacted some services, like Amazon WorkSpaces, that can be configured to use Directory Services for user authentication. The issue has been resolved and the service is operating normally. Customers using other Active Directory functionality were not impacted by this issue. If you experience any issues with this service or need further assistance, please contact AWS Support.</div>","service":"directoryservice-us-east-1"},{"service_name":"AWS Internet Connectivity (Mumbai)","summary":"[RESOLVED] Internet connectivity","date":"1640371277","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:41 AM PST</span>&nbsp;Between 8:59 AM and 9:32 AM PST and between 9:40 AM and 10:16 AM PST we observed Internet connectivity issues with a network provider outside of our network in the AP-SOUTH-1 Region. This impacted Internet connectivity from some customer networks to the AP-SOUTH-1 Region. Connectivity between EC2 instances and other AWS services within the Region was not impacted by this event. The issue has been resolved and we continue to work with the external provider to ensure it does not reoccur.\n</div>","service":"internetconnectivity-ap-south-1"},{"service_name":"Amazon Pinpoint (N. Virginia)","summary":"[RESOLVED] Pinpoint Sending/Receiving Delays","date":"1642185596","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:39 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of Pinpoint customers sending and receiving SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">10:53 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:42 AM PST</span>&nbsp;We can confirm that the sending and receiving of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:43 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while sending and receiving SMS messages using US toll-free numbers. Starting at 5:14 AM SMS message delivery receipts were delayed. These delays will continue while we work with our downstream partners through the backlog of delayed delivery receipts.  The issues have been resolved and the service is operating normally.</div>","service":"pinpoint-us-east-1"},{"service_name":"Amazon Simple Notification Service (N. Virginia)","summary":"[RESOLVED] SMS Delivery Delays","date":"1642186756","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:59 AM PST</span>&nbsp;We can confirm increased error rates and delivery latency for a subset of SNS and Pinpoint customers delivering SMS messages via US toll-free numbers and some long codes. We are working with our partners to resolve this issue, and will continue to provide updates until the issue is fully resolved.</div><div><span class=\"yellowfg\">11:00 AM PST</span>&nbsp;We can confirm that long codes are not impacted, only US toll-free numbers are impacted.  We continue to work with our partners to fully resolve this issue.</div><div><span class=\"yellowfg\">11:12 AM PST</span>&nbsp;We are seeing early signs of recovery, and continue to work with our downstream partners to fully resolve the issue.</div><div><span class=\"yellowfg\">11:38 AM PST</span>&nbsp;We can confirm that the delivery of SMS messages for US toll free numbers has recovered. However, we continue to see issues with delivery receipts being delayed and are working with our downstream partners to resolve the issue.</div><div><span class=\"yellowfg\"> 2:44 PM PST</span>&nbsp;Between 5:14 AM and 11:38 AM PST, we experienced increased delivery latency while delivering SMS messages using US toll-free numbers. Also starting at 5:14 AM, SMS message delivery receipts were delayed, which created a backlog of undelivered delivery receipts. We are continuing to work with our downstream partners to clear this backlog. Receipts for new SMS deliveries will also be delayed until this backlog clears. The issues have been resolved and the service is operating normally.</div>","service":"sns-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Seoul)","summary":"[RESOLVED] Increased API Error Rates ","date":"1642432540","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:15 AM PST</span>&nbsp;메시지: AP-NORTHEAST-2 리전에서 API 오류율 증가에 대해 조사하고 있습니다. | We are investigating increased API error rates in the ap-northeast-2 Region.</div><div><span class=\"yellowfg\"> 7:50 AM PST</span>&nbsp;태평양 표준시(PST) 오전 6시 55분부터 오전 7시 40분 사이에 AP-NORTHEAST-2 리전에서 API 오류율이 증가했습니다. 현재 문제가 해결되었으며 서비스가 정상적으로 작동하고 있습니다. 궁금한 점이 있거나 서비스와 관련하여 운영상의 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 AWS 지원 부서에 문의 부탁 드립니다. | Between 6:55 AM and 7:40 AM PST we experienced increased API error rates in the AP-NORTHEAST-2 Region. The issue has been resolved and the service is operating normally. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support</div>","service":"ec2-ap-northeast-2"},{"service_name":"AWS Internet Connectivity (Seoul)","summary":"[RESOLVED] 네트워크 연결 | Network Connectivity","date":"1644150596","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 4:29 AM PST</span>&nbsp;English follows Korean | 한국어버전 뒤에 영어버전이 있습니다\n\n태평양 표준시 (PST) 기준 오전 3시 32분에서 오전 3시 54분사이에 AP-NORTHEAST-2 리전의 단일 가용 영역(apne2-az3)에 있는 일부 EC2 인스턴스의 인터넷 연결에 영향을 미치는 연결 문제가 발생했습니다. 리전 내의 인스턴스 및 서비스에 대한 연결은 영향을 받지 않았습니다. 현재 문제가 해결되었고 연결이 복원되었습니다. 현재 이 문제를 해결하기 위해 필요한 조치는 없습니다. 궁금하신 점이 있으시거나 서비스 운영에 문제가 있는 경우 AWS 지원 센터 (https://console.aws.amazon.com/support) 를 통해 문의하시기 바랍니다.\n\nBetween 3:32 AM and 3.54 AM PST we experienced connectivity issues affecting Internet connectivity for some EC2 instances in a single Availability Zone (apne2-az3) in AP-NORTHEAST-2 Region. Connectivity to instances and services within the Region was not impacted. The issue has been resolved and connectivity has been restored. No action is currently required to address this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support .</div>","service":"internetconnectivity-ap-northeast-2"},{"service_name":"Amazon Virtual Private Cloud (Frankfurt)","summary":"[RESOLVED] Elevated Error Rates for VPC Console","date":"1646295847","status":"1","details":"","description":"<div><span class=\"yellowfg\">12:24 AM PST</span>&nbsp;We are investigating elevated error rates for the VPC Management Console in the EU-CENTRAL-1 Region.</div><div><span class=\"yellowfg\">12:51 AM PST</span>&nbsp;We have identified the root cause and are starting to see recovery of the VPC Management Console in the EU-CENTRAL-1 Region. We recommend signing out and signing back into your account to refresh your session. We continue working towards full recovery and will continue to keep you updated.</div><div><span class=\"yellowfg\"> 1:06 AM PST</span>&nbsp;Between March 2 10:03 PM and March 3 12:30 AM PST, the VPC Management Console experienced elevated error rates in the EU-CENTRAL-1 Region. API and CLI access were not affected. The issue has been resolved and the service is operating normally.</div>","service":"vpc-eu-central-1"},{"service_name":"AWS Lambda (US-West)","summary":"[RESOLVED] Increased API and Invoke Error Rates","date":"1646856563","status":"1","details":"","description":"<div><span class=\"yellowfg\">12:09 PM PST</span>&nbsp;We are investigating increased invoke and API error rates in the US-GOV-WEST-1 Region.</div><div><span class=\"yellowfg\">12:37 PM PST</span>&nbsp;We can confirm increased invoke and API error rates in the US-GOV-WEST-1 Region. We have deployed a mitigation strategy and continue to work through full resolution.</div><div><span class=\"yellowfg\"> 1:06 PM PST</span>&nbsp;Between 11:07 AM and 12:50 PM PST, we experienced increased invoke and API error rates in the US-GOV-WEST-1 Region. The issue has been resolved and the service is operating normally.</div>","service":"lambda-us-gov-west-1"},{"service_name":"Multiple services (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1646859683","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 1:01 PM PST</span>&nbsp;We are investigating increased API error rates in the US-EAST-1 Region.\n</div><div><span class=\"yellowfg\"> 1:16 PM PST</span>&nbsp;We are seeing recovery in API error rates for all services.</div><div><span class=\"yellowfg\"> 1:29 PM PST</span>&nbsp;Between 12:43 PM and 12:59 PM PST, we experienced increased error rates and latencies for some AWS services within the US-EAST-1 Region. All services are now operating normally, but S3 Event Notifications continues to process a backlog of events that developed during the event. \n\nThe root cause of this issue was an update to the SQS and Lambda endpoints that inadvertently prevented some traffic from reaching these endpoints.</div><div><span class=\"yellowfg\"> 1:45 PM PST</span>&nbsp;S3 Event Notifications have delivered the backlog of events. This issue is resolved and all services are now operating normally.</div>","service":"multipleservices-us-east-1"},{"service_name":"AWS DataSync (N. Virginia)","summary":"[RESOLVED] Elevated error rates","date":"1647437391","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 6:29 AM PDT</span>&nbsp;We are investigating elevated error rates for DataSync tasks with EFS source or destination locations resulting in \"The DataSync destination location is not mounted correctly\".</div><div><span class=\"yellowfg\"> 7:00 AM PDT</span>&nbsp;We continue to investigate elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We'll provide an update at 8:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We have identified the cause of the elevated error rates for DataSync tasks with EFS and FSx source or destination locations resulting in \"The DataSync destination location is not mounted correctly.\" We continue to work towards resolution. We'll provide another update at 9:00 AM PDT if not sooner.</div><div><span class=\"yellowfg\"> 9:11 AM PDT</span>&nbsp;We have started to deploy an update to mitigate the elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The deployment will take approximately 1 hour and 45 minutes to reach all affected regions. We will provide another update once the deployment is complete.</div><div><span class=\"yellowfg\">10:23 AM PDT</span>&nbsp;Between March 15 10:57 PM and March 16 9:57 AM PDT we experienced elevated error rates for DataSync tasks with EFS and FSx source or destination locations. The issue has been resolved and the service is operating normally.</div>","service":"datasync-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Delayed ENI attachment times","date":"1648779488","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:18 PM PDT</span>&nbsp;We are investigating delayed ENI attachment times for EC2 instances within a single Availability Zone (USE1-AZ4) in the US-EAST-1 Region. Newly launched EC2 instances or new ENI attachments, may experience delay in establishing network connectivity within the affected Availability Zone. This issue may also affect resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We have identified the root cause and are working to resolve the issue.</div><div><span class=\"yellowfg\"> 7:47 PM PDT</span>&nbsp;While ENI attachment times have improved, they are still taking longer than normal in the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. The root cause is resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone. We have identified the root cause for this resource contention and are working to fully resolve the issue. For customers launching instances in the affected Availability Zone or attaching new ENIs to existing instances, full network connectivity on the ENIs may take several minutes to be established, instead of seconds. While we expect attachment times to continue to improve, full recovery here may take up to 2 hours. This issue also affects resource provisioning for other services, such as EMR, ECS and Glue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:23 PM PDT</span>&nbsp;We continue to see an improvement in ENI attachment times, and while they are getting much closer to normal levels, we're still seeing some ENI attachments take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. We are making progress in resolving the resource contention within the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, and remain on track for full recovery within 1.5 hours. This issue also affects resource provisioning for other services, such as ELB, EMR, ECS and Glue. Some of these services, such as EMR, have mitigated impact by shifting traffic away from the affected Availability Zone, and others like ELB, are now seeing recovery as ENI attachment latencies have improved. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 8:58 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We remain on track for full recovery within the next hour. At these ENI attachments times, many of the affected services are seeing recovery, or limited impact, as a result of the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\"> 9:31 PM PDT</span>&nbsp;ENI attachment times continue to take a low number of single digit minutes to establish full network connectivity within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. We do not expect much further improvement until we fully resolve the resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone, at which point ENI attachment times will return to normal levels. We expect to see full recovery within the next 30 to 60 minutes. At these ENI attachments times, ELB and Glue are seeing recovery, while other services - including EMR, EKS, ECS, and RDS - are seeing limited impact within the affected Availability Zone. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:04 PM PDT</span>&nbsp;Over the last 15 minutes, we have seen a further improvement in ENI attachment times within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. At these levels, many workflows will operate normally but some may still timeout waiting for full network connectivity to be established for a newly launched instance or newly attached ENI. The resource contention issue affecting the subsystem responsible for the propagation of network ENI mappings within the affected Availability Zone has been resolved, and network state is now beginning to propagate through the affected Availability Zone. As this happens, we would expect to see ENI attachment times return to normal levels over the next 15 - 30 minutes. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">10:39 PM PDT</span>&nbsp;While we have continued to make some progress over the last 30 minutes, progress has been slower than expected and ENI attachment times have not yet returned to normal levels within the affected Availability Zone (USE1-AZ4) in the US-EAST-1 Region. ENI network state continues to propagate through the affected Availability Zone, but is expected to take another 15 - 30 minutes before we reach normal ENI attachment times. Affected AWS services - including EMR, EKS, ECS, and RDS - continue to experience limited impact within the affected Availability Zone, and we would expect them to recover as ENI attachment times return to normal levels. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">11:27 PM PDT</span>&nbsp;ENI network state continues to propagate through the affected Availability Zone (USE1-AZ4) further reducing ENI attachment times. Several affected services - including ELB, Glue and RDS - are now seeing full recovery, while other services - including EMR, EKS, and ECS - are experiencing limited impact at this stage. While we have not yet seen ENI attachment times return to normal levels just yet, we continue to make progress in resolving the issue. Networking for existing instances within the affected Availability Zone and instances in other Availability Zones in the US-EAST-1 Region are not affected by this issue. We’ll continue to provide updates on our progress as we work on resolving the issue.</div><div><span class=\"yellowfg\">Apr 1,  1:45 AM PDT</span>&nbsp;Starting at 5:03 PM PDT, we experienced increased ENI attachment times for newly launched EC2 instances and newly attached ENIs within a single Availability Zone  (USE1-AZ4) in the US-EAST-1 Region. The issue was caused by increased resource contention in the subsystem responsible for the propagation of ENI network mappings within the affected Availability Zone. Engineers worked to identify the root cause of the resource contention and took steps to resolve it. By 7:45 PM PDT, ENI attachment times had returned to low single digit minute levels, which allowed most workflows to proceed and limited the impact to other AWS services. Some internal services, such as EMR, weighted away from the affected Availability Zone, which mitigated the impact. ENI attachment times continued to improve as the resource contention issue was addressed, and by 12:50 AM PDT, ENI attachment times had returned to normal levels. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Singapore)","summary":"[RESOLVED] Power event impacting some instances","date":"1649238391","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 2:46 AM PDT</span>&nbsp;Starting at 1:23 AM PDT, some EC2 instances experienced a loss of power and some EBS volumes experienced degraded performance within a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. Power was quickly restored to the affected instances and EBS volumes and by 1:40 AM PDT, the majority of EC2 instances and EBS volumes had fully recovered. By 2:05 AM PDT, the vast majority of affected EC2 instances and EBS volumes had fully recovered. Some RDS databases were also affected by the event, and recovered after power was restored. Customers with affected EC2 instances and EBS volumes were notified via the Personal Health Dashboard. The issue has been resolved and the service is operating normally.</div>","service":"ec2-ap-southeast-1"},{"service_name":"Amazon Elastic Compute Cloud (N. Virginia)","summary":"[RESOLVED] Increased Launch Failures","date":"1650382771","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:39 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption by Default in the US-EAST-1 Region for three Availability Zones: use1-az2, use1-az6 and use1-az5.  Existing instances are not affected, and no other launch requests are affected.  We have identified the root cause and are working towards recovery. </div><div><span class=\"yellowfg\"> 9:18 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that use EBS Encryption by Default in the US-EAST-1 Region.  Existing instances are not affected, and no other launch requests are affected. We have identified the root cause.  We are seeing recovery in use1-az5.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT.  We continue to work towards recovery in the following Availability Zones: use1-az2, use1-az6.</div><div><span class=\"yellowfg\">10:46 AM PDT</span>&nbsp;We have identified the root cause for increased error rates for new launches in the US-EAST-1 Region. The event affects instances which use EBS Encryption with unencrypted AMIs. Existing instances are not affected. The Availability Zone use1-az5 recovered at 9:05 AM PDT. We began seeing significant recovery in use-az6 at 10:20 AM PDT and are beginning to see recovery in use1-az2.</div><div><span class=\"yellowfg\">11:30 AM PDT</span>&nbsp;Between 6:35 AM and 11:00 AM PDT we experienced increased error rates for new instance launches in the US-EAST-1 Region. Existing instances were unaffected by this issue. By 9:05 AM, we had full recovery in use1-az5. By 10:20 AM, we had full recovery in use1-az6, and at 11:00 AM we had recovered the final Availability Zone, use1-az2. The issue has been resolved and the service is operating normally.</div>","service":"ec2-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Singapore)","summary":"[RESOLVED] Increased Launch Error Rates","date":"1650969049","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 3:30 AM PDT</span>&nbsp;We are investigating increased error rates for new launches that uses EBS Encryption from an unencrypted AMI in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. For a small number of volumes in the Availability Zone, increased EBS I/O latencies may also be experienced. Existing instances are not affected, and no other launch requests are affected. We have identified the root cause and are working towards recovery.</div><div><span class=\"yellowfg\"> 4:30 AM PDT</span>&nbsp;Between 1:32 AM and 3:46 AM PDT, we experienced increased error rates for new launches that used EBS volumes in a single Availability Zone (apse1-az1) in the AP-SOUTHEAST-1 Region. A small number of volumes in the Availability Zone may have experienced increased EBS I/O latencies during this time, and this issue has also been resolved for the vast majority of customers. We will be directly notifying a small subset of customers who may still be affected by increased EBS I/O latencies via the AWS Health Dashboard.</div>","service":"ec2-ap-southeast-1"},{"service_name":"Amazon Redshift (N. Virginia)","summary":"[RESOLVED] Increased Error Rates","date":"1651071794","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:03 AM PDT</span>&nbsp;We are investigating increased Amazon Redshift API and Console error rates in the US-EAST-1 Region.</div><div><span class=\"yellowfg\"> 8:58 AM PDT</span>&nbsp;We continue to investigate increased Amazon Redshift API and Console error rates in the US-EAST-1 Region that impact cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers using temporary credentials.</div><div><span class=\"yellowfg\"> 9:34 AM PDT</span>&nbsp;We are narrowing in on the root cause of the issue causing increased Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue is impacting cluster management operations and may impact customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials.</div><div><span class=\"yellowfg\">10:18 AM PDT</span>&nbsp;Between 7:12 AM and 9:52 AM PDT, we experienced increased error rates in Amazon Redshift API and Console error rates in the US-EAST-1 Region. The issue impacted cluster management operations and certain customer queries and applications using the Redshift Data API and JDBC/ODBC drivers when using temporary credentials. The issue has been resolved and the service is operating normally.</div>","service":"redshift-us-east-1"},{"service_name":"Amazon Elastic Compute Cloud (Seoul)","summary":"[RESOLVED] Network Connectivity Issues","date":"1651363256","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 5:00 PM PDT</span>&nbsp;AP-NORTHEAST-2 리전의 단일 가용 영역 (APNE2-AZ2) 에서 일부 EC2 인스턴스, RDS 데이터베이스의 연결 문제 및 일부 EBS 볼륨의 성능 저하를 조사하고 있습니다.이 문제를 해결하기 위해 노력하고 있습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우  AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We are investigating connectivity issues for some EC2 instances, RDS databases and degraded performance for some EBS volumes in a single Availability Zone (APNE2-AZ2) in the AP-NORTHEAST-2 Region. We are working to resolve the issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:11 PM PDT</span>&nbsp;태평양 표준시 기준 오후 4시 29분부터 AP-NORTHEAST-2 리전의 단일 가용 영역 내에 있는 일부 EC2 인스턴스 및 EBS 볼륨의 기본 하드웨어에 대한 전력 손실이 발생했습니다.전력은 태평양 표준시 기준 오후 4시 36분에 복원되었고, 영향을 받은 EC2 인스턴스 및 EBS 볼륨이 복구되기 시작했습니다.이 단계에서 영향을 받는 EC2 인스턴스 및 EBS 볼륨의 대부분이 복구되었으며, AWS에서는 여전히 영향을 받는 소수의 인스턴스에 대한 작업을 계속 진행하고 있습니다.일부 RDS 데이터베이스에서도 이 기간 동안 연결 문제가 발생했습니다.다중 AZ 데이터베이스는 영향을 받는 가용 영역에서 성공적으로 장애가 회복되었지만 단일 AZ 데이터베이스는 전원이 복원될 때까지 손상된 상태로 남아 있었습니다.다른 가용 영역은 이 문제의 영향을 받지 않습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support )를 통해 AWS 지원 부서에 문의하십시오. | Starting at 4:29 PM PDT, we experienced a loss of power to the underlying hardware for some EC2 instances and EBS volumes within a single Availability Zone in the AP-NORTHEAST-2 Region. Power was restored at 4:36 PM PDT, and the affected EC2 instances and EBS volumes began to recover. At this stage, the majority of affected EC2 instances and EBS volumes have recovered, and we continue to work on the small number that are still affected. Some RDS databases also experienced connectivity issues during this time period. Multi-AZ databases successfully failed away from the affected Availability Zone, but Single-AZ databases would remain impaired until the power was restored. Other Availability Zones are not affected by this issue. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 5:31 PM PDT</span>&nbsp;영향을 받는 모든 EC2 인스턴스 및 EBS 볼륨의 기본 하드웨어의 전원이 복원되었음을 확인하였습니다. EC2 인스턴스와 EBS 볼륨은 계속 복구되며, 지금은 소수의  인스턴스만 남아있습니다 (대부분 i3 인스턴스) 그리고, 영향을 받는 가용 영역에서 소수의 EBS 볼륨의 성능이 저하되었습니다.RDS Aurora를 포함한 단일 AZ RDS 데이터베이스는 영향을 받는 가용 영역에 영향을 주었지만 전원이 복원되면서 복구되고 있습니다.모든 다중 AZ RDS 데이터베이스가 영향을 받는 가용 영역에서 장애 조치가 되었습니다.소수의 단일 AZ 애플리케이션 로드 밸런서가 영향을 받는 가용 영역 내에서 패킷 손실이 증가했지만, 다른 부하 분산 트래픽은 다른 가용 영역으로 이동되었습니다.API Gateway는 영향을 받는 가용 영역 내에서 M-TLS 요청에 대한 패킷 손실이 증가했지만 이제는 완전히 복구되었습니다.AWS에서는 여전히 영향을 받는 소수의 EC2 인스턴스 및 EBS 볼륨에 대해 계속 작업하고 있습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We can confirm that power has been restored to the underlying hardware of all affected EC2 instances and EBS volumes. EC2 instances and EBS volumes continue to recover and we now have a small number of instances – mostly i3 instances – and degraded performance for a small number of EBS volumes in the affected Availability Zone. Single-AZ RDS databases – including Amazon Aurora – experienced impact in the affected Availability Zone, but are recovering as power is restored. All Multi-AZ RDS databases have failed away from the affected Availability Zone. A small number of Single-AZ Application Load Balancers experienced elevated packet loss within the affected Availability Zone, but other load balancing traffic was shifted to other Availability Zones. API Gateway experienced elevated packet loss for M-TLS requests within the affected Availability Zone, but has now fully recovered. We continue to work on the small number of EC2 instances and EBS volumes that are still affected. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div><div><span class=\"yellowfg\"> 6:00 PM PDT</span>&nbsp;영향을 받은 EC2 인스턴스 및 EBS 볼륨의 대부분을 복구했습니다.아직 성능 저하를 겪고 있는 적은수의 EBS 볼륨이 있으며, 이를 해결하기 위해 계속 노력하고 있습니다.일부 EC2 인스턴스 및 EBS 볼륨은 전력 손실 후 복구할 수 없는 하드웨어에서 호스팅되었을 수 있습니다.고객은 해당하는 EC2 인스턴스 및 EBS 볼륨에 대한 만료 통지를 받을것입니다. EC2 인스턴스, EBS 볼륨 또는 RDS 데이터베이스에서 여전히 영향이 있는 경우 영향을 받는 리소스를 다시 시작하거나 다시 생성하는 단계를 수행하는 것이 좋습니다.AWS 서비스에 대해 궁금한 점이 있거나 운영상의 문제가 있는 경우 AWS 지원 센터(https://console.aws.amazon.com/support)를 통해 AWS 지원 부서에 문의하십시오. | We have recovered the vast majority of the affected EC2 instances and EBS volumes. We have a small number of EBS volumes that are still experiencing degraded performance and continue to work to resolve them. Some EC2 instances and EBS volumes may have been hosted on hardware that was not recoverable after the loss of power. Customers will receive retirement notices for EC2 instances and EBS volumes where that is the case. If you continue to see impact for an EC2 instance, EBS volume, or RDS database, we recommend taking steps to relaunch or recreate the affected resource. If you have any questions or are experiencing any operational issue with any of our services, please contact the AWS Support department via the AWS Support Center at https://console.aws.amazon.com/support.</div>","service":"ec2-ap-northeast-2"},{"service_name":"AWS IoT Core (Oregon)","summary":"[RESOLVED] Increased API Errors and Latencies","date":"1651634279","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 8:17 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for Connect, Subscribe &amp; Publish operations in the US-WEST-2 Region.</div><div><span class=\"yellowfg\"> 8:40 PM PDT</span>&nbsp;We have identified the root cause for increased error rates and latency of Connect, Subscribe and Publish operations for new connections in the US-WEST-2 Region and are working towards resolution. A scheduled update to optimize the underlying infrastructure for the AWS IoT registry resulted in lower memory availability than required - this has been corrected, and recovery is in progress. Existing connections are not affected.</div><div><span class=\"yellowfg\"> 8:55 PM PDT</span>&nbsp;Between 7:17 PM and 8:41 PM PDT, we experienced increased error rates and latency for Connect, Subscribe, Publish and Registry operations in the US-WEST-2 Region. Existing connections were not affected by the event. The issue has been resolved and the service is operating normally.</div>","service":"awsiot-us-west-2"},{"service_name":"Amazon API Gateway (Ireland)","summary":"[RESOLVED] Increased Error Rates","date":"1652721211","status":"1","details":"","description":"<div><span class=\"yellowfg\">10:13 AM PDT</span>&nbsp;We are investigating increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Some other AWS services are also affected by this issue, which we will provide further details on shortly. We are working to determine root cause and resolve the issues.</div><div><span class=\"yellowfg\">10:31 AM PDT</span>&nbsp;We can confirm increased error rates and latencies for API Gateway in the EU-WEST-1 Region. Both the error rates and latencies are at a low enough level where retries would allow for a request to succeed. We have identified the root cause and are working to fully resolve the issue. The following AWS services are experiencing low levels of error rates as a result of this issue: Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\">10:38 AM PDT</span>&nbsp;Between 9:39 AM and 10:27 AM PDT, we experienced increased error rates and latencies for API Gateway in the EU-WEST-1 Region. All AWS services - including Amazon Certificate Manager (ACM), AppSync, Cognito, EKS, Fault Injection Simulator (FIS), Service Catalog, and Sagemaker - have now recovered. The issue has been resolved and the service is operating normally.</div>","service":"apigateway-eu-west-1"},{"service_name":"Amazon Elastic Load Balancing (N. Virginia)","summary":"[RESOLVED] Increased API errors and latencies","date":"1652830549","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 4:35 PM PDT</span>&nbsp;We are investigating increased error rates and latencies for ELB APIs in the US-EAST-1 Region. This issue does not affect traffic on running load balancers.</div><div><span class=\"yellowfg\"> 5:09 PM PDT</span>&nbsp;We continue to investigate increased error rates and latencies for ELB APIs in the US-EAST-1 Region. Traffic remains unaffected on running load balancers. In many cases, a retry of the request may succeed as some requests are still succeeding. Other AWS services that utilize these affected APIs for their own workflows may also be experiencing impact. These services have posted impact to your account events. We’ll continue to update this post as we have more information to share.</div><div><span class=\"yellowfg\"> 5:35 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for ELB APIs in the US-EAST-1 Region and have taken steps to mitigate the issue. We are now seeing recovery for the affected ELB APIs. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. We will continue to monitor until we can confirm full recovery.</div><div><span class=\"yellowfg\"> 5:39 PM PDT</span>&nbsp;Between 3:58 PM and 5:28 PM PDT, we experienced increased error rates and latencies for some ELB APIs in the US-EAST-1 Region. For the duration of the event, API error rates have remained at a level where retries are likely to succeed. Traffic to existing load balancers was not affected by this event. The issue has been resolved and the service is operating normally.</div>","service":"elb-us-east-1"},{"service_name":"Amazon Kinesis Data Streams (Stockholm)","summary":"[RESOLVED] Elevated API latencies and error rates and packetloss","date":"1652985887","status":"1","details":"","description":"<div><span class=\"yellowfg\">11:44 AM PDT</span>&nbsp;We are investigating elevated API errors and latencies and elevated packet loss in the EU-NORTH-1 Region.</div><div><span class=\"yellowfg\">12:08 PM PDT</span>&nbsp;Between 11:12 AM and 11:45 AM PDT, we experienced elevated API errors and latencies and connectivity issues in the EU-NORTH-1 Region. The issue has been resolved and the services are operating normally.</div>","service":"kinesis-eu-north-1"},{"service_name":"AWS Billing Console","summary":"[RESOLVED] Increased Errors Managing Payment Methods","date":"1653519230","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 3:53 PM PDT</span>&nbsp;Between 12:30 PM and 2:10 PM PDT we experienced increased errors managing payment methods. The issue is resolved and the service is operating normally.</div>","service":"billingconsole"},{"service_name":"Amazon Route 53","summary":"[RESOLVED] Intermittent DNS resolution failures","date":"1653618095","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 7:21 PM PDT</span>&nbsp;We are aware of intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside AWS.  DNS queries for domains hosted on Route 53 are operating without any issues at this time.  We are actively working with the third-party DNS provider to resolve the issue as quickly as possible.</div><div><span class=\"yellowfg\"> 8:07 PM PDT</span>&nbsp;We confirm intermediate DNS resolution issues for certain specific AWS names. This is due to an issue with a third-party DNS provider outside of AWS. The third-party DNS provider is working toward resolution. We are also working toward a resolution that addresses the issues the third-party provider has encountered. Queries for DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 8:42 PM PDT</span>&nbsp;We can confirm the start of recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider.  We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\"> 9:01 PM PDT</span>&nbsp;We can confirm the broad recovery in the DNS resolution issues for certain specific AWS names, as the change that caused this issue has been reverted by the third-party provider. We are continuing to work towards full recovery. Queries for the DNS records hosted on Route 53 are not affected by this issue.</div><div><span class=\"yellowfg\">10:24 PM PDT</span>&nbsp;Between 5:35 PM and 9:58 PM PDT, we experienced intermittent DNS resolution issues for certain specific AWS endpoints. We can confirm that DNS resolution issues for these AWS names have been resolved. Since DNS answers for some of these names that were affected by issues with the third-party DNS provider could have been cached on AWS DNS resolvers, we are flushing these resolver caches over the next few hours. If you run your own DNS Resolvers, and you experience DNS resolution issues we suggest to flush your DNS Resolver cache. Queries for the DNS records hosted on Route 53 were not affected by this issue.</div>","service":"route53"},{"service_name":"Amazon Elastic Load Balancing (N. Virginia)","summary":"[RESOLVED] Increased API Error Rates","date":"1654825968","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 6:52 PM PDT</span>&nbsp;We are investigating an increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. Existing load balancers are not affected by the issue. We are working to identify the root cause and resolve the issue.</div><div><span class=\"yellowfg\"> 7:03 PM PDT</span>&nbsp;We have identified the root cause of the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. This is causing increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We are working to resolve the issue and expect to see an improvement in error rates as that progresses.</div><div><span class=\"yellowfg\"> 7:54 PM PDT</span>&nbsp;We continue to make progress towards resolving the increased error rates and latencies for the ELB APIs in the US-EAST-1 Region. While error rates have stabilized, we continue to see increased provisioning times for new load balancers, as well as delays in registering new instances and targets. Connections and traffic to existing load balancers are not affected by the issue. Some other AWS services - including Elastic Container Service, Amazon Certificate Manager, and Directory Services - are also experiencing API error rates and latencies. We will continue to keep you updated on our progress.</div><div><span class=\"yellowfg\"> 8:13 PM PDT</span>&nbsp;Starting at 7:46 PM PDT we experienced higher error rates and latencies for AWS services within the US-EAST-1 region. These error rates and latencies have seen some improvement from 7:55 PM PDT, but remain elevated. The issue is also affecting the AWS Management Console for the US-EAST-1 region. We continue to work towards mitigating the impact and will continue to provide updates as we progress.</div><div><span class=\"yellowfg\"> 8:38 PM PDT</span>&nbsp;We continue to work on addressing the error rates and latencies for AWS services in the US-EAST-1 Region. The issue initially affected the Elastic Load Balancing APIs, with some impact to other services, including Elastic Container Service, Amazon Certificate Manager, and Directory Services. At 7:46 PM PDT, other AWS services began to experience an increase in error rates and latencies. Action was taken and error rates started to improve at 7:55 PM PDT. This issue also affected access to the the AWS Management Console for the US-EAST-1 Region. Error rates and latencies for services in the US-EAST-1 Region remain elevated for a number of services, including Connect, DynamoDB, SQS, SNS, EC2, CloudFormation, CloudFront, amongst others. We continue to work towards resolving the issue.</div><div><span class=\"yellowfg\"> 9:13 PM PDT</span>&nbsp;We continue to see a reduction in error rates and latencies for AWS services within the US-EAST-1 Region as we work to resolve the issue. While many AWS services are experiencing elevated error rates and latencies, services that are experiencing higher error rates have been tagged on this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. The AWS Management Console is operating normally, however some customers may observe API Errors at times. AWS Connect error rates have improved as well and we continue to work towards full recovery. We are working on applying mitigations to fully resolve the issue.</div><div><span class=\"yellowfg\"> 9:32 PM PDT</span>&nbsp;We continue to see a reduction in API error rates and latencies for services within the US-EAST-1 Region. Elastic Load Balancer APIs have recovered and returned to normal levels. The AWS Management Console has recovered. AWS Connect error rates have returned to normal levels. CloudWatch metrics have recovered and the EC2 API error rates have returned to normal levels. DynamoDB has recovered and is operating normally. Services that were affected by this event remain tagged to this Service Health Dashboard event. The event continues to affect APIs for affected services, while the EC2 network, Elastic Load Balancing and API Gateway data planes are not affected by this issue. </div><div><span class=\"yellowfg\"> 9:58 PM PDT</span>&nbsp;Starting at 6:01 PM PDT, we experienced elevated error rates and latencies for AWS services within the US-EAST-1 Region. The issue affected AWS service APIs, with no impact to data plane services such as EC2 instances, EBS volumes, or Elastic Load Balancers. We started to see recovery at 7:55 PM PDT and were fully recovered by 9:25 PM PDT. The issue has been resolved and the service is operating normally.</div>","service":"elb-us-east-1"},{"service_name":"AWS Billing Console","summary":"[RESOLVED] Incorrect Free Tier Email Alerts","date":"1655342154","status":"1","details":"","description":"<div><span class=\"yellowfg\"> 6:15 PM PDT</span>&nbsp;We are investigating customers receiving email alerts about Free Tier Usage which does not match what customers are seeing in the billing console.</div><div><span class=\"yellowfg\"> 6:39 PM PDT</span>&nbsp;Between 11:00 AM and 6:00 PM PDT, some customers received incorrect email alerts about their Free Tier Usage. This was caused by an incorrect software deployment, which has since been rolled back. During this time, the AWS Billing Console was reporting the correct information. Customers who received an incorrect email alert will receive a notification via the AWS Health Dashboard Event Log in your AWS account.</div><div><span class=\"yellowfg\"> 6:53 PM PDT</span>&nbsp;We want to clarify our previous post. Between 11:00 AM and 6:00 PM PDT, the Home page and the Bills page on the AWS Billing Console reported correct information. The Free Tier page on the AWS Billing Console displayed the same incorrect information included in the incorrect email alert.  We have corrected the issue that caused both the incorrect emails and the incorrect information in the Free Tier page on the AWS Billing Console, and they are all reporting correct information now.</div>","service":"billingconsole"}],"current":[]}