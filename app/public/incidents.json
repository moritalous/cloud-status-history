[{"id":"TaA75H7XA1DQyM2ReK2f","number":"15267780622191605299","begin":"2022-07-01T13:37:44+00:00","created":"2022-07-01T14:05:02+00:00","end":"2022-07-01T19:25:45+00:00","modified":"2022-07-01T19:25:45+00:00","external_desc":"[Google BigQuery] Lower throughput from the Read API","updates":[{"created":"2022-07-01T19:25:45+00:00","modified":"2022-07-01T19:25:46+00:00","when":"2022-07-01T19:25:45+00:00","text":"The issue with Google BigQuery has been resolved for all affected users as of Friday, 2022-07-01 12:23 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-07-01T17:48:07+00:00","modified":"2022-07-01T17:48:13+00:00","when":"2022-07-01T17:48:07+00:00","text":"Summary: [Google BigQuery] Lower throughput from the Read API\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2022-07-01 13:00 US/Pacific.\nDiagnosis: Customers may be observing lower throughput from the Read API\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-07-01T16:46:20+00:00","modified":"2022-07-01T16:46:21+00:00","when":"2022-07-01T16:46:20+00:00","text":"Summary: [Google BigQuery] Lower throughput from the Read API\nDescription: We believe our engineering team identified a potential fix and are working on rolling out the fix.\nWe do not have an ETA for mitigation at this point.\nWe will provide an update by Friday, 2022-07-01 11:00 US/Pacific with current details.\nDiagnosis: Customers may be observing lower throughput from the Read API\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-07-01T14:54:19+00:00","modified":"2022-07-01T14:54:24+00:00","when":"2022-07-01T14:54:19+00:00","text":"Summary: [Google BigQuery] Lower throughput from the Read API\nDescription: We believe our engineering team identified a potential fix and is currently testing it.\nWe do not have an ETA for test completion at this point.\nWe will provide an update by Friday, 2022-07-01 10:00 US/Pacific with current details.\nDiagnosis: Customers may be observing lower throughput from the Read API\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-07-01T14:04:53+00:00","modified":"2022-07-01T14:05:04+00:00","when":"2022-07-01T14:04:53+00:00","text":"Summary: [Google BigQuery] Lower throughput from the Read API\nDescription: We are experiencing an issue with Google BigQuery.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-07-01 07:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may be observing lower throughput from the Read API\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-07-01T19:25:45+00:00","modified":"2022-07-01T19:25:46+00:00","when":"2022-07-01T19:25:45+00:00","text":"The issue with Google BigQuery has been resolved for all affected users as of Friday, 2022-07-01 12:23 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/TaA75H7XA1DQyM2ReK2f","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"mDY7jN545et9XUV541MZ","number":"2463565635339146947","begin":"2022-06-30T18:12:00+00:00","created":"2022-06-30T18:45:06+00:00","end":"2022-07-01T02:15:28+00:00","modified":"2022-07-01T02:15:28+00:00","external_desc":"Intermittent Cloud Composer creation failures in select regions and elevated error rates during Cloud SQL instance creation.","updates":[{"created":"2022-07-01T02:15:28+00:00","modified":"2022-07-01T02:15:28+00:00","when":"2022-07-01T02:15:28+00:00","text":"We experienced an issue with Google Cloud Composer, Google Cloud SQL beginning at Thursday, 2022-06-28 09:00 US/Pacific.\nThe issue has been resolved for all affected users as of Thursday, 2022-06-30 19:13 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-07-01T00:50:56+00:00","modified":"2022-07-01T00:51:03+00:00","when":"2022-07-01T00:50:56+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions and elevated error rates during Cloud SQL instance creation.\nDescription: Mitigation work is underway by our engineering team.\nWe will provide more information by Thursday, 2022-06-30 20:00 US/Pacific.\nDiagnosis: Customers may experience latency and errors with Composer creation. Customer may face errors stating \"databases already exist\" error upon retrying the creation of Composer.\nCustomer using Cloud SQL may see failures in calls shortly after database instance creation.\nWorkaround: Customers may retry the API calls for affected instances while using Cloud SQL.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-30T19:49:08+00:00","modified":"2022-06-30T19:49:10+00:00","when":"2022-06-30T19:49:08+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions and elevated error rates during Cloud SQL instance creation.\nDescription: Our engineering team has determined that further investigation is required to mitigate the issue.\nWe will provide an update by Thursday, 2022-06-30 18:00 US/Pacific with current details.\nDiagnosis: Customers may experience latency and errors with Composer creation. Customer may face errors stating \"databases already exist\" error upon retrying the creation of Composer.\nCustomer using Cloud SQL may see failures in calls shortly after database instance creation.\nWorkaround: Customers may retry the API calls for affected instances while using Cloud SQL.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-30T19:04:16+00:00","modified":"2022-06-30T19:04:23+00:00","when":"2022-06-30T19:04:16+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions and elevated error rates during Cloud SQL instance creation.\nDescription: Mitigation work is still underway by our engineering team. We will provide more information by Thursday, 2022-06-30 13:00 US/Pacific.\nDiagnosis: Customers may experience latency and errors with Composer creation. Customer may face errors stating \"databases already exist\" error upon retrying the creation of Composer.\nCustomer using Cloud SQL may see failures in calls shortly after database instance creation.\nWorkaround: Customers may retry the API calls for affected instances while using Cloud SQL.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-30T18:45:05+00:00","modified":"2022-06-30T18:45:08+00:00","when":"2022-06-30T18:45:05+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: Mitigation work is still underway by our engineering team. We will provide more information by Thursday, 2022-06-30 13:00 US/Pacific.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]}],"most_recent_update":{"created":"2022-07-01T02:15:28+00:00","modified":"2022-07-01T02:15:28+00:00","when":"2022-07-01T02:15:28+00:00","text":"We experienced an issue with Google Cloud Composer, Google Cloud SQL beginning at Thursday, 2022-06-28 09:00 US/Pacific.\nThe issue has been resolved for all affected users as of Thursday, 2022-06-30 19:13 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Composer","id":"YxkG5FfcC42cQmvBCk4j"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"}],"uri":"incidents/mDY7jN545et9XUV541MZ","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"id":"sPyYB214xAhqnoxNrrnW","number":"8795563426724731296","begin":"2022-06-29T20:54:00+00:00","created":"2022-06-29T22:14:46+00:00","end":"2022-06-29T22:59:00+00:00","modified":"2022-06-30T18:13:20+00:00","external_desc":"Global: Apigee X is experiencing issues with integrated portals and API Monitoring.","updates":[{"created":"2022-06-30T18:13:17+00:00","modified":"2022-06-30T18:13:17+00:00","when":"2022-06-30T18:13:17+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 29 June 2022 13:54\n**Incident End:** 29 June 2022 15:59\n**Duration:** 2 hours, 5 minutes\n**Affected Services and Features:**\nGoogle Cloud Apigee Integrated Portals\n**Regions/Zones:** Global/ All Regions\n**Description:**\nGoogle Cloud Apigee experienced intermittent errors while accessing Integrated Portals. From preliminary analysis, the root cause of the issue was an SSL certificate for an internal Apigee service not properly rotated.\n**Customer Impact:**\nIntermittent errors accessing Integrated Portals","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-29T23:11:51+00:00","modified":"2022-06-29T23:11:52+00:00","when":"2022-06-29T23:11:51+00:00","text":"The issue with Apigee has been resolved for all affected users as of Wednesday, 2022-06-29 16:11 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-29T22:33:21+00:00","modified":"2022-06-29T22:33:27+00:00","when":"2022-06-29T22:33:21+00:00","text":"Summary: Global: Apigee X is experiencing issues with integrated portals and API Monitoring.\nDescription: We are experiencing an issue with Apigee X beginning at Wednesday, 2022-06-29 13:54 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2022-06-29 16:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience intermittent failures when trying to access portal pages via the UI or APIs\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-29T22:14:41+00:00","modified":"2022-06-29T22:14:47+00:00","when":"2022-06-29T22:14:41+00:00","text":"Summary: Global: Apigee X is experiencing issues with integrated portals.\nDescription: We are experiencing an issue with Apigee X beginning at Wednesday, 2022-06-29 13:54 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2022-06-29 15:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience intermittent failures when trying to access portal pages\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-30T18:13:17+00:00","modified":"2022-06-30T18:13:17+00:00","when":"2022-06-30T18:13:17+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 29 June 2022 13:54\n**Incident End:** 29 June 2022 15:59\n**Duration:** 2 hours, 5 minutes\n**Affected Services and Features:**\nGoogle Cloud Apigee Integrated Portals\n**Regions/Zones:** Global/ All Regions\n**Description:**\nGoogle Cloud Apigee experienced intermittent errors while accessing Integrated Portals. From preliminary analysis, the root cause of the issue was an SSL certificate for an internal Apigee service not properly rotated.\n**Customer Impact:**\nIntermittent errors accessing Integrated Portals","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9Y13BNFy4fJydvjdsN3X","service_name":"Apigee","affected_products":[{"title":"Apigee","id":"9Y13BNFy4fJydvjdsN3X"}],"uri":"incidents/sPyYB214xAhqnoxNrrnW","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"V3AJNVDysYF1EXttKC3N","number":"6035749805300313659","begin":"2022-06-29T18:29:36+00:00","created":"2022-06-29T18:29:38+00:00","end":"2022-06-30T18:42:09+00:00","modified":"2022-06-30T18:42:09+00:00","external_desc":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","updates":[{"created":"2022-06-30T18:42:09+00:00","modified":"2022-06-30T18:42:11+00:00","when":"2022-06-30T18:42:09+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-30T16:57:20+00:00","modified":"2022-06-30T16:57:28+00:00","when":"2022-06-30T16:57:20+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Thursday, 2022-06-30 13:00 US/Pacific.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-30T13:22:58+00:00","modified":"2022-06-30T13:23:08+00:00","when":"2022-06-30T13:22:58+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Thursday, 2022-06-30 10:00 US/Pacific.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-29T22:53:32+00:00","modified":"2022-06-29T22:53:33+00:00","when":"2022-06-29T22:53:32+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: Engineering has identified the root cause and mitigation work is currently underway.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-06-30 07:00 US/Pacific.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-29T22:43:04+00:00","modified":"2022-06-29T22:43:04+00:00","when":"2022-06-29T22:43:04+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: We are experiencing an intermittent issue with Google Cloud Composer creation failures due to latency for CloudSQL Database creations.\nOur engineering team continues to investigate the issue and root cause.\nWe will provide an update by Wednesday, 2022-06-29 19:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-29T20:53:26+00:00","modified":"2022-06-29T20:53:29+00:00","when":"2022-06-29T20:53:26+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: We are experiencing an intermittent issue with Google Cloud Composer creation failures due to latency for CloudSQL Database creations.\nOur engineering team continues to investigate the issue and root cause.\nWe will provide an update by Wednesday, 2022-06-29 16:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-29T18:47:44+00:00","modified":"2022-06-29T18:47:46+00:00","when":"2022-06-29T18:47:44+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: We are experiencing an intermittent issue with Google Cloud Composer creation failures due to latency for CloudSQL Database creations.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2022-06-29 14:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-29T18:29:37+00:00","modified":"2022-06-29T18:29:40+00:00","when":"2022-06-29T18:29:37+00:00","text":"Summary: Intermittent Cloud Composer creation failures in select regions\nDescription: We are experiencing an intermittent issue with Google Cloud Composer creation failures due to latency for CloudSQL Database creations.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2022-06-29 12:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience latency and errors with Composer creation.\nWorkaround: Customers may retry the Composer creation operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]}],"most_recent_update":{"created":"2022-06-30T18:42:09+00:00","modified":"2022-06-30T18:42:11+00:00","when":"2022-06-30T18:42:09+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"YxkG5FfcC42cQmvBCk4j","service_name":"Google Cloud Composer","affected_products":[{"title":"Google Cloud Composer","id":"YxkG5FfcC42cQmvBCk4j"}],"uri":"incidents/V3AJNVDysYF1EXttKC3N","currently_affected_locations":[],"previously_affected_locations":[{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"id":"pXUM8jeE61fpp3VXz5sr","number":"14057655586445957884","begin":"2022-06-28T16:08:29+00:00","created":"2022-06-28T16:46:51+00:00","end":"2022-06-28T17:04:17+00:00","modified":"2022-06-28T17:04:18+00:00","external_desc":"Elevated packet loss in europe-north1","updates":[{"created":"2022-06-28T17:04:11+00:00","modified":"2022-06-28T17:04:19+00:00","when":"2022-06-28T17:04:11+00:00","text":"The issue with Google Cloud Networking, Hybrid Connectivity has been resolved for all affected users as of Tuesday, 2022-06-28 09:55 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Finland (europe-north1)","id":"europe-north1"}]},{"created":"2022-06-28T16:46:45+00:00","modified":"2022-06-28T16:46:53+00:00","when":"2022-06-28T16:46:45+00:00","text":"Summary: Elevated packet loss in europe-north1\nDescription: We are experiencing an issue with Google Cloud Networking.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-28 10:20 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience packet loss.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Finland (europe-north1)","id":"europe-north1"}]}],"most_recent_update":{"created":"2022-06-28T17:04:11+00:00","modified":"2022-06-28T17:04:19+00:00","when":"2022-06-28T17:04:11+00:00","text":"The issue with Google Cloud Networking, Hybrid Connectivity has been resolved for all affected users as of Tuesday, 2022-06-28 09:55 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Finland (europe-north1)","id":"europe-north1"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/pXUM8jeE61fpp3VXz5sr","currently_affected_locations":[],"previously_affected_locations":[{"title":"Finland (europe-north1)","id":"europe-north1"}]},{"id":"mbtvSs2qKE3Nz6nxR6rY","number":"3478893343316437994","begin":"2022-06-28T09:55:31+00:00","created":"2022-06-28T10:20:33+00:00","end":"2022-06-28T10:25:35+00:00","modified":"2022-06-28T10:25:35+00:00","external_desc":"We are experiencing an issue with Hybrid Connectivity specifically in Geneva.","updates":[{"created":"2022-06-28T10:25:30+00:00","modified":"2022-06-28T10:25:35+00:00","when":"2022-06-28T10:25:30+00:00","text":"The issue with Hybrid Connectivity has been resolved for all affected users as of Tuesday, 2022-06-28 03:02 US/Pacific. The impact was limited to Cloud Interconnect customers peering in Equinix GV2, Geneva West.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-06-28T10:20:32+00:00","modified":"2022-06-28T10:20:33+00:00","when":"2022-06-28T10:20:32+00:00","text":"Summary: We are experiencing an issue with Hybrid Connectivity specifically in Geneva\nDescription: We are experiencing an issue with Hybrid Connectivity beginning at Tuesday, 2022-06-28 02:31 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-28 04:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers might experience packet loss of 30-80%\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]}],"most_recent_update":{"created":"2022-06-28T10:25:30+00:00","modified":"2022-06-28T10:25:35+00:00","when":"2022-06-28T10:25:30+00:00","text":"The issue with Hybrid Connectivity has been resolved for all affected users as of Tuesday, 2022-06-28 03:02 US/Pacific. The impact was limited to Cloud Interconnect customers peering in Equinix GV2, Geneva West.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/mbtvSs2qKE3Nz6nxR6rY","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"6ocKuGf4LTJTjaB84Ms4","number":"12615573420888182146","begin":"2022-06-27T22:33:40+00:00","created":"2022-06-27T22:54:59+00:00","end":"2022-06-28T14:04:37+00:00","modified":"2022-06-28T14:04:38+00:00","external_desc":"Cloud Function 2nd Generation deletes were erroring out in multiple regions.","updates":[{"created":"2022-06-28T14:04:35+00:00","modified":"2022-06-28T14:04:40+00:00","when":"2022-06-28T14:04:35+00:00","text":"The issue with Google Cloud Functions has been resolved for all affected users as of Tuesday, 2022-06-28 01:11 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-28T10:30:11+00:00","modified":"2022-06-28T10:30:12+00:00","when":"2022-06-28T10:30:11+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-06-28 09:30 US/Pacific.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-28T08:59:43+00:00","modified":"2022-06-28T08:59:43+00:00","when":"2022-06-28T08:59:43+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-06-28 04:30 US/Pacific.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-28T04:36:01+00:00","modified":"2022-06-28T04:36:02+00:00","when":"2022-06-28T04:36:01+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-06-28 02:00 US/Pacific.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-28T02:41:12+00:00","modified":"2022-06-28T02:41:13+00:00","when":"2022-06-28T02:41:12+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2022-06-27 22:00 US/Pacific.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-28T02:16:35+00:00","modified":"2022-06-28T02:16:35+00:00","when":"2022-06-28T02:16:35+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2022-06-27 20:00 US/Pacific.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-28T00:32:38+00:00","modified":"2022-06-28T00:32:39+00:00","when":"2022-06-28T00:32:38+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2022-06-27 19:17 US/Pacific.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-27T23:46:21+00:00","modified":"2022-06-27T23:46:22+00:00","when":"2022-06-27T23:46:21+00:00","text":"Summary: Cloud Function 2nd Generation deletes are erroring out in multiple regions.\nDescription: We are experiencing an issue impacting Google Cloud Functions 2nd Generation beginning at Thursday, 2022-06-23 18:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-06-27 17:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-27T23:26:49+00:00","modified":"2022-06-27T23:26:55+00:00","when":"2022-06-27T23:26:49+00:00","text":"Summary: Cloud Function deletes are erroring out in multiple regions.\nDescription: We are experiencing an issue with Google Cloud Functions beginning at Thursday, 2022-06-23 18:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-06-27 17:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-27T22:54:53+00:00","modified":"2022-06-27T22:55:00+00:00","when":"2022-06-27T22:54:53+00:00","text":"Summary: Cloud Function deletes are erroring out in multiple regions.\nDescription: We are experiencing an issue with Google Cloud Functions beginning at Thursday, 2022-06-23 18:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-06-27 16:33 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Delete function operations are erroring with following message although the corresponding resources are getting deleted and customers are able to reuse the function name\nError message:\nOperation Failed. Public Error Status: code: 5 message: Delete package operation for projects/[PROJECT_ID]/locations/[REGION]/repositories/gcf-artifacts/packages/[FUNCTION_ID]%2Fcustom-run has failed. Additional information: Requested entity was not found.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]}],"most_recent_update":{"created":"2022-06-28T14:04:35+00:00","modified":"2022-06-28T14:04:40+00:00","when":"2022-06-28T14:04:35+00:00","text":"The issue with Google Cloud Functions has been resolved for all affected users as of Tuesday, 2022-06-28 01:11 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"oW4vJ7VNqyxTWNzSHopX","service_name":"Google Cloud Functions","affected_products":[{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"}],"uri":"incidents/6ocKuGf4LTJTjaB84Ms4","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"id":"fh6jfua8BdRbCkwTegBy","number":"8449268330911625257","begin":"2022-06-24T10:37:31+00:00","created":"2022-06-24T11:12:42+00:00","end":"2022-06-29T08:56:17+00:00","modified":"2022-06-29T08:56:17+00:00","external_desc":"Cloud Redis BASIC Tier Instances cannot proceed version upgrade after their maintenance or capacity update","updates":[{"created":"2022-06-29T08:56:12+00:00","modified":"2022-06-29T08:56:19+00:00","when":"2022-06-29T08:56:12+00:00","text":"We experienced an intermittent issue with Cloud Memorystore beginning at Wednesday, 2022-06-22 21:42 US/Pacific.\nThe issue has been resolved for all affected projects as of Wednesday, 2022-06-29 01:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-24T11:12:39+00:00","modified":"2022-06-24T11:12:45+00:00","when":"2022-06-24T11:12:39+00:00","text":"Summary: Cloud Redis BASIC Tier Instances cannot proceed version upgrade after their maintenance or capacity update\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Friday, 2022-07-01 06:00 US/Pacific.\nWe will provide more information by Wednesday, 2022-06-29 06:00 US/Pacific.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Once the Basic Tier Instance went through a maintenance or a capacity update, it will fail all the version upgrade requests due to internal error (with code 13).\nVersion upgrade behavior can be found under: https://cloud.google.com/memorystore/docs/redis/version-upgrade-behavior\nWorkaround: None at this time","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-29T08:56:12+00:00","modified":"2022-06-29T08:56:19+00:00","when":"2022-06-29T08:56:12+00:00","text":"We experienced an intermittent issue with Cloud Memorystore beginning at Wednesday, 2022-06-22 21:42 US/Pacific.\nThe issue has been resolved for all affected projects as of Wednesday, 2022-06-29 01:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"LGPLu3M5pcUAKU1z6eP3","service_name":"Cloud Memorystore","affected_products":[{"title":"Cloud Memorystore","id":"LGPLu3M5pcUAKU1z6eP3"}],"uri":"incidents/fh6jfua8BdRbCkwTegBy","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"drLHg29pR5QHUvr5buDq","number":"17340949811212957340","begin":"2022-06-23T07:13:00+00:00","created":"2022-06-23T11:02:49+00:00","end":"2022-06-23T13:36:00+00:00","modified":"2022-06-24T21:44:18+00:00","external_desc":"vSphere vCenter and NSX may not have been available for a short period of time in us-central1","updates":[{"created":"2022-06-23T21:57:00+00:00","modified":"2022-06-23T21:57:00+00:00","when":"2022-06-23T21:57:00+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 23 June 2022 00:13 PT\n**Incident End:** 23 June 2022 06:36 PT\n**Duration:** 6 hours, 23 minutes\n**Affected Services and Features:**\nVMWare Engine\n**Regions/Zones:** us-central1a\n**Description:**\nGoogle Cloud customers using VMWare Engine experienced connectivity issues while using vSphere, vCenter and NSX in us-central1a zone for a duration of 6 hours, 23 minutes.\nFrom preliminary analysis, the issue was caused due to a network switch-over not working as expected during a planned maintenance window of ToR switches. The issue was mitigated by performing a file system recovery and restart of related services on the resources hosted by Google Cloud VMware Engine.\n**Customer Impact:**\n- Affected Customers may have observed issues with network partitions and accessing workload VMs.\n- Affected Customers may have experienced connectivity issues using vCenter, HCX, NSX Manager, and NSX-T Edge virtual machines.\n**Additional details:**\nGoogle engineers have paused the maintenance upgrade and also all redundant devices have been brought back to operational state. We are also working with the impacted customers to check the state of remaining workload VMs and shall assist in recovery if required.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-23T13:36:43+00:00","modified":"2022-06-23T13:36:57+00:00","when":"2022-06-23T13:36:43+00:00","text":"The issue with VMWare engine has been resolved for all affected projects as of Thursday, 2022-06-23 06:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-23T11:02:40+00:00","modified":"2022-06-23T11:02:52+00:00","when":"2022-06-23T11:02:40+00:00","text":"Summary: vSphere vCenter and NSX may not have been available for a short period of time in us-central1\nDescription: We are experiencing an issue with VMWare engine beginning at Thursday, 2022-06-23 07:13 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-23 09:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: vSphere vCenter and NSX may not have been available for a short period of time in us-central1, vCenter VMs has been recovered. Some of the Workload VMs might be still impacted.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-06-23T21:57:00+00:00","modified":"2022-06-23T21:57:00+00:00","when":"2022-06-23T21:57:00+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 23 June 2022 00:13 PT\n**Incident End:** 23 June 2022 06:36 PT\n**Duration:** 6 hours, 23 minutes\n**Affected Services and Features:**\nVMWare Engine\n**Regions/Zones:** us-central1a\n**Description:**\nGoogle Cloud customers using VMWare Engine experienced connectivity issues while using vSphere, vCenter and NSX in us-central1a zone for a duration of 6 hours, 23 minutes.\nFrom preliminary analysis, the issue was caused due to a network switch-over not working as expected during a planned maintenance window of ToR switches. The issue was mitigated by performing a file system recovery and restart of related services on the resources hosted by Google Cloud VMware Engine.\n**Customer Impact:**\n- Affected Customers may have observed issues with network partitions and accessing workload VMs.\n- Affected Customers may have experienced connectivity issues using vCenter, HCX, NSX Manager, and NSX-T Edge virtual machines.\n**Additional details:**\nGoogle engineers have paused the maintenance upgrade and also all redundant devices have been brought back to operational state. We are also working with the impacted customers to check the state of remaining workload VMs and shall assist in recovery if required.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"VMWare engine","id":"9H6gWUHvb2ZubeoxzQ1Y"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"}],"uri":"incidents/drLHg29pR5QHUvr5buDq","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"58JrPVZpPTo1S6xJoiMj","number":"10942582489061747224","begin":"2022-06-21T10:22:58+00:00","created":"2022-06-21T12:21:50+00:00","end":"2022-06-21T15:05:39+00:00","modified":"2022-06-21T15:05:40+00:00","external_desc":"Cloud Router route priorities being wrong","updates":[{"created":"2022-06-21T15:05:37+00:00","modified":"2022-06-21T15:05:45+00:00","when":"2022-06-21T15:05:37+00:00","text":"The issue with Hybrid Connectivity has been resolved for all affected users as of Tuesday, 2022-06-21 07:45 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-21T13:48:26+00:00","modified":"2022-06-21T13:48:33+00:00","when":"2022-06-21T13:48:26+00:00","text":"Summary: Cloud Router route priorities being wrong\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2022-06-21 09:00 US/Pacific.\nWe will provide more information by Tuesday, 2022-06-21 09:00 US/Pacific.\nDiagnosis: Route priorities between regions can differ between HA0 and HA1 attachments within GCP and thus traffic might flow in unexpected way\nWorkaround: Adjust route priorities or shutdown secondary paths","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-21T13:00:32+00:00","modified":"2022-06-21T13:00:43+00:00","when":"2022-06-21T13:00:32+00:00","text":"Summary: Cloud Router route priorities being wrong\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Tuesday, 2022-06-21 07:00 US/Pacific.\nDiagnosis: Route priorities between regions can differ between HA0 and HA1 attachments within GCP and thus traffic might flow in unexpected way\nWorkaround: Adjust route priorities or shutdown secondary paths","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-21T12:21:41+00:00","modified":"2022-06-21T12:21:52+00:00","when":"2022-06-21T12:21:41+00:00","text":"Summary: Cloud Router route priorities being wrong\nDescription: We are experiencing an issue with Hybrid Connectivity beginning at Friday, 2022-06-17 17:15 US/Pacific\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-21 06:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Route priorities between regions can differ between HA0 and HA1 attachments within GCP and thus traffic might flow in unexpected way\nWorkaround: Adjust route priorities or shutdown secondary paths","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-21T15:05:37+00:00","modified":"2022-06-21T15:05:45+00:00","when":"2022-06-21T15:05:37+00:00","text":"The issue with Hybrid Connectivity has been resolved for all affected users as of Tuesday, 2022-06-21 07:45 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/58JrPVZpPTo1S6xJoiMj","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"DXiQQW2ipyeeNsq9RFxz","number":"11865608804010450284","begin":"2022-06-19T19:01:38+00:00","created":"2022-06-19T19:03:51+00:00","end":"2022-06-19T21:27:53+00:00","modified":"2022-06-20T11:52:17+00:00","external_desc":"Google engineer are currently investigating a issue with the cloud networking product","updates":[{"created":"2022-06-20T11:52:17+00:00","modified":"2022-06-20T11:52:17+00:00","when":"2022-06-20T11:52:17+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 19 June 2022 10:14\n**Incident End:** 19 June 2022 13:39\n**Duration:** 3 hours, 25 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking.\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Networking experienced an issue with the Cloud Armour config changes that got stalled for a duration of 3 hours, 25 minutes. From preliminary analysis, the root cause of the issue is Google Cloud Armor Configuration Server experienced errors in writing configs to the config storage system.\n**Customer Impact:**\nCustomer configs propagation depending on the dos_cloud configs got delayed.\nSome impacted customers would not be able to make Cloud Armor config changes.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T21:27:52+00:00","modified":"2022-06-19T21:27:57+00:00","when":"2022-06-19T21:27:52+00:00","text":"The issue with Google Cloud Networking has been resolved for all affected projects as of Sunday, 2022-06-19 14:27 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T20:31:14+00:00","modified":"2022-06-19T20:31:21+00:00","when":"2022-06-19T20:31:14+00:00","text":"Summary: Google engineer are currently investigating a issue with the cloud networking product\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Sunday, 2022-06-19 14:30 US/Pacific.\nDiagnosis: Customer configs propagation depending on the dos_cloud configs is delayed.\nCustomer will be able to make cloud armour config changes but expect higher latency for backlog to clear\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T20:30:47+00:00","modified":"2022-06-19T20:30:51+00:00","when":"2022-06-19T20:30:47+00:00","text":"Summary: Google engineer are currently investigating a issue with the cloud networking product\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Sunday, 2022-06-19 14:30 US/Pacific.\nDiagnosis: Customer configs propagation depending on the dos_cloud configs is delayed.\nCustomer will be able to make cloud armour config changes but expect higher latency for backlog to clear\nCustomer configs propagation depending on the dos_cloud configs is delayed.\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T20:00:08+00:00","modified":"2022-06-19T20:00:18+00:00","when":"2022-06-19T20:00:08+00:00","text":"Summary: Google engineer are currently investigating a issue with the cloud networking product\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Sunday, 2022-06-19 13:40 US/Pacific.\nDiagnosis: Customer are unable to make cloud armour config changes globally.\nCustomer configs propagation depending on the dos_cloud configs is delayed.\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T19:47:37+00:00","modified":"2022-06-19T19:47:50+00:00","when":"2022-06-19T19:47:37+00:00","text":"Summary: Google engineer are currently investigating a issue with the cloud networking product\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Sunday, 2022-06-19 00:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-06-19 13:35 US/Pacific with current details.\nDiagnosis: Customer are unable to make cloud armour config changes globally.\nCustomer configs propagation depending on the dos_cloud configs is delayed.\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T19:10:21+00:00","modified":"2022-06-19T19:10:26+00:00","when":"2022-06-19T19:10:21+00:00","text":"Summary: Google engineer are currently investigating a issue with the cloud networking product\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Sunday, 2022-06-19 00:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-06-19 13:35 US/Pacific with current details.\nDiagnosis: Customer are unable to make cloud armour config changes globally.\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-19T19:03:43+00:00","modified":"2022-06-19T19:03:55+00:00","when":"2022-06-19T19:03:43+00:00","text":"Summary: Google engineer are currently investigating a issue with the cloud networking product\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Sunday, 2022-06-19 00:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-06-19 13:35 US/Pacific with current details.\nDiagnosis: Customer are unable to make cloud amour config changes globally.\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-20T11:52:17+00:00","modified":"2022-06-20T11:52:17+00:00","when":"2022-06-20T11:52:17+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 19 June 2022 10:14\n**Incident End:** 19 June 2022 13:39\n**Duration:** 3 hours, 25 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking.\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Networking experienced an issue with the Cloud Armour config changes that got stalled for a duration of 3 hours, 25 minutes. From preliminary analysis, the root cause of the issue is Google Cloud Armor Configuration Server experienced errors in writing configs to the config storage system.\n**Customer Impact:**\nCustomer configs propagation depending on the dos_cloud configs got delayed.\nSome impacted customers would not be able to make Cloud Armor config changes.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/DXiQQW2ipyeeNsq9RFxz","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"sY4rnjxM4pSudhn6LZUA","number":"7149754702357146867","begin":"2022-06-16T18:14:59+00:00","created":"2022-06-16T18:26:23+00:00","end":"2022-06-16T23:25:47+00:00","modified":"2022-06-16T23:25:47+00:00","external_desc":"[Global] Cloud Spanner backups restore operations may be delayed.","updates":[{"created":"2022-06-16T23:25:47+00:00","modified":"2022-06-16T23:25:48+00:00","when":"2022-06-16T23:25:47+00:00","text":"The issue with Cloud Spanner latencies on backup restore operations has been resolved for all affected projects as of Thursday, 2022-06-16 15:40 US/Pacific.\nIf you have further questions, or need immediate assistance please open a case with the support team and we will work with you.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T19:51:43+00:00","modified":"2022-06-16T19:51:44+00:00","when":"2022-06-16T19:51:43+00:00","text":"Summary: [Global] Cloud Spanner backups restore operations may be delayed up to 3 Hours\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-06-16 17:00 US/Pacific.\nDiagnosis: Cloud Spanner Backups Restore Operations May Be Delayed Up to 3 Hours\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T19:50:48+00:00","modified":"2022-06-16T19:50:49+00:00","when":"2022-06-16T19:50:48+00:00","text":"Summary: [Global] Cloud Spanner backups restore operations may be delayed up to 3 Hours\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-06-16 15:00 US/Pacific.\nDiagnosis: Cloud Spanner Backups Restore Operations May Be Delayed Up to 3 Hours\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T18:26:22+00:00","modified":"2022-06-16T18:26:23+00:00","when":"2022-06-16T18:26:22+00:00","text":"Summary: [Global] Cloud Spanner backups restore operations may be delayed up to 3 Hours\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-06-16 13:00 US/Pacific.\nDiagnosis: Cloud Spanner Backups Restore Operations May Be Delayed Up to 3 Hours\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-16T23:25:47+00:00","modified":"2022-06-16T23:25:48+00:00","when":"2022-06-16T23:25:47+00:00","text":"The issue with Cloud Spanner latencies on backup restore operations has been resolved for all affected projects as of Thursday, 2022-06-16 15:40 US/Pacific.\nIf you have further questions, or need immediate assistance please open a case with the support team and we will work with you.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"EcNGGUgBtBLrtm4mWvqC","service_name":"Cloud Spanner","affected_products":[{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"}],"uri":"incidents/sY4rnjxM4pSudhn6LZUA","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"iAgVMxJUfE1J9et7j9KB","number":"13909014623288227313","begin":"2022-06-16T08:11:00+00:00","created":"2022-06-16T09:46:24+00:00","end":"2022-06-16T09:44:00+00:00","modified":"2022-06-17T00:10:46+00:00","external_desc":"Google Cloud Networking packet loss issue","updates":[{"created":"2022-06-17T00:10:38+00:00","modified":"2022-06-17T00:10:38+00:00","when":"2022-06-17T00:10:38+00:00","text":"# MINI INCIDENT REPORT\nWe apologize for the inconvenience this service disruption/outage caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 16 June 2022 01:11\n**Incident End:** 16 June 2022 02:44\n**Duration:** 1 hour, 33 minutes\n**Affected Services and Features:**\n- Google Cloud Networking\n- Cloud Monitoring\n- Cloud NAT\n- Hybrid Connectivity\n- Cloud Run\n- Cloud Interconnect\n- Virtual Private Cloud (VPC)\n- Gmail\n- Cloud Bigtable\n**Regions/Zones:** Europe and Asia\n**Description:**\nGoogle Cloud Networking experienced elevated packet loss and latency with Google Cloud Services across Europe and Asia for a duration of 1 hour, 36 minutes. Any service that uses Cloud Networking may have observed impact. We have included available details of service specific impact below; however, this may not be a comprehensive accounting of all downstream networking impact.\nFrom preliminary analysis, the root cause of the issue was due to a large reconvergence event on our user facing backbone, which was triggered by a typical fiber cut in North America. The failure resulted in packet loss due to either some MPLS tunnels going down, or due to congestion on remaining tunnels that quickly grew to accommodate demand on next-best paths. Tunnels going down is not expected behavior of MPLS control plane in response to best-path capacity going down when adjacent capacity remains available and is being investigated with highest priority with the hardware vendor. Mitigation of the packet loss required Google engineers to manually intervene on the network control plane. The underlying Cloud Networking impact ended at 02:58, however, some Cloud Services may have taken longer to recover.\nDue to backbone control plane behavior during this event, network forwarding may have been suboptimal for some destinations resulting in higher network latency.\n**Customer Impact:**\n- **Google Cloud Networking** - Public IP traffic connectivity failed from 01:22 to 02:58 US/Pacific.\n- **Cloud Monitoring** - Affected customers observed metric data failing to publish in europe-north1.\n- **Cloud NAT** - Affected customers experienced packet loss on some flows between regions in Europe, South America, and North America.\n- **Hybrid Connectivity** - Affected customers experienced packet loss on some flows between regions in Europe, South America, and North America.\n- **Cloud Run** - Affected customers experienced packet loss in europe-north1.\n- **Cloud Interconnect** - Affected customers experienced packet loss on some flows between regions in Europe, South America, and North America.\n- **Virtual Private Cloud (VPC)** - Affected customers experienced up to 100% packet loss with traffic between Europe and Asia over private and public IP.\n- **Cloud Bigtable** - Affected customers experienced elevated latency with read and write operations in europe-north1.\n- **Gmail** - Affected web client users experienced transient service unavailability.\n**Additional details:**\nFiber cuts on the Google network are quite common with no observable malfunction to the MPLS control plane. This incident surfaces a novel MPLS control plane behavior. We are committed to fully identifying the root cause and addressing it to prevent it with our partners.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Finland (europe-north1)","id":"europe-north1"}]},{"created":"2022-06-16T10:03:47+00:00","modified":"2022-06-16T10:03:57+00:00","when":"2022-06-16T10:03:47+00:00","text":"The issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) has been resolved for all affected projects as of Thursday, 2022-06-16 03:02 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Finland (europe-north1)","id":"europe-north1"}]},{"created":"2022-06-16T09:46:13+00:00","modified":"2022-06-16T09:46:26+00:00","when":"2022-06-16T09:46:13+00:00","text":"Summary: Google Cloud Networking packet loss issue\nDescription: We are experiencing an issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) beginning at Thursday, 2022-06-16 01:11 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-16 03:15 US/Pacific with current details.\nDiagnosis: The customers may experience persistent packet loss for some connection flows\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Finland (europe-north1)","id":"europe-north1"}]}],"most_recent_update":{"created":"2022-06-17T00:10:38+00:00","modified":"2022-06-17T00:10:38+00:00","when":"2022-06-17T00:10:38+00:00","text":"# MINI INCIDENT REPORT\nWe apologize for the inconvenience this service disruption/outage caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 16 June 2022 01:11\n**Incident End:** 16 June 2022 02:44\n**Duration:** 1 hour, 33 minutes\n**Affected Services and Features:**\n- Google Cloud Networking\n- Cloud Monitoring\n- Cloud NAT\n- Hybrid Connectivity\n- Cloud Run\n- Cloud Interconnect\n- Virtual Private Cloud (VPC)\n- Gmail\n- Cloud Bigtable\n**Regions/Zones:** Europe and Asia\n**Description:**\nGoogle Cloud Networking experienced elevated packet loss and latency with Google Cloud Services across Europe and Asia for a duration of 1 hour, 36 minutes. Any service that uses Cloud Networking may have observed impact. We have included available details of service specific impact below; however, this may not be a comprehensive accounting of all downstream networking impact.\nFrom preliminary analysis, the root cause of the issue was due to a large reconvergence event on our user facing backbone, which was triggered by a typical fiber cut in North America. The failure resulted in packet loss due to either some MPLS tunnels going down, or due to congestion on remaining tunnels that quickly grew to accommodate demand on next-best paths. Tunnels going down is not expected behavior of MPLS control plane in response to best-path capacity going down when adjacent capacity remains available and is being investigated with highest priority with the hardware vendor. Mitigation of the packet loss required Google engineers to manually intervene on the network control plane. The underlying Cloud Networking impact ended at 02:58, however, some Cloud Services may have taken longer to recover.\nDue to backbone control plane behavior during this event, network forwarding may have been suboptimal for some destinations resulting in higher network latency.\n**Customer Impact:**\n- **Google Cloud Networking** - Public IP traffic connectivity failed from 01:22 to 02:58 US/Pacific.\n- **Cloud Monitoring** - Affected customers observed metric data failing to publish in europe-north1.\n- **Cloud NAT** - Affected customers experienced packet loss on some flows between regions in Europe, South America, and North America.\n- **Hybrid Connectivity** - Affected customers experienced packet loss on some flows between regions in Europe, South America, and North America.\n- **Cloud Run** - Affected customers experienced packet loss in europe-north1.\n- **Cloud Interconnect** - Affected customers experienced packet loss on some flows between regions in Europe, South America, and North America.\n- **Virtual Private Cloud (VPC)** - Affected customers experienced up to 100% packet loss with traffic between Europe and Asia over private and public IP.\n- **Cloud Bigtable** - Affected customers experienced elevated latency with read and write operations in europe-north1.\n- **Gmail** - Affected web client users experienced transient service unavailability.\n**Additional details:**\nFiber cuts on the Google network are quite common with no observable malfunction to the MPLS control plane. This incident surfaces a novel MPLS control plane behavior. We are committed to fully identifying the root cause and addressing it to prevent it with our partners.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Finland (europe-north1)","id":"europe-north1"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud NAT","id":"hCNpnTQHkUCCGxJy35Yq"}],"uri":"incidents/iAgVMxJUfE1J9et7j9KB","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Finland (europe-north1)","id":"europe-north1"}]},{"id":"Lvd7FBrPJmL7oxhqrDwt","number":"15199014936264361317","begin":"2022-06-14T18:40:51+00:00","created":"2022-06-14T18:55:08+00:00","end":"2022-06-15T00:22:31+00:00","modified":"2022-06-15T00:22:31+00:00","external_desc":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/fc7GCA6kAgnBihezUAkx","updates":[{"created":"2022-06-15T00:22:30+00:00","modified":"2022-06-15T00:22:32+00:00","when":"2022-06-15T00:22:30+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/fc7GCA6kAgnBihezUAkx","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T21:38:08+00:00","modified":"2022-06-14T21:38:09+00:00","when":"2022-06-14T21:38:08+00:00","text":"Summary: Customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nDescription: We are experiencing an issue with Cloud Filestore beginning at Tuesday, 2022-06-14 08:02 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 18:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nWorkaround: Affected customers can create instances in another region.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T20:43:19+00:00","modified":"2022-06-14T20:43:20+00:00","when":"2022-06-14T20:43:19+00:00","text":"Summary: Customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nDescription: We are experiencing an issue with Cloud Filestore beginning at Tuesday, 2022-06-14 08:02 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 14:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nWorkaround: Affected customers can create instances in another region.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T19:44:29+00:00","modified":"2022-06-14T19:44:30+00:00","when":"2022-06-14T19:44:29+00:00","text":"Summary: Customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nDescription: We are experiencing an issue with Cloud Filestore beginning at Tuesday, 2022-06-14 08:02 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 13:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nWorkaround: Affected customers can create instances in another region.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T18:57:39+00:00","modified":"2022-06-14T18:57:40+00:00","when":"2022-06-14T18:57:39+00:00","text":"Summary: Customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nDescription: We are experiencing an issue with Cloud Filestore beginning at Tuesday, 2022-06-14 08:02 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 13:01 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nWorkaround: Affected customers can create instances in another region.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T18:55:02+00:00","modified":"2022-06-14T18:55:08+00:00","when":"2022-06-14T18:55:02+00:00","text":"Summary: Customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nDescription: We are experiencing an issue with Cloud Filestore beginning at Tuesday, 2022-06-14 08:02 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 13:01 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create Cloud Filestore High Scale and Enterprise instances in us-central1\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-06-15T00:22:30+00:00","modified":"2022-06-15T00:22:32+00:00","when":"2022-06-15T00:22:30+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/fc7GCA6kAgnBihezUAkx","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"jog4nyYkquiLeSK5s26q","service_name":"Cloud Filestore","affected_products":[{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"}],"uri":"incidents/Lvd7FBrPJmL7oxhqrDwt","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"fc7GCA6kAgnBihezUAkx","number":"16466106385248773020","begin":"2022-06-14T13:56:00+00:00","created":"2022-06-15T00:09:03+00:00","end":"2022-06-15T00:43:00+00:00","modified":"2022-06-28T18:46:43+00:00","external_desc":"We are experiencing an issue with Google Cloud Networking at us-central1 across multiple products, beginning at Tuesday, 2022-06-14 06:00 US/Pacific.","updates":[{"created":"2022-06-28T18:27:16+00:00","modified":"2022-06-28T18:44:58+00:00","when":"2022-06-28T18:27:16+00:00","text":"# Incident Report\n**Summary:**\nOn Tuesday, 14 June 2022, at approximately 06:00 US/Pacific, Google Cloud Networking in us-central1 began experiencing increased delays in applying administrative operations, impacting several downstream services. Customers performing administrative actions on resources in us-central1 experienced delays, connectivity issues, and elevated rates of failure.\nTo our customers that were impacted during this outage, we sincerely apologize. We are conducting an internal investigation and are taking steps to improve our service.\n**Background:**\nGoogle’s Cloud Load Balancer (GCLB) is a collection of software and services that load balances HTTP traffic across customer services. A key component of GCLB is the Google Front End (GFE), which load balances traffic over customer backend instances. GCLB also includes a health checking service to determine whether backends such as customer virtual machines, are responding to traffic as expected, or are unhealthy and should be removed from service.\n**Root Cause:**\nThe health checking service and GFE share a common resource pool. For a period of time, GCLB traffic was directed away from certain clusters, and, independently, during this time, health checking load in these clusters increased significantly.\nThe incident was triggered when Google engineers rerouted traffic to these clusters as part of a standard maintenance activity. This rerouted traffic increased load in those clusters, and this slowed the performance of the health checking service, resulting in an increasing rate of health check failures. This in turn led to a general networking control plane slowdown, as the service struggled to keep up with erroneous and rapid health status changes of load balancer backends. The slowdown in the networking control plane resulted in the impact on administrative operations for other resource types which involve network configuration. Customers may have experienced this as slowness or timeouts in administrative operations for the resource types listed in the Impact section, or delays in new resources (like VM instances) connecting to networks.\n**Remediation and Prevention:**\nOn Monday, 13 June 2022, at 16:30 US/Pacific, Google engineers rerouted traffic to additional compute resources in the us-central1 region as part of a standard maintenance activity.\nGoogle engineers were alerted to control plane slowness on Tuesday, 14 June 2022, at 09:17 and started an investigation. Initially, Google engineers were unable to determine the severity of the latency, due to insufficient monitoring. After attempts to mitigate by adding resources were unsuccessful, the incident was escalated at 15:50. A cross team effort of Google engineers was launched and at 17:39 mitigated the incident by removing the earlier reroute. This reduced the load on the health check system, and subsequently the networking control plane recovered at 17:43.\nGoogle is committed to improving our service in the future and will be completing the following actions:\n- Prevent similar incidents by improving the capacity modeling of our health checking service, and implementing improved resource isolation in our health checking service.\n- Detect similar incidents more rapidly by improving alerting related to the health checking process to notify responsible teams earlier and speed time to mitigate.\n- Add defense in depth: protect the downstream networking control plane from high rates of load balancing health reports, thus avoiding this type of incident in future.\n**Detailed Description of Impact:**\nOn 13 June 16:30 to 14 June 17:43 2022 US/Pacific:\n## Google Kubernetes Engine (GKE)\nAffected customers would have observed latency and up to ~40% elevated errors or timeouts during GKE Private Service Connect cluster operations including creation, deletion, and updates for a subset of clusters in us-central1. ## Google Cloud Load Balancing (GCLB)\nCustomers with resources in us-central1-c and us-central1-f would have observed increased latency or timeouts and connection errors from the Load Balancer service for resources in us-central1. Customers would have seen up to a 4.5% overall error rate, with up to 23% of requests timing out. ## Filestore\nAffected customers would have observed new instance creation failures for ~50% of the outage duration. In addition, existing instances would have been running at reduced capacity as some of their nodes may have been incorrectly marked down. ## Virtual Private Cloud (VPC)\nIncreased latency and timeouts for creating and updating networking resources in us-central1.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-15T05:58:48+00:00","modified":"2022-06-28T18:46:43+00:00","when":"2022-06-15T05:58:48+00:00","text":"# Mini Incident Report\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 14 Jun 2022 06:56\n**Incident End:** 14 Jun 2022 17:43\n**Duration:** 10 hrs, 47 minutes\n**Affected Services and Features:**\nGoogle Compute Engine, Google Kubernetes Engine, Filestore, Google Cloud Load Balancers\n**Regions/Zones:** us-central1\n**Description:**\nDelays in creating Virtual machines or connecting to existing virtual machines for a period of 10 hrs 47 minutes. From preliminary analysis, the root cause of the issue is slow down in global health checking infrastructure check to an extent where flow cache in packet processing service was expiring.\n**Customer Impact:**\n**Google Compute Engine** - Users may have observed an error while creating new VMs or connecting VMs in use.\n**Google Kubernetes Engine** - Users observed latency and timeouts during GKE Cluster operations.\n**Google Cloud Load Balancers** - Customers may have observed errors from Load Balancer service for resources in us-central1.\n**Filestore** - Users may have observed an error while creating new VMs.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-15T01:15:01+00:00","modified":"2022-06-15T01:15:02+00:00","when":"2022-06-15T01:15:01+00:00","text":"The issue with Google Cloud Networking that impacted multiple products is fully mitigated as of 2022-06-14 17:43 US/Pacific. Our Engineering team is continuing to monitor the environment and working towards full resolution.\nIf you still have questions or are impacted, please open a case with the Support Team, and we will continue to work with you.\nWe thank you for your patience while we are working towards resolving this issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-15T00:47:02+00:00","modified":"2022-06-15T00:47:03+00:00","when":"2022-06-15T00:47:02+00:00","text":"Summary: We are experiencing an issue with Google Cloud Networking at us-central1 across multiple products, beginning at Tuesday, 2022-06-14 06:00 US/Pacific.\nDescription: We are experiencing an issue with Google Cloud Networking at us-central1, beginning at Tuesday, 2022-06-14 06:00 US/Pacific, with new VM creation errors, latencies, timeouts, and connection errors reported across multiple products.\n- Google Compute Engine\n- Google Kubernetes Engine\n- Google Cloud Load Balancers\n- Filestore\nMitigation work is currently underway by our engineering team. We do not have an ETA for mitigation at this point.\nWe will provide an update by Tuesday, 2022-06-14 18:20 US/Pacific with current details.\nDiagnosis: Customers may see new VM creation errors, latencies, timeouts, and connection errors reported across multiple products in us-central1.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-15T00:15:30+00:00","modified":"2022-06-15T00:15:31+00:00","when":"2022-06-15T00:15:30+00:00","text":"Summary: We are experiencing an issue with Google Cloud Networking at us-central1 across multiple products, beginning at Tuesday, 2022-06-14 06:00 US/Pacific.\nDescription: We are experiencing an issue with Google Cloud Networking at us-central1 beginning at Tuesday, 2022-06-14 06:00 US/Pacific, with latencies across products.\n- Google Compute Engine.\n- Google Kubernetes Engine.\n- Google Cloud Load Balancers.\n- Filestore.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 17:45 US/Pacific with current details.\nDiagnosis: Services running from us-central1 may experience latency issues.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-15T00:09:02+00:00","modified":"2022-06-15T00:09:04+00:00","when":"2022-06-15T00:09:02+00:00","text":"Summary: We are experiencing an issue with Google Cloud Networking at us-central1 across multiple products, beginning at Tuesday, 2022-06-14 06:00 US/Pacific.\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Tuesday, 2022-06-14 06:00 US/Pacific with reports of possible latencies across products\n- Google Kubernetes Engine\n- Google Cloud Load Balancers\n- Filestore.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 17:45 US/Pacific with current details.\nDiagnosis: Services running from us-central1 may experience latency issues.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-06-28T18:27:16+00:00","modified":"2022-06-28T18:44:58+00:00","when":"2022-06-28T18:27:16+00:00","text":"# Incident Report\n**Summary:**\nOn Tuesday, 14 June 2022, at approximately 06:00 US/Pacific, Google Cloud Networking in us-central1 began experiencing increased delays in applying administrative operations, impacting several downstream services. Customers performing administrative actions on resources in us-central1 experienced delays, connectivity issues, and elevated rates of failure.\nTo our customers that were impacted during this outage, we sincerely apologize. We are conducting an internal investigation and are taking steps to improve our service.\n**Background:**\nGoogle’s Cloud Load Balancer (GCLB) is a collection of software and services that load balances HTTP traffic across customer services. A key component of GCLB is the Google Front End (GFE), which load balances traffic over customer backend instances. GCLB also includes a health checking service to determine whether backends such as customer virtual machines, are responding to traffic as expected, or are unhealthy and should be removed from service.\n**Root Cause:**\nThe health checking service and GFE share a common resource pool. For a period of time, GCLB traffic was directed away from certain clusters, and, independently, during this time, health checking load in these clusters increased significantly.\nThe incident was triggered when Google engineers rerouted traffic to these clusters as part of a standard maintenance activity. This rerouted traffic increased load in those clusters, and this slowed the performance of the health checking service, resulting in an increasing rate of health check failures. This in turn led to a general networking control plane slowdown, as the service struggled to keep up with erroneous and rapid health status changes of load balancer backends. The slowdown in the networking control plane resulted in the impact on administrative operations for other resource types which involve network configuration. Customers may have experienced this as slowness or timeouts in administrative operations for the resource types listed in the Impact section, or delays in new resources (like VM instances) connecting to networks.\n**Remediation and Prevention:**\nOn Monday, 13 June 2022, at 16:30 US/Pacific, Google engineers rerouted traffic to additional compute resources in the us-central1 region as part of a standard maintenance activity.\nGoogle engineers were alerted to control plane slowness on Tuesday, 14 June 2022, at 09:17 and started an investigation. Initially, Google engineers were unable to determine the severity of the latency, due to insufficient monitoring. After attempts to mitigate by adding resources were unsuccessful, the incident was escalated at 15:50. A cross team effort of Google engineers was launched and at 17:39 mitigated the incident by removing the earlier reroute. This reduced the load on the health check system, and subsequently the networking control plane recovered at 17:43.\nGoogle is committed to improving our service in the future and will be completing the following actions:\n- Prevent similar incidents by improving the capacity modeling of our health checking service, and implementing improved resource isolation in our health checking service.\n- Detect similar incidents more rapidly by improving alerting related to the health checking process to notify responsible teams earlier and speed time to mitigate.\n- Add defense in depth: protect the downstream networking control plane from high rates of load balancing health reports, thus avoiding this type of incident in future.\n**Detailed Description of Impact:**\nOn 13 June 16:30 to 14 June 17:43 2022 US/Pacific:\n## Google Kubernetes Engine (GKE)\nAffected customers would have observed latency and up to ~40% elevated errors or timeouts during GKE Private Service Connect cluster operations including creation, deletion, and updates for a subset of clusters in us-central1. ## Google Cloud Load Balancing (GCLB)\nCustomers with resources in us-central1-c and us-central1-f would have observed increased latency or timeouts and connection errors from the Load Balancer service for resources in us-central1. Customers would have seen up to a 4.5% overall error rate, with up to 23% of requests timing out. ## Filestore\nAffected customers would have observed new instance creation failures for ~50% of the outage duration. In addition, existing instances would have been running at reduced capacity as some of their nodes may have been incorrectly marked down. ## Virtual Private Cloud (VPC)\nIncreased latency and timeouts for creating and updating networking resources in us-central1.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"},{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"}],"uri":"incidents/fc7GCA6kAgnBihezUAkx","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"huJFgyiNYPPypbw8PX9Y","number":"9278202795851610263","begin":"2022-06-14T10:06:00+00:00","created":"2022-06-16T16:27:22+00:00","end":"2022-06-16T18:33:00+00:00","modified":"2022-06-30T14:51:46+00:00","external_desc":"Global: Some Google Cloud Console Customers are unable to create/edit GCE instances","updates":[{"created":"2022-06-24T21:47:40+00:00","modified":"2022-06-30T14:51:16+00:00","when":"2022-06-24T21:47:40+00:00","text":"**SUMMARY:**\nOn Tuesday, 14 June 2022, customers were unable to create or edit Google Compute Engine (GCE) instances via the Google Cloud Console for 2 days, 8 hours, 27 minutes. To our customers that were impacted during this outage, we sincerely apologize. We are conducting an internal investigation and are taking steps to improve our service.\n**ROOT CAUSE:**\nCustomers can specify organization policies to limit what instances can use external IP addresses (compute.vmExternalIpAccess). Setting an IP address on an instance not allowed by policy will cause an operation failure.\nThe incident was triggered by a compute frontend UI release which made it impossible for certain users to modify instances due to interactions between org policies and a bug that forced an Ephemeral IP address while on either the edit or create page. Any user with the compute.vmExternalIpAccess policy could not create or edit instances without a public IP.\nA bug was identified where customers with a policy restricting external IP addresses were not able to select one during instance creation. An attempt to fix this bug created a regression where changing any field in the Edit instance page would change the IP address to ephemeral for instances that had no IP address selected. Because of the org policy blocks assigning the instance a public IP, the save operation would fail.The release containing this change included a fix to address the bug however, once in production several customers reported issues.\n**REMEDIATION AND PREVENTION:**\nGoogle engineers were alerted to the issue via customer support case on Thursday, 16 June 2022 at 04:15 and started an investigation. At 06:11, Google engineers were able to reproduce the issue and escalated the incident at 09:08. At 09:50, Google engineers initiated a bug of the release which was completed at 11:33 fully mitigating the issue.\nGoogle is committed to improving our service in the future and will be completing the following actions:\n- Improve unit testing for org policies to identify issues of this type.\n- Improve alerting to quickly detect configuration failures.\n**DETAILED DESCRIPTION OF IMPACT:**\nOn Tuesday, 14 June 2022 03:06 to Thursday 16 June 11:33 US/Pacific\n# Google Compute Engine\nAffected customers experienced failures creating or editing GCE instances via the Google Cloud Console and may have received an error “Constraint constraints/compute.vmExternalIpAccess violated for project [project ID].“\n**ADDITIONAL INFORMATION FOR CUSTOMERS:**\nAs a workaround, customers were still able to create or edit GCE instances via the gcloud CLI or via Google Cloud Console by disabling the constraints/compute.vmExternalIpAccess policy.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T23:54:30+00:00","modified":"2022-06-16T23:54:30+00:00","when":"2022-06-16T23:54:30+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 13 June 2022 14:55\n**Incident End:** 16 June 2022 11:26\n**Duration:** 2 days, 20 hours, 31 minutes\n**Affected Services and Features:**\nGoogle Cloud Console, Google Compute Engine\n**Regions/Zones:** Global\n**Description:**\nCustomers may have been unable to create or edit Google Compute Engine (GCE) instances via the Google Cloud Console for 2 days 20 hours 31 minutes. From preliminary analysis, the root cause of the issue was a recent update to the GCE frontend UI.\nGoogle engineers rolled back the GCE frontend UI update to mitigate the issue on 16 June 2022 11:33 US/Pacific.\n**Customer Impact:**\nCustomers attempting to create or edit GCE instances via the Google Cloud Console may have received an error “Constraint constraints/compute.vmExternalIpAccess violated for project [project ID].“\nAs a workaround, customers were still able to create or edit GCE instances via the gcloud CLI or via Google Cloud Console by disabling the constraints/compute.vmExternalIpAccess policy.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T18:55:37+00:00","modified":"2022-06-16T18:55:38+00:00","when":"2022-06-16T18:55:37+00:00","text":"The issue with Google Cloud Console, Google Compute Engine has been resolved for all affected projects as of Thursday, 2022-06-16 11:33 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T17:22:41+00:00","modified":"2022-06-16T17:22:42+00:00","when":"2022-06-16T17:22:41+00:00","text":"Summary: Global: Some Google Cloud Console Customers are unable to create/edit GCE instances\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-06-16 12:00 US/Pacific.\nDiagnosis: Affected customers are unable to create/edit GCE instances via Cloud Console.\nWorkaround: 1) Using gcloud CLI\n2) Disabling the policy of constraints/compute.vmExternalIpAccess","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T16:52:07+00:00","modified":"2022-06-16T16:52:08+00:00","when":"2022-06-16T16:52:07+00:00","text":"Summary: Global: Some Google Cloud Console Customers are unable to create/edit GCE instances\nDescription: We are experiencing an issue with Google Compute Engine, Google Cloud Console beginning at Thursday, 2022-06-16 09:08 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-16 10:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create/edit GCE instances via Cloud Console.\nWorkaround: 1) Using gcloud CLI\n2) Disabling the policy of constraints/compute.vmExternalIpAccess","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T16:44:46+00:00","modified":"2022-06-16T16:44:52+00:00","when":"2022-06-16T16:44:46+00:00","text":"Summary: Global: Some Google Cloud Console Customers are able to edit GCE instances\nDescription: We are experiencing an issue with Google Compute Engine, Google Cloud Console beginning at Thursday, 2022-06-16 09:08 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-16 09:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create/edit GCE instances via Cloud Console.\nWorkaround: 1) Using gcloud CLI\n2) Disabling the policy of constraints/compute.vmExternalIpAccess","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-16T16:27:16+00:00","modified":"2022-06-16T16:27:23+00:00","when":"2022-06-16T16:27:16+00:00","text":"Summary: Global: Some Google Cloud Console Customers are able to edit GCE instances\nDescription: We are experiencing an issue with Google Compute Engine, Google Cloud Console beginning at Thursday, 2022-06-16 09:08 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-16 09:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to edit any GCE instances via Cloud Console.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-24T21:47:40+00:00","modified":"2022-06-30T14:51:16+00:00","when":"2022-06-24T21:47:40+00:00","text":"**SUMMARY:**\nOn Tuesday, 14 June 2022, customers were unable to create or edit Google Compute Engine (GCE) instances via the Google Cloud Console for 2 days, 8 hours, 27 minutes. To our customers that were impacted during this outage, we sincerely apologize. We are conducting an internal investigation and are taking steps to improve our service.\n**ROOT CAUSE:**\nCustomers can specify organization policies to limit what instances can use external IP addresses (compute.vmExternalIpAccess). Setting an IP address on an instance not allowed by policy will cause an operation failure.\nThe incident was triggered by a compute frontend UI release which made it impossible for certain users to modify instances due to interactions between org policies and a bug that forced an Ephemeral IP address while on either the edit or create page. Any user with the compute.vmExternalIpAccess policy could not create or edit instances without a public IP.\nA bug was identified where customers with a policy restricting external IP addresses were not able to select one during instance creation. An attempt to fix this bug created a regression where changing any field in the Edit instance page would change the IP address to ephemeral for instances that had no IP address selected. Because of the org policy blocks assigning the instance a public IP, the save operation would fail.The release containing this change included a fix to address the bug however, once in production several customers reported issues.\n**REMEDIATION AND PREVENTION:**\nGoogle engineers were alerted to the issue via customer support case on Thursday, 16 June 2022 at 04:15 and started an investigation. At 06:11, Google engineers were able to reproduce the issue and escalated the incident at 09:08. At 09:50, Google engineers initiated a bug of the release which was completed at 11:33 fully mitigating the issue.\nGoogle is committed to improving our service in the future and will be completing the following actions:\n- Improve unit testing for org policies to identify issues of this type.\n- Improve alerting to quickly detect configuration failures.\n**DETAILED DESCRIPTION OF IMPACT:**\nOn Tuesday, 14 June 2022 03:06 to Thursday 16 June 11:33 US/Pacific\n# Google Compute Engine\nAffected customers experienced failures creating or editing GCE instances via the Google Cloud Console and may have received an error “Constraint constraints/compute.vmExternalIpAccess violated for project [project ID].“\n**ADDITIONAL INFORMATION FOR CUSTOMERS:**\nAs a workaround, customers were still able to create or edit GCE instances via the gcloud CLI or via Google Cloud Console by disabling the constraints/compute.vmExternalIpAccess policy.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"}],"uri":"incidents/huJFgyiNYPPypbw8PX9Y","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Columbus (us-east5)","id":"us-east5"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"4Sie4sXFPougA2Ku24pT","number":"16190899427126199648","begin":"2022-06-14T10:03:00+00:00","created":"2022-06-14T11:00:36+00:00","end":"2022-06-14T13:22:00+00:00","modified":"2022-06-14T18:54:41+00:00","external_desc":"GKE API seeing high latency and errors in us-central1","updates":[{"created":"2022-06-14T18:54:05+00:00","modified":"2022-06-14T18:54:05+00:00","when":"2022-06-14T18:54:05+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 14 June 2022 03:03\n**Incident End:** 14 June 2022 06:22\n**Duration:** 3 hours, 19 minutes\n**Affected Services and Features:**\nGoogle Kubernetes Engine, Cloud Data Fusion, Cloud Logging\n**Regions/Zones:** us-central1\n**Description:**\nGoogle Kubernetes Engine (GKE) experienced increased latency and errors from the GKE API in us-central1 for 3 hours and 19 minutes. From preliminary analysis, the root cause of the issue was an unexpected increase in backend traffic that was compounded by clients retrying failed requests. The issue was mitigated by scaling up the GKE control plane to handle the increased traffic.\n**Customer Impact:**\n* **Google Kubernetes Engine (GKE)** customers may have experienced increased latency and errors for GKE API calls via gcloud and the Google Cloud Console UI. GKE cluster workloads continued to run; however customers may have been unable to perform operations such as confirm cluster status, trigger upgrades, view workloads, or create/delete clusters and node pools.\n* **Cloud Data Fusion** customers may have experienced instance creation and deletion failures in us-central1.\n* **Cloud Logging** customers may have experienced increased latency for suggested searches for GKE clusters in us-central1.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T13:26:34+00:00","modified":"2022-06-14T13:26:39+00:00","when":"2022-06-14T13:26:34+00:00","text":"The issue with Google Kubernetes Engine has been resolved for all affected users as of Tuesday, 2022-06-14 06:17 US/Pacific.\nIf customers are still experiencing issues, please raise the case via normal channels .\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T13:15:26+00:00","modified":"2022-06-14T13:15:31+00:00","when":"2022-06-14T13:15:26+00:00","text":"Summary: GKE API seeing high latency and errors in us-central1\nDescription: Mitigation work is still underway by our engineering team.\nCustomers who are able to use another region are advised to do so.\nWe will provide more information by Tuesday, 2022-06-14 06:35 US/Pacific.\nDiagnosis: Users will see high errors and latency from the GKE API in us-central1. Retries may be successful.\nWorkaround: None at this time","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T12:34:41+00:00","modified":"2022-06-14T12:34:46+00:00","when":"2022-06-14T12:34:41+00:00","text":"Summary: GKE API seeing high latency and errors in us-central1\nDescription: Mitigation work is still underway by our engineering team.\nCustomers who are able to use another region are advised to do so.\nWe will provide more information by Tuesday, 2022-06-14 06:10 US/Pacific.\nDiagnosis: Users will see high errors and latency from the GKE API in us-central1. Retries may be successful.\nWorkaround: None at this time","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T12:01:11+00:00","modified":"2022-06-14T12:01:18+00:00","when":"2022-06-14T12:01:11+00:00","text":"Summary: GKE API seeing high latency and errors in us-central1\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2022-06-14 05:30 US/Pacific.}\nWe will provide more information by Tuesday, 2022-06-14 05:31 US/Pacific.\nDiagnosis: Users will see high errors and latency from the GKE API in us-central1. Retries may be successful.\nWorkaround: None at this time","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T11:28:50+00:00","modified":"2022-06-14T11:28:55+00:00","when":"2022-06-14T11:28:50+00:00","text":"Summary: GKE API seeing high latency and errors in us-central1\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-06-14 05:00 US/Pacific.\nDiagnosis: Users will see high errors and latency from the GKE API in us-central1. Retries may be successful.\nWorkaround: None at this time","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-06-14T11:00:28+00:00","modified":"2022-06-14T11:00:41+00:00","when":"2022-06-14T11:00:28+00:00","text":"Summary: GKE API seeing high latency and errors in us-central1\nDescription: We are experiencing an issue with Google Kubernetes Engine beginning at Tuesday, 2022-06-14 03:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-14 04:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Users will see high errors and latency from the GKE API in us-central1. Retries may be successful.\nWorkaround: None at this time","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-06-14T18:54:05+00:00","modified":"2022-06-14T18:54:05+00:00","when":"2022-06-14T18:54:05+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 14 June 2022 03:03\n**Incident End:** 14 June 2022 06:22\n**Duration:** 3 hours, 19 minutes\n**Affected Services and Features:**\nGoogle Kubernetes Engine, Cloud Data Fusion, Cloud Logging\n**Regions/Zones:** us-central1\n**Description:**\nGoogle Kubernetes Engine (GKE) experienced increased latency and errors from the GKE API in us-central1 for 3 hours and 19 minutes. From preliminary analysis, the root cause of the issue was an unexpected increase in backend traffic that was compounded by clients retrying failed requests. The issue was mitigated by scaling up the GKE control plane to handle the increased traffic.\n**Customer Impact:**\n* **Google Kubernetes Engine (GKE)** customers may have experienced increased latency and errors for GKE API calls via gcloud and the Google Cloud Console UI. GKE cluster workloads continued to run; however customers may have been unable to perform operations such as confirm cluster status, trigger upgrades, view workloads, or create/delete clusters and node pools.\n* **Cloud Data Fusion** customers may have experienced instance creation and deletion failures in us-central1.\n* **Cloud Logging** customers may have experienced increased latency for suggested searches for GKE clusters in us-central1.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"},{"title":"Cloud Data Fusion","id":"rLKDHeeaBiXTeutF1air"}],"uri":"incidents/4Sie4sXFPougA2Ku24pT","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"KPYTXQW4gRhXvTsZ1Fj9","number":"18213986659195085044","begin":"2022-06-11T12:16:00+00:00","created":"2022-06-11T13:46:24+00:00","end":"2022-06-11T14:45:00+00:00","modified":"2022-06-13T20:35:03+00:00","external_desc":"We are experiencing an issue with Cloud Armor control plane","updates":[{"created":"2022-06-13T20:33:50+00:00","modified":"2022-06-13T20:33:50+00:00","when":"2022-06-13T20:33:50+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 11 June 2022 05:16\n**Incident End:** 11 June 2022 07:45\n**Duration:** 2 hours, 29 minutes\n**Affected Services and Features:**\nGoogle Cloud Armour\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Armor control plane was not processing updates in the global locale for a duration of 2 hours and 29 minutes. From preliminary analysis, the root cause of the issue appears to be multiple Storage errors that caused the control plane writer to stop updating. The control plane processed no or minimal updates within 2022-06-11 05:16 - 07:45 PDT.\n**Customer Impact:**\nCustomers may have experienced Cloud Armor rule changes that were not applied since the control plane was not processing changes.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-11T15:44:17+00:00","modified":"2022-06-11T15:44:18+00:00","when":"2022-06-11T15:44:17+00:00","text":"The issue with Cloud Armor has been resolved for all affected users as of Saturday, 2022-06-11 07:45 US/Pacific.\nCustomers who made updates during the timeframe of 2022-06-11 05:16 - 07:45 PDT are not effective and will need to make a fresh update to their Cloud Armor config.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-11T14:22:25+00:00","modified":"2022-06-11T14:22:26+00:00","when":"2022-06-11T14:22:25+00:00","text":"Summary: We are experiencing an issue with Cloud Armor control plane\nDescription: We are experiencing an issue with Cloud Armor beginning at Saturday, 2022-06-11 05:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Saturday, 2022-06-11 08:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience that Cloud Armor control plane is stuck with no updates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-11T13:46:24+00:00","modified":"2022-06-11T14:02:54+00:00","when":"2022-06-11T13:46:24+00:00","text":"Summary: We are experiencing an issue with Cloud Armor control plane\nDescription: We are experiencing an issue with Cloud Armor beginning at Saturday, 2022-06-11 05:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Saturday, 2022-06-11 07:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience that Cloud Armor control plane is stuck with no updates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-06-13T20:33:50+00:00","modified":"2022-06-13T20:33:50+00:00","when":"2022-06-13T20:33:50+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 11 June 2022 05:16\n**Incident End:** 11 June 2022 07:45\n**Duration:** 2 hours, 29 minutes\n**Affected Services and Features:**\nGoogle Cloud Armour\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Armor control plane was not processing updates in the global locale for a duration of 2 hours and 29 minutes. From preliminary analysis, the root cause of the issue appears to be multiple Storage errors that caused the control plane writer to stop updating. The control plane processed no or minimal updates within 2022-06-11 05:16 - 07:45 PDT.\n**Customer Impact:**\nCustomers may have experienced Cloud Armor rule changes that were not applied since the control plane was not processing changes.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Armor","id":"Kakg69gTC3xFyeJCY2va"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/KPYTXQW4gRhXvTsZ1Fj9","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"HzDuJz8t92LzvER4pT3y","number":"5087447213217570600","begin":"2022-06-09T22:34:28+00:00","created":"2022-06-09T22:47:24+00:00","end":"2022-06-10T00:27:41+00:00","modified":"2022-06-10T00:27:41+00:00","external_desc":"[US Multiregion] Customers May Experience BigQuery Latency","updates":[{"created":"2022-06-10T00:27:40+00:00","modified":"2022-06-10T00:27:41+00:00","when":"2022-06-10T00:27:40+00:00","text":"The issue with Google BigQuery has been resolved for all affected projects as of Thursday, 2022-06-09 17:24 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-06-09T23:04:25+00:00","modified":"2022-06-09T23:04:25+00:00","when":"2022-06-09T23:04:25+00:00","text":"Summary: [US Multiregion] Customers May Experience BigQuery Latency\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-06-09 17:10 US/Pacific.\nWe will provide more information by Thursday, 2022-06-09 17:30 US/Pacific.\nDiagnosis: Affected customers may experience latencies with BigQuery in the US Multiregion\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-06-09T23:03:51+00:00","modified":"2022-06-09T23:03:52+00:00","when":"2022-06-09T23:03:51+00:00","text":"Summary: [US Multiregion] Customers May Experience BigQuery Latency\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-06-09 17:30 US/Pacific.\nWe will provide more information by Thursday, 2022-06-09 17:02 US/Pacific.\nDiagnosis: Affected customers may experience latencies with BigQuery in the US Multiregion\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-06-09T22:47:24+00:00","modified":"2022-06-09T22:47:25+00:00","when":"2022-06-09T22:47:24+00:00","text":"Summary: [US Multiregion] Customers May Experience BigQuery Latency\nDescription: We are experiencing a performance issue with Google BigQuery.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-09 16:05 US/Pacific with current details.\nDiagnosis: Affected customers may experience latencies with BigQuery in the US Multiregion\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Multi-region: us","id":"us"}]}],"most_recent_update":{"created":"2022-06-10T00:27:40+00:00","modified":"2022-06-10T00:27:41+00:00","when":"2022-06-10T00:27:40+00:00","text":"The issue with Google BigQuery has been resolved for all affected projects as of Thursday, 2022-06-09 17:24 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/HzDuJz8t92LzvER4pT3y","currently_affected_locations":[],"previously_affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"id":"Q9MUHCFVpqouJbGrzneJ","number":"13932012708372562392","begin":"2022-06-08T16:16:00+00:00","created":"2022-06-14T13:12:45+00:00","end":"2022-06-15T03:02:00+00:00","modified":"2022-06-15T09:39:27+00:00","external_desc":"[Global] Google App Engine - Customers using the Secrets Manager are experiencing increased errors.","updates":[{"created":"2022-06-15T09:38:38+00:00","modified":"2022-06-15T09:38:38+00:00","when":"2022-06-15T09:38:38+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 08 June 2022 09:16\n**Incident End:** 14 June 2022 20:02\n**Duration:** 6 day, 10 hours, 46 minutes\n**Affected Services and Features:**\nGoogle Cloud App Engine, Google Cloud Functions.\n**Regions/Zones:**\nGlobal\n**Description:**\nGoogle Cloud App Engine customers might have experienced elevated errors while using the Secrets Manager for a duration of 6 days, 10 hours and 46 minutes. From preliminary analysis, the root cause of the issue is, regression in a recent software release relating to Secret Manager functionality in Google Cloud App Engine.\n**Customer Impact:**\nCustomers might have experienced increased errors from apps or functions that use the Secrets Manager.\n**Additional details:**\nThe Secret Manager functionality related release updates are rolled back to mitigate the issue. During the impact customers were advised to mount each secret in a separate path as per https://cloud.google.com/functions/docs/configuring/secrets.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-15T04:03:22+00:00","modified":"2022-06-15T04:03:23+00:00","when":"2022-06-15T04:03:22+00:00","text":"The issue with Google App Engine has been resolved for all affected projects as of Tuesday, 2022-06-14 20:02 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-15T00:36:49+00:00","modified":"2022-06-15T00:36:57+00:00","when":"2022-06-15T00:36:49+00:00","text":"Summary: [Global] Google App Engine - Customers using the Secrets Manager are experiencing increased errors.\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Tuesday, 2022-06-14 22:00 US/Pacific.\nDiagnosis: Customers using the Secrets Manager are experiencing increased errors.\nWorkaround: A workaround is to mount each secret in a separate path. Please refer to https://cloud.google.com/functions/docs/configuring/secrets for information on Secret Manager.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-14T21:28:14+00:00","modified":"2022-06-14T21:28:15+00:00","when":"2022-06-14T21:28:14+00:00","text":"Summary: [Global] Google App Engine - Customers using the Secrets Manager are experiencing increased errors.\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Tuesday, 2022-06-14 18:00 US/Pacific.\nDiagnosis: Customers using the Secrets Manager are experiencing increased errors.\nWorkaround: A workaround is to mount each secret in a separate path. Please refer to https://cloud.google.com/functions/docs/configuring/secrets for information on Secret Manager.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-14T16:37:43+00:00","modified":"2022-06-14T16:37:53+00:00","when":"2022-06-14T16:37:43+00:00","text":"Summary: Increase in errors correlated with binary rollout\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is completed completed for europe-west3. The mitigation ETA for us-central1 is Tuesday, 2022-06-14 14:00 US/Pacific. ETA will be provided for other regions in a subsequent update.\nWe will provide more information by Tuesday, 2022-06-14 14:30 US/Pacific.\nDiagnosis: Customers using Secrets Manager are experiencing increased errors.\nWorkaround: A workaround is to mount each secret in a separate path. Please refer to https://cloud.google.com/functions/docs/configuring/secrets for information on Secret Manager.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-14T15:08:32+00:00","modified":"2022-06-14T15:08:39+00:00","when":"2022-06-14T15:08:32+00:00","text":"Summary: Increase in errors correlated with binary rollout\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to be completed for europe-west3 Tuesday, 2022-06-14 10:00 US/Pacific, and for us-central1 Tuesday, 2022-06-14 14:00 US/Pacific. ETA will be provided for other regions in a subsequent update.\nWe will provide more information by Tuesday, 2022-06-14 10:10 US/Pacific.\nDiagnosis: Customers using Secrets Manager are experiencing increased errors.\nWorkaround: A workaround is to mount each secret in a separate path. Please refer to https://cloud.google.com/functions/docs/configuring/secrets for information on Secret Manager.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-14T13:12:43+00:00","modified":"2022-06-14T13:12:48+00:00","when":"2022-06-14T13:12:43+00:00","text":"Summary: Increase in errors correlated with binary rollout\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to be completed for europe-west3 Tuesday, 2022-06-14 10:00 US/Pacific, and for us-central1 Tuesday, 2022-06-14 14:00 US/Pacific. ETA will be provided for other regions in a subsequent update.\nWe will provide more information by Tuesday, 2022-06-14 08:27 US/Pacific.\nDiagnosis: Customers are experiencing increased errors.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-15T09:38:38+00:00","modified":"2022-06-15T09:38:38+00:00","when":"2022-06-15T09:38:38+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 08 June 2022 09:16\n**Incident End:** 14 June 2022 20:02\n**Duration:** 6 day, 10 hours, 46 minutes\n**Affected Services and Features:**\nGoogle Cloud App Engine, Google Cloud Functions.\n**Regions/Zones:**\nGlobal\n**Description:**\nGoogle Cloud App Engine customers might have experienced elevated errors while using the Secrets Manager for a duration of 6 days, 10 hours and 46 minutes. From preliminary analysis, the root cause of the issue is, regression in a recent software release relating to Secret Manager functionality in Google Cloud App Engine.\n**Customer Impact:**\nCustomers might have experienced increased errors from apps or functions that use the Secrets Manager.\n**Additional details:**\nThe Secret Manager functionality related release updates are rolled back to mitigate the issue. During the impact customers were advised to mount each secret in a separate path as per https://cloud.google.com/functions/docs/configuring/secrets.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"kchyUtnkMHJWaAva8aYc","service_name":"Google App Engine","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/Q9MUHCFVpqouJbGrzneJ","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"e383eLoLFGdSMeUQo7Ch","number":"13605570326637952215","begin":"2022-06-08T00:49:06+00:00","created":"2022-06-08T01:29:03+00:00","end":"2022-06-13T22:16:02+00:00","modified":"2022-06-13T22:16:02+00:00","external_desc":"Google Analytics Daily BigQuery exports for certain customers may experience delays","updates":[{"created":"2022-06-13T22:16:01+00:00","modified":"2022-06-13T22:16:02+00:00","when":"2022-06-13T22:16:01+00:00","text":"The issue with Google Analytics Daily BigQuery exports has been resolved for all affected users as of Monday, 2022-06-13 12:47 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-13T16:14:33+00:00","modified":"2022-06-13T16:14:35+00:00","when":"2022-06-13T16:14:33+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: *Note 'This is not a problem with BigQuery, only Google Analytics BigQuery export is affected'\nMitigation work is still underway by our engineering team.\nWe will provide more information by Tuesday, 2022-06-14 09:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-13T14:55:31+00:00","modified":"2022-06-13T14:55:41+00:00","when":"2022-06-13T14:55:31+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: *Note 'This is not a problem with BigQuery, only Google Analytics BigQuery export is affected'\nMitigation work is currently underway by our engineering team.\nWe will provide more information by Monday, 2022-06-13 09:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-10T20:04:29+00:00","modified":"2022-06-10T20:04:30+00:00","when":"2022-06-10T20:04:29+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: *Note 'This is not a problem with BigQuery, only Google Analytics BigQuery export is affected'\nMitigation work is currently underway by our engineering team.\nWe will provide more information by Monday, 2022-06-13 08:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-10T17:57:27+00:00","modified":"2022-06-10T17:57:35+00:00","when":"2022-06-10T17:57:27+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: *Note 'This is not a problem with BigQuery, only Google Analytics BigQuery export is affected'\nMitigation work is currently underway by our engineering team.\nWe will provide more information by Friday, 2022-06-10 14:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-10T03:37:40+00:00","modified":"2022-06-10T03:37:41+00:00","when":"2022-06-10T03:37:40+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: *Note 'This is not a problem with BigQuery, only Google Analytics BigQuery export is affected'\nMitigation work is currently underway by our engineering team.\nWe will provide more information by Friday, 2022-06-10 11:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-09T23:08:28+00:00","modified":"2022-06-09T23:08:29+00:00","when":"2022-06-09T23:08:28+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: *Note 'This is not a problem with BigQuery, only Google Analytics BigQuery export is affected'\nMitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-06-09 11:59 US/Pacific.\nWe will provide more information by Friday, 2022-06-10 00:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-08T23:21:13+00:00","modified":"2022-06-08T23:21:14+00:00","when":"2022-06-08T23:21:13+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: Our engineering team has determined that further investigation is required to mitigate the issue.\nWe will provide an update by Thursday, 2022-06-09 17:22 US/Pacific with current details.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-08T20:54:32+00:00","modified":"2022-06-08T20:54:38+00:00","when":"2022-06-08T20:54:32+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Wednesday, 2022-06-08 16:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-08T17:47:19+00:00","modified":"2022-06-08T17:47:20+00:00","when":"2022-06-08T17:47:19+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Wednesday, 2022-06-08 14:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-08T17:42:28+00:00","modified":"2022-06-08T17:42:34+00:00","when":"2022-06-08T17:42:28+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Wednesday, 2022-06-08 10:00 US/Pacific.\nWe will provide more information by Wednesday, 2022-06-08 14:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-08T02:54:19+00:00","modified":"2022-06-08T02:54:20+00:00","when":"2022-06-08T02:54:19+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Wednesday, 2022-06-08 10:00 US/Pacific.\nWe will provide more information by Wednesday, 2022-06-08 10:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-08T01:28:57+00:00","modified":"2022-06-08T01:29:03+00:00","when":"2022-06-08T01:28:57+00:00","text":"Summary: Google Analytics Daily BigQuery exports for certain customers may experience delays\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-06-08 17:00 US/Pacific.\nDiagnosis: Affected customers may notice that their Google Analytics BigQuery Daily Export jobs are experiencing delays\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-13T22:16:01+00:00","modified":"2022-06-13T22:16:02+00:00","when":"2022-06-13T22:16:01+00:00","text":"The issue with Google Analytics Daily BigQuery exports has been resolved for all affected users as of Monday, 2022-06-13 12:47 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/e383eLoLFGdSMeUQo7Ch","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"YrjzRWPFBUZU5HJZ4mN7","number":"12112012328903018953","begin":"2022-06-07T12:50:00+00:00","created":"2022-06-07T14:01:19+00:00","end":"2022-06-07T16:02:00+00:00","modified":"2022-06-14T20:35:45+00:00","external_desc":"Google Cloud Networking experienced increased packet loss for egress traffic from Google to the Middle East, and elevated latency between our Europe and Asia Regions.","updates":[{"created":"2022-06-14T20:35:45+00:00","modified":"2022-06-14T20:35:45+00:00","when":"2022-06-14T20:35:45+00:00","text":"# INCIDENT REPORT\n**Summary:**\nOn Tuesday, 7 June 2022, Google Cloud Networking experienced increased packet loss for egress traffic from Google to users and customers in the Middle East and elevated latency between our Europe and Asia Regions for 3 hours and 12 minutes, affecting several Cloud Networking products.\nTo our customers that were impacted during this outage, we sincerely apologize. We are conducting an internal investigation and are taking steps to improve our service.\n**Root Cause:**\nThe outage was triggered by two simultaneous fiber cuts within our Middle East network. This affected the end-to-end path for several submarine cables, reducing capacity for many telecom and technology companies, including Google.\nThe reduced capacity increased latency and caused packet loss between Google regions in Europe and regions in the Middle East and the rest of Asia.\n**Remediation and Prevention:**\nGoogle Engineers were alerted to the network bandwidth drop on Tuesday, 7 June 2022 at 06:00 US/Pacific. Google Engineers immediately started to redirect Internet egress demand away from the region. At 07:02, Europe-Asia traffic was redirected across backup circuits via America, trading increased latency for lower packet loss. Fiber engineers restored interim capacity by 08:26, ending packet loss between Europe and the Middle East. Once final stability was confirmed, Europe-Asia traffic was moved back to the direct path at 09:02.\nGoogle is committed to improving our service in the future and will be completing the following actions:\n- We are increasing the resilience of terrestrial network paths to provide more resilience to dual failures.\n- We are continuing with our planned programme of additional investments in the Europe-Middle East network to offer alternative network paths.\n- We are improving the effectiveness of tooling to reduce Internet egress demand during capacity shortfalls.\n**Detailed Description of Impact:**\nOn 07 June 2022 05:50 from 05:50 to 09:03 US/Pacific: ## Cloud Load Balancing\nCustomers using Cloud Load Balancing would have experienced increased packet loss up to 60% for egress traffic from Google to the Middle East and increased network latency between our Europe and Asia regions. ## Cloud Interconnect\nCloud Interconnect customers peering in the Middle East region would have experienced packet loss up to 80% during the incident. ## Virtual Private Cloud (VPC)\nCustomers using Virtual Private Cloud requiring cross-regional traffic between the following regions would have experienced packet loss of up to 50% between 05:47 and 07:03, then elevated network latency until 09:03:\n- asia-east1 to europe-southwest1, europe-west6, and europe-west8;\n- asia-east2 to all regions in Europe;\n- asia-south1 and asia-south2 to all regions in Europe, all northamerica-northeast and northamerica-southeast, and all us-east regions;\n- asia-southeast1 and asia-southeast2 to all regions in Europe;\n- australia-southeast2 to europe-west8.\n## Other Services\nWhile not specifically called out, all Google Cloud Services in the Middle East and our Europe and Asia regions could have been affected by the packet loss for egress traffic from Google to the Middle East, as well as increased latency between our Europe and Asia regions. We have listed the major services that our telemetry identified as causing customer impact.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-07T20:09:36+00:00","modified":"2022-06-07T20:09:36+00:00","when":"2022-06-07T20:09:36+00:00","text":"# Mini Incident Report\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213. We will provide a full Incident Report with additional details.\n(All Times US/Pacific)\n**Incident Start:** 07 June 2022 05:50\n**Incident End:** 07 June 2022 09:02\n**Duration:** 3 hours, 12 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking\nCloud NAT\nHybrid Connectivity\nVirtual Private Cloud (VPC)\nCloud Interconnect\nGoogle Kubernetes Engine (GKE)\n**Regions/Zones:** Packet Loss (Global to Middle East Internet Users), Latency (Between Asia and Europe regions)\n**Description:**\nGoogle Cloud Networking experienced increased packet loss for egress traffic from Google to the Middle East, and elevated latency between our Europe and Asia Regions as a result, for 3 hours and 12 minutes, affecting several related products including Cloud NAT, Hybrid Connectivity and Virtual Private Cloud (VPC). From preliminary analysis, the root cause of the issue was a capacity shortage following two simultaneous fiber-cuts.\n**Customer Impact:**\nAffected customers would have experienced increased packet loss and increased latency for the affected metros which would have caused delays and timeouts for the following services : - Cloud Load Balancing - Cloud Interconnect - Virtual Private Cloud (VPC) - Cloud NAT - Google Kubernetes Engine (GKE)","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-07T16:12:37+00:00","modified":"2022-06-07T16:12:39+00:00","when":"2022-06-07T16:12:37+00:00","text":"The issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) has been resolved for all affected users as of Tuesday, 2022-06-07 09:02 US/Pacific.\nWe will publish an analysis of this incident once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-07T15:51:24+00:00","modified":"2022-06-07T15:51:28+00:00","when":"2022-06-07T15:51:24+00:00","text":"Summary: Packet loss observed from Internet in Middle East to Google\nDescription: We believe the issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) is partially resolved. Packet loss should be mitigated, but some traffic continues to have elevated latency.\nFull resolution is expected to complete by Tuesday, 2022-06-07 09:30 US/Pacific.\nWe will provide an update by Tuesday, 2022-06-07 09:30 US/Pacific with current details.\nDiagnosis: Increased packet loss for traffic traversing the affected metros.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-07T15:40:23+00:00","modified":"2022-06-07T15:40:26+00:00","when":"2022-06-07T15:40:23+00:00","text":"Summary: Packet loss observed from Internet in Middle East to Google\nDescription: Mitigation work is currently underway by our engineering team to redirect traffic and increase capacity.\nThe mitigation is expected to complete around Tuesday, 2022-06-07 09:00 US/Pacific.\nWe will provide more information by Tuesday, 2022-06-07 09:30 US/Pacific.\nDiagnosis: Increased packet loss for traffic traversing the affected metros.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-07T14:44:56+00:00","modified":"2022-06-07T14:45:02+00:00","when":"2022-06-07T14:44:56+00:00","text":"Summary: Packet loss observed from Internet in Middle East to Google\nDescription: We are experiencing an issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) beginning at Tuesday, 2022-06-07 06:00 US/Pacific.\nThere is a latency in connectivity between Europe and Asia.\nMitigation work is still underway by our engineering team.\nWe will provide more information by Tuesday, 2022-06-07 09:15 US/Pacific.\nDiagnosis: Increased packet loss for traffic traversing the affected metros.\nWorkaround: None.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-06-07T14:21:54+00:00","modified":"2022-06-07T14:40:49+00:00","when":"2022-06-07T14:21:54+00:00","text":"Summary: Major packet loss observed from Internet in Middle East to Google\nDescription: We are experiencing an issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) beginning at Tuesday, 2022-06-07 06:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-07 08:00 US/Pacific with current details.\nDiagnosis: Increased packet loss for traffic traversing the affected metros.\nWorkaround: None.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-07T14:01:08+00:00","modified":"2022-06-07T14:01:21+00:00","when":"2022-06-07T14:01:08+00:00","text":"Summary: Major packet loss observed between multiple regions - Europe and Asia\nDescription: We are experiencing an issue with Cloud NAT, Google Cloud Networking, Hybrid Connectivity, Virtual Private Cloud (VPC) beginning at Tuesday, 2022-06-07 06:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-06-07 08:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Increased packet loss for traffic traversing the affected metros.\nWorkaround: None.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2022-06-14T20:35:45+00:00","modified":"2022-06-14T20:35:45+00:00","when":"2022-06-14T20:35:45+00:00","text":"# INCIDENT REPORT\n**Summary:**\nOn Tuesday, 7 June 2022, Google Cloud Networking experienced increased packet loss for egress traffic from Google to users and customers in the Middle East and elevated latency between our Europe and Asia Regions for 3 hours and 12 minutes, affecting several Cloud Networking products.\nTo our customers that were impacted during this outage, we sincerely apologize. We are conducting an internal investigation and are taking steps to improve our service.\n**Root Cause:**\nThe outage was triggered by two simultaneous fiber cuts within our Middle East network. This affected the end-to-end path for several submarine cables, reducing capacity for many telecom and technology companies, including Google.\nThe reduced capacity increased latency and caused packet loss between Google regions in Europe and regions in the Middle East and the rest of Asia.\n**Remediation and Prevention:**\nGoogle Engineers were alerted to the network bandwidth drop on Tuesday, 7 June 2022 at 06:00 US/Pacific. Google Engineers immediately started to redirect Internet egress demand away from the region. At 07:02, Europe-Asia traffic was redirected across backup circuits via America, trading increased latency for lower packet loss. Fiber engineers restored interim capacity by 08:26, ending packet loss between Europe and the Middle East. Once final stability was confirmed, Europe-Asia traffic was moved back to the direct path at 09:02.\nGoogle is committed to improving our service in the future and will be completing the following actions:\n- We are increasing the resilience of terrestrial network paths to provide more resilience to dual failures.\n- We are continuing with our planned programme of additional investments in the Europe-Middle East network to offer alternative network paths.\n- We are improving the effectiveness of tooling to reduce Internet egress demand during capacity shortfalls.\n**Detailed Description of Impact:**\nOn 07 June 2022 05:50 from 05:50 to 09:03 US/Pacific: ## Cloud Load Balancing\nCustomers using Cloud Load Balancing would have experienced increased packet loss up to 60% for egress traffic from Google to the Middle East and increased network latency between our Europe and Asia regions. ## Cloud Interconnect\nCloud Interconnect customers peering in the Middle East region would have experienced packet loss up to 80% during the incident. ## Virtual Private Cloud (VPC)\nCustomers using Virtual Private Cloud requiring cross-regional traffic between the following regions would have experienced packet loss of up to 50% between 05:47 and 07:03, then elevated network latency until 09:03:\n- asia-east1 to europe-southwest1, europe-west6, and europe-west8;\n- asia-east2 to all regions in Europe;\n- asia-south1 and asia-south2 to all regions in Europe, all northamerica-northeast and northamerica-southeast, and all us-east regions;\n- asia-southeast1 and asia-southeast2 to all regions in Europe;\n- australia-southeast2 to europe-west8.\n## Other Services\nWhile not specifically called out, all Google Cloud Services in the Middle East and our Europe and Asia regions could have been affected by the packet loss for egress traffic from Google to the Middle East, as well as increased latency between our Europe and Asia regions. We have listed the major services that our telemetry identified as causing customer impact.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud NAT","id":"hCNpnTQHkUCCGxJy35Yq"}],"uri":"incidents/YrjzRWPFBUZU5HJZ4mN7","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"TujZ6j7u14hirgg5WqZ4","number":"9012572856388055433","begin":"2022-06-06T18:23:23+00:00","created":"2022-06-06T18:23:28+00:00","end":"2022-06-06T19:01:28+00:00","modified":"2022-06-06T19:01:28+00:00","external_desc":"US \u0026 EU Multiregions : Elevated errors with BigQuery streaming inserts","updates":[{"created":"2022-06-06T19:01:27+00:00","modified":"2022-06-06T19:01:30+00:00","when":"2022-06-06T19:01:27+00:00","text":"The issue with Google BigQuery is believed to be affecting a very small number of projects and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-06-06T18:23:27+00:00","modified":"2022-06-06T18:23:30+00:00","when":"2022-06-06T18:23:27+00:00","text":"Summary: US \u0026 Europe : Failure in BigQuery insert due to upsert Check\nDescription: We are experiencing an issue with Google BigQuery beginning at Monday, 2022-06-06 10:26 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-06-06 11:50 US/Pacific with current details.\nDiagnosis: Customers could see insert failures due to PK key check failures\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Multi-region: us","id":"us"}]}],"most_recent_update":{"created":"2022-06-06T19:01:27+00:00","modified":"2022-06-06T19:01:30+00:00","when":"2022-06-06T19:01:27+00:00","text":"The issue with Google BigQuery is believed to be affecting a very small number of projects and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/TujZ6j7u14hirgg5WqZ4","currently_affected_locations":[],"previously_affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"id":"BnzamL7pXnge8zLHyjyK","number":"17344967214763707246","begin":"2022-06-05T19:17:00+00:00","created":"2022-06-05T21:49:14+00:00","end":"2022-06-05T23:14:00+00:00","modified":"2022-06-06T18:54:33+00:00","external_desc":"Global: Elevated delays in propagating changes to Cloud IAM policies and group memberships.","updates":[{"created":"2022-06-06T18:54:17+00:00","modified":"2022-06-06T18:54:17+00:00","when":"2022-06-06T18:54:17+00:00","text":"# Mini Incident Report\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 05 June 2022 12:17\n**Incident End:** 05 June 2022 16:14\n**Duration:** 3 hours, 57 minutes\n**Affected Services and Features:**\nCloud IAM\n**Regions/Zones:** Global\n**Description:**\nCloud IAM experienced elevated delays in propagating changes to Cloud IAM policies due to an unexpected increase in deletion traffic.\n**Customer Impact:**\n- Affected customers would experience delays in processing Cloud IAM policy changes and the use of group membership changes in IAM policy evaluation.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-05T23:22:00+00:00","modified":"2022-06-05T23:22:01+00:00","when":"2022-06-05T23:22:00+00:00","text":"The issue with Identity and Access Management has been resolved for all affected projects as of Sunday, 2022-06-05 16:15 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-05T23:15:10+00:00","modified":"2022-06-05T23:15:16+00:00","when":"2022-06-05T23:15:10+00:00","text":"Summary: Global: Elevated delays in propagating changes to Cloud IAM policies and group memberships\nDescription: Mitigation work is still underway by our engineering team. We are seeing improvements in several regions globally.\nThe mitigation is expected to complete by Sunday, 2022-06-05 17:00 US/Pacific.\nWe will provide more information by Sunday, 2022-06-05 17:00 US/Pacific.\nDiagnosis: Affected customers when making IAM policy or group membership changes will experience increased delays before they take effect. Current delays are up to 45 minutes.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-05T22:19:39+00:00","modified":"2022-06-05T22:19:40+00:00","when":"2022-06-05T22:19:39+00:00","text":"Summary: Global: Elevated delays in propagating changes to Cloud IAM policies and group memberships\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Sunday, 2022-06-05 17:00 US/Pacific.\nDiagnosis: Affected customers when making IAM policy or group membership changes will experience increased delays before they take effect. Current delays are up to 45 minutes.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-05T22:06:48+00:00","modified":"2022-06-05T22:06:50+00:00","when":"2022-06-05T22:06:48+00:00","text":"Summary: Global: Elevated delays in propagating changes to Cloud IAM policies and group memberships\nDescription: We are experiencing an issue with Identity and Access Management.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-06-05 16:45 US/Pacific with current details.\nDiagnosis: Affected customers when making IAM policy or group membership changes will experience increased delays before they take effect. Current delays are up to 30 minutes.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-06-05T22:04:02+00:00","modified":"2022-06-05T22:04:04+00:00","when":"2022-06-05T22:04:02+00:00","text":"Summary: Global: Elevated delays in propagating changes to Cloud IAM policies and group memberships\nDescription: We are experiencing an issue with Identity and Access Management.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-06-05 16:30 US/Pacific with current details.\nDiagnosis: Affected customers when making IAM policy or group membership changes will experience increased delays before they take effect. Current delays are up to 30 minutes.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-06-05T21:49:13+00:00","modified":"2022-06-05T21:49:16+00:00","when":"2022-06-05T21:49:13+00:00","text":"Summary: Global: Elevated delays in propagating changes to Cloud IAM Policies\nDescription: We are experiencing an issue with Identity and Access Management beginning at Sunday, 2022-06-05 09:15 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-06-05 16:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers when making IAM policies changes will experience increased delays before they take effect.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-06-06T18:54:17+00:00","modified":"2022-06-06T18:54:17+00:00","when":"2022-06-06T18:54:17+00:00","text":"# Mini Incident Report\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 05 June 2022 12:17\n**Incident End:** 05 June 2022 16:14\n**Duration:** 3 hours, 57 minutes\n**Affected Services and Features:**\nCloud IAM\n**Regions/Zones:** Global\n**Description:**\nCloud IAM experienced elevated delays in propagating changes to Cloud IAM policies due to an unexpected increase in deletion traffic.\n**Customer Impact:**\n- Affected customers would experience delays in processing Cloud IAM policy changes and the use of group membership changes in IAM policy evaluation.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"adnGEDEt9zWzs8uF1oKA","service_name":"Identity and Access Management","affected_products":[{"title":"Identity and Access Management","id":"adnGEDEt9zWzs8uF1oKA"}],"uri":"incidents/BnzamL7pXnge8zLHyjyK","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"6sfxucMj6DuhVzmj9Xr8","number":"702824853392148085","begin":"2022-06-04T17:20:00+00:00","created":"2022-06-04T17:40:35+00:00","end":"2022-06-04T20:24:00+00:00","modified":"2022-06-06T09:16:04+00:00","external_desc":"us-east1: Elevated errors affecting multiple services.","updates":[{"created":"2022-06-06T09:14:00+00:00","modified":"2022-06-06T09:14:00+00:00","when":"2022-06-06T09:14:00+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 04 June 2022 10:20\n**Incident End:** 04 June 2022 13:24\n**Duration:** 3 hours 04 minutes\n**Affected Services and Features:**\nBigQuery\n**Regions/Zones:** us-east1\n**Description:**\nBigQuery experienced elevated latency for a period of 3 hrs 04 minutes. Preliminary root cause of the issue was an outage in an internal Google storage system causing some bigquery data operations in the cell to fail.\n**Customer Impact:**\nCustomers might have experienced failures or slowness in their requests to BigQuery's us-east1 region.","status":"AVAILABLE","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-05T00:09:17+00:00","modified":"2022-06-05T00:09:19+00:00","when":"2022-06-05T00:09:17+00:00","text":"The issue with Apigee, Cloud Data Fusion, Cloud Filestore, Cloud Load Balancing, Cloud Run, Data Catalog, Datastream, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Dataflow, Google Cloud Datastore, Google Cloud Functions, Google Cloud Networking, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Tasks, Google Compute Engine, Google Kubernetes Engine, Persistent Disk, Virtual Private Cloud (VPC) has been resolved for all affected users as of Saturday, 2022-06-04 17:03 US/Pacific.\nMost service impact was mitigated by 2022-06-04 13:20 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T23:43:24+00:00","modified":"2022-06-04T23:43:27+00:00","when":"2022-06-04T23:43:24+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: We believe the issue with Apigee, Cloud Data Fusion, Cloud Filestore, Cloud Load Balancing, Cloud Run, Data Catalog, Datastream, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Dataflow, Google Cloud Datastore, Google Cloud Functions, Google Cloud Networking, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Tasks, Google Compute Engine, Google Kubernetes Engine, Persistent Disk, Virtual Private Cloud (VPC) is partially resolved.\nWe believe most service impact was mitigated by 2022-06-04 13:30 US/Pacific. We do not have an ETA for full resolution at this point.\nWe will provide an update by Saturday, 2022-06-04 18:00 US/Pacific with current details.\nDiagnosis: Please see the additional details below for product specific impact where available:\n- BigQuery: [Mitigated] Elevated BigQuery latency and errors on Queries.\n- Dataflow: [Mitigated] Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: [Mitigated] Elevated unavailability.\n- Cloud Tasks: [Mitigated] Elevated unavailability.\n- Cloud Pub/Sub: [Mitigated] Increased errors and/or latency.\n- Persistent Disk: [Mitigated] 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\n- Cloud Run: [Mitigated] Elevated unavailability.\n- Compute Engine: No impact outside of side effects from underlying Persistent Disk impact.\n- Cloud Networking: [Mitigated] Entire zone us-east1-c has delayed firewall programming for new changes to data plane, and delays launching or ssh'ing to instances. Existing programming should continue to work.\n- Cloud Functions: [Mitigated] Elevated unavailability.\n- Cloud Composer: [Mitigated] Creations fail because of errors creating App Engine apps, may be in additional regions.\n- Datastore: [Mitigated] Increased latency particular for queries.\n- Data Catalog: [Mitigated]\nWorkaround: In general, retry failed requests or use an alternative region than us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible. Please see the additional details below for product specific workarounds where available:\n- Dataflow: If feasible, customers can consider restarting jobs again. They should be restarted in an unimpacted zone.\n- Persistent Disk: Use a different zone or PD-SSD disks.\n- Cloud Networking: If you are using load balancers or HA configuration, failover away from us-east1-c","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T22:31:50+00:00","modified":"2022-06-04T22:31:50+00:00","when":"2022-06-04T22:31:50+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: We believe the issue with Apigee, Cloud Data Fusion, Cloud Filestore, Cloud Load Balancing, Cloud Run, Data Catalog, Datastream, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Dataflow, Google Cloud Datastore, Google Cloud Functions, Google Cloud Networking, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Tasks, Google Compute Engine, Google Kubernetes Engine, Persistent Disk, Virtual Private Cloud (VPC) is partially resolved.\nWe believe most service impact was mitigated by 2022-06-04 13:30 US/Pacific. We do not have an ETA for full resolution at this point.\nWe will provide an update by Saturday, 2022-06-04 17:00 US/Pacific with current details.\nDiagnosis: Please see the additional details below for product specific impact where available:\n- BigQuery: [Mitigated] Elevated BigQuery latency and errors on Queries.\n- Dataflow: [Mitigated] Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: [Mitigated] Elevated unavailability.\n- Cloud Tasks: [Mitigated] Elevated unavailability.\n- Cloud Pub/Sub: [Mitigated] Increased errors and/or latency.\n- Persistent Disk: [Mitigated] 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\n- Cloud Run: [Mitigated] Elevated unavailability.\n- Compute Engine: No impact outside of side effects from underlying Persistent Disk impact.\n- Cloud Networking: [Mitigated] Entire zone us-east1-c has delayed firewall programming for new changes to data plane, and delays launching or ssh'ing to instances. Existing programming should continue to work.\n- Cloud Functions: [Mitigated] Elevated unavailability.\n- Cloud Composer: [Mitigated] Creations fail because of errors creating App Engine apps, may be in additional regions.\n- Datastore: [Mitigated] Increased latency particular for queries.\n- Data Catalog: [Mitigated]\nWorkaround: In general, retry failed requests or use an alternative region than us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible. Please see the additional details below for product specific workarounds where available:\n- Dataflow: If feasible, customers can consider restarting jobs again. They should be restarted in an unimpacted zone.\n- Persistent Disk: Use a different zone or PD-SSD disks.\n- Cloud Networking: If you are using load balancers or HA configuration, failover away from us-east1-c","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T21:21:43+00:00","modified":"2022-06-04T21:21:43+00:00","when":"2022-06-04T21:21:43+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: We believe the issue with Apigee, Cloud Data Fusion, Cloud Filestore, Cloud Load Balancing, Cloud Run, Data Catalog, Datastream, Google App Engine, Google BigQuery, Google Cloud Bigtable, Google Cloud Composer, Google Cloud Dataflow, Google Cloud Datastore, Google Cloud Functions, Google Cloud Networking, Google Cloud Pub/Sub, Google Cloud SQL, Google Cloud Tasks, Google Compute Engine, Google Kubernetes Engine, Persistent Disk, Virtual Private Cloud (VPC) is partially resolved.\nWe believe we have addressed the root cause and are seeing substantial improvements. Currently we are working to verify mitigation for each of the services. We do not have an ETA for full resolution at this point.\nWe will provide an update by Saturday, 2022-06-04 15:30 US/Pacific with current details.\nDiagnosis: Please see the additional details below for product specific impact where available:\n- BigQuery: [Mitigated] Elevated BigQuery latency and errors on Queries.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: [Mitigated] Elevated unavailability.\n- Cloud Tasks: [Mitigated] Elevated unavailability.\n- Cloud Pub/Sub: Increased errors and/or latency.\n- Persistent Disk: 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\n- Cloud Run: [Mitigated] Elevated unavailability.\n- Compute Engine: No impact outside of side effects from underlying Persistent Disk impact.\n- Cloud Networking: Entire zone us-east1-c has delayed firewall programming for new changes to data plane, and delays launching or ssh'ing to instances. Existing programming should continue to work.\n- Cloud Functions: [Mitigated] Elevated unavailability.\n- Cloud Composer: Creations fail because of errors creating App Engine apps, may be in additional regions.\n- Datastore: [Mitigated] Increased latency particular for queries.\n- Data Catalog: [Mitigated]\nWorkaround: In general, retry failed requests or use an alternative region than us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible. Please see the additional details below for product specific workarounds where available:\n- Dataflow: If feasible, customers can consider restarting jobs again. They should be restarted in an unimpacted zone.\n- Persistent Disk: Use a different zone or PD-SSD disks.\n- Cloud Networking: If you are using load balancers or HA configuration, failover away from us-east1-c","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T20:57:54+00:00","modified":"2022-06-04T21:00:29+00:00","when":"2022-06-04T20:57:54+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Saturday, 2022-06-04 14:00 US/Pacific.\nDiagnosis: Please see the additional details below for product specific impact where available:\n- BigQuery: [Mitigated] Elevated BigQuery latency and errors on Queries.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: [Mitigated] Elevated unavailability.\n- Cloud Tasks: [Mitigated] Elevated unavailability.\n- Cloud Pub/Sub: Increased errors and/or latency.\n- Persistent Disk: 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\n- Cloud Run: [Mitigated] Elevated unavailability.\n- Compute Engine: No impact outside of side effects from underlying Persistent Disk impact.\n- Cloud Networking: Entire zone us-east1-c has delayed firewall programming for new changes to data plane, and delays launching or ssh'ing to instances. Existing programming should continue to work.\n- Cloud Functions: [Mitigated] Elevated unavailability.\n- Cloud Composer: Creations fail because of errors creating App Engine apps, may be in additional regions.\n- Datastore: [Mitigated] Increased latency particular for queries.\n- Data Catalog: [Mitigated]\nWorkaround: In general, retry failed requests or use an alternative region than us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible. Please see the additional details below for product specific workarounds where available:\n- Dataflow: If feasible, customers can consider restarting jobs again. They should be restarted in an unimpacted zone.\n- Persistent Disk: Use a different zone or PD-SSD disks.\n- Cloud Networking: If you are using load balancers or HA configuration, failover away from us-east1-c","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T19:50:22+00:00","modified":"2022-06-04T21:00:35+00:00","when":"2022-06-04T19:50:22+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Saturday, 2022-06-04 14:00 US/Pacific.\nDiagnosis: Please see the additional details below for product specific impact where available:\n- BigQuery: Elevated BigQuery latency and errors on Queries.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: [Mitigated] Elevated unavailability.\n- Cloud Tasks: [Mitigated] Elevated unavailability.\n- Cloud Pub/Sub: Increased errors and/or latency.\n- Persistent Disk: 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\n- Cloud Run: [Mitigated] Elevated unavailability.\n- Compute Engine: No impact outside of side effects from underlying Persistent Disk impact.\n- Cloud Networking: Entire zone us-east1-c has delayed firewall programming for new changes to data plane, and delays launching or ssh'ing to instances. Existing programming should continue to work.\n- Cloud Functions: [Mitigated] Elevated unavailability.\nWorkaround: In general, retry failed requests or use an alternative region than us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible. Please see the additional details below for product specific workarounds where available:\n- Dataflow: If feasible, customers can consider restarting jobs again. They should be restarted in an unimpacted zone.\n- Persistent Disk: Use a different zone or PD-SSD disks.\n- Cloud Networking: If you are using load balancers or HA configuration, failover away from us-east1-c","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T19:01:36+00:00","modified":"2022-06-04T21:00:40+00:00","when":"2022-06-04T19:01:36+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: Mitigation work is still underway by our engineering team.\nWe will provide more information by Saturday, 2022-06-04 13:00 US/Pacific.\nDiagnosis: Please see the additional details below for product specific impact where available:\n- BigQuery: Elevated BigQuery latency and errors on Queries.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: [Mitigated] Elevated unavailability.\n- Cloud Tasks: [Mitigated] Elevated unavailability.\n- Cloud Pub/Sub: Increased errors and/or latency.\n- Persistent Disk: 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\n- Cloud Run: [Mitigated] Elevated unavailability.\n- Compute Engine: No impact outside of side effects from underlying Persistent Disk impact.\n- Cloud Networking: Entire zone us-east1-c has delayed firewall programming for new changes to data plane, and delays launching or ssh'ing to instances. Existing programming should continue to work.\n- Cloud Functions: [Mitigated] Elevated unavailability.\nWorkaround: In general, retry failed requests or use an alternative region than us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible. Please see the additional details below for product specific workarounds where available:\n- Dataflow: If feasible, customers can consider restarting jobs again. They should be restarted in an unimpacted zone.\n- Persistent Disk: Use a different zone or PD-SSD disks.\n- Cloud Networking: If you are using load balancers or HA configuration, failover away from us-east1-c","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T18:43:38+00:00","modified":"2022-06-04T21:01:03+00:00","when":"2022-06-04T18:43:38+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Saturday, 2022-06-04 13:00 US/Pacific.\nDiagnosis:\nPlease see the additional details below for product specific impact where available:\n- BigQuery: Elevated BigQuery latency and errors on Queries.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: Elevated unavailability.\n- Cloud Tasks: Elevated unavailability.\n- Cloud Pub/Sub: Increased errors and/or latency.\n- Persistent Disk: 1% of PD-Standard (PD-HDD) disks in us-east1-c, and regional disks in us-east-1 are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\n- Cloud Bigtable: Elevated latency and errors in east1-c.\nWorkaround: Retry failed requests or use an alternative region that us-east1 for regional services, and an alternative zone than us-east1-c for zonal services where possible.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T18:22:38+00:00","modified":"2022-06-04T21:00:54+00:00","when":"2022-06-04T18:22:38+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: Our engineering team continues to investigate the issue, which appears to be impacting multiple products in us-east1.\nWe will provide an update by Saturday, 2022-06-04 12:30 US/Pacific with current details.\nDiagnosis: Please see the following details for product specific impact:\n- BigQuery: Elevated BigQuery latency and errors on Queries.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\n- App Engine: Elevated unavailability.\n- Cloud Tasks: Elevated unavailability.\n- Cloud Pub/Sub: Elevated unavailability.\n- Persistent DIsk: Small number of PD-Standard (PD-HDD) disks in us-east1-c only are experiencing slow or stuck disk reads (operations hanging indefinitely), which may cause instances to become unresponsive.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T17:55:42+00:00","modified":"2022-06-04T21:00:49+00:00","when":"2022-06-04T17:55:42+00:00","text":"Summary: us-east1: Elevated errors affecting multiple services.\nDescription: Our engineering team continues to investigate the issue, which appears to be impacting multiple products in us-east1.\nWe will provide an update by Saturday, 2022-06-04 12:00 US/Pacific with current details.\nDiagnosis:\n- BigQuery: Elevated BigQuery errors.\n- Dataflow: Some customers will see stuckness or slowness in their Dataflow batch and streaming jobs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-06-04T17:40:29+00:00","modified":"2022-06-04T17:40:37+00:00","when":"2022-06-04T17:40:29+00:00","text":"Summary: us-east1: Elevated BigQuery errors.\nDescription: We are experiencing an issue with Google BigQuery beginning at Saturday, 2022-06-04 10:20 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Saturday, 2022-06-04 11:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]}],"most_recent_update":{"created":"2022-06-06T09:14:00+00:00","modified":"2022-06-06T09:14:00+00:00","when":"2022-06-06T09:14:00+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 04 June 2022 10:20\n**Incident End:** 04 June 2022 13:24\n**Duration:** 3 hours 04 minutes\n**Affected Services and Features:**\nBigQuery\n**Regions/Zones:** us-east1\n**Description:**\nBigQuery experienced elevated latency for a period of 3 hrs 04 minutes. Preliminary root cause of the issue was an outage in an internal Google storage system causing some bigquery data operations in the cell to fail.\n**Customer Impact:**\nCustomers might have experienced failures or slowness in their requests to BigQuery's us-east1 region.","status":"AVAILABLE","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"},{"title":"Apigee","id":"9Y13BNFy4fJydvjdsN3X"},{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"},{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"},{"title":"Data Catalog","id":"TFedVRYgKGRGMSJrUpup"},{"title":"Datastream","id":"ibJgP4CNKnFojHHw8L3s"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Composer","id":"YxkG5FfcC42cQmvBCk4j"},{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"},{"title":"Google Cloud Datastore","id":"MaS3dKoqp1oqkea4qB9U"},{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Google Cloud Tasks","id":"tMWyzhyKK4rAzAf7x62h"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"},{"title":"Cloud Data Fusion","id":"rLKDHeeaBiXTeutF1air"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"}],"uri":"incidents/6sfxucMj6DuhVzmj9Xr8","currently_affected_locations":[],"previously_affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"id":"XdQANahFJvMuxxkgA34K","number":"8853971020270422709","begin":"2022-06-02T17:10:00+00:00","created":"2022-06-02T20:58:28+00:00","end":"2022-06-02T21:30:00+00:00","modified":"2022-06-03T20:07:35+00:00","external_desc":"Global: Vertex AI Online Prediction Is Experiencing Increased Error Rates","updates":[{"created":"2022-06-03T20:06:43+00:00","modified":"2022-06-03T20:06:43+00:00","when":"2022-06-03T20:06:43+00:00","text":"# Mini Incident Report\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 02 June 2022 10:10 US/Pacific\n**Incident End:** 02 June 2022 14:30 US/Pacific\n**Duration:** 4 hours, 20 minutes\n**Affected Services and Features:** Vertex AI Online Prediction\n**Regions/Zones:** Global\n**Description:**\nVertex AI Online Prediction experienced increased error rates from 30% up to 100% per region depending on user usage patterns for a duration of 4 hours, 20 minutes. From preliminary analysis, the root cause of the issue was that Vertex Prediction Endpoints were globally marked as deleted due to faulty resource cleanup process. The service fully recovered when the Vertex Prediction Endpoints were restored.\n**Customer Impact:**\nAffected customers may have experienced:\n- All pre-existing Vertex models undeployed on Vertex AI Endpoints\n- Empty responses when listing the deployed models\n- Runtime exceptions and general errors on Predict and Explain requests\n- Quota failure when trying to re-deploy models","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-02T22:24:14+00:00","modified":"2022-06-02T22:24:15+00:00","when":"2022-06-02T22:24:14+00:00","text":"The issue with Vertex AI Online Prediction has been resolved for all affected users as of Thursday, 2022-06-02 15:21 US/Pacific.\nWe will publish an analysis of this incident once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-02T21:51:12+00:00","modified":"2022-06-02T21:51:12+00:00","when":"2022-06-02T21:51:12+00:00","text":"Summary: Global: Vertex AI Online Prediction Is Experiencing Increased Error Rates\nDescription: We believe the issue with Vertex AI Online Prediction is partially resolved.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Thursday, 2022-06-02 16:10 US/Pacific with current details.\nDiagnosis: For affected customers: When listing the deployed models in Endpoints, the list will be empty and Predict and Explain requests would fail.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-02T21:50:34+00:00","modified":"2022-06-02T21:50:35+00:00","when":"2022-06-02T21:50:34+00:00","text":"Summary: Global: Vertex AI Online Prediction Is Experiencing Increased Error Rates\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-06-02 15:07 US/Pacific.\nWe will provide more information by Thursday, 2022-06-02 15:07 US/Pacific.\nDiagnosis: For affected customers: When listing the deployed models in Endpoints, the list will be empty and Predict and Explain requests would fail.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-02T21:06:31+00:00","modified":"2022-06-02T21:06:32+00:00","when":"2022-06-02T21:06:31+00:00","text":"Summary: Global: Vertex AI Online Prediction Is Experiencing Increased Error Rates\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-06-02 15:07 US/Pacific.\nWe will provide more information by Thursday, 2022-06-02 15:07 US/Pacific.\nDiagnosis: Customers will experiences increased error rates.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"created":"2022-06-02T20:58:27+00:00","modified":"2022-06-02T20:58:28+00:00","when":"2022-06-02T20:58:27+00:00","text":"Summary: Global: Vertex AI Online Prediction Is Experiencing Increased Error Rates\nDescription: We are experiencing an issue with Vertex AI Online Prediction beginning at Thursday, 2022-06-02 10:20 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-06-02 14:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers will experiences errors\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]}],"most_recent_update":{"created":"2022-06-03T20:06:43+00:00","modified":"2022-06-03T20:06:43+00:00","when":"2022-06-03T20:06:43+00:00","text":"# Mini Incident Report\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 02 June 2022 10:10 US/Pacific\n**Incident End:** 02 June 2022 14:30 US/Pacific\n**Duration:** 4 hours, 20 minutes\n**Affected Services and Features:** Vertex AI Online Prediction\n**Regions/Zones:** Global\n**Description:**\nVertex AI Online Prediction experienced increased error rates from 30% up to 100% per region depending on user usage patterns for a duration of 4 hours, 20 minutes. From preliminary analysis, the root cause of the issue was that Vertex Prediction Endpoints were globally marked as deleted due to faulty resource cleanup process. The service fully recovered when the Vertex Prediction Endpoints were restored.\n**Customer Impact:**\nAffected customers may have experienced:\n- All pre-existing Vertex models undeployed on Vertex AI Endpoints\n- Empty responses when listing the deployed models\n- Runtime exceptions and general errors on Predict and Explain requests\n- Quota failure when trying to re-deploy models","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Vertex AI Online Prediction","id":"sdXM79fz1FS6ekNpu37K"},{"title":"Cloud Machine Learning","id":"z9PfKanGZYvYNUbnKzRJ"}],"uri":"incidents/XdQANahFJvMuxxkgA34K","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"}]},{"id":"YkkPCVAzdyuNJJFS54kM","number":"666965533070443831","begin":"2022-05-31T19:32:50+00:00","created":"2022-05-31T19:50:19+00:00","end":"2022-05-31T20:56:11+00:00","modified":"2022-05-31T20:56:11+00:00","external_desc":"Cloud Build ListBuild HTTP calls having increased latency and timeouts","updates":[{"created":"2022-05-31T20:56:10+00:00","modified":"2022-05-31T20:56:11+00:00","when":"2022-05-31T20:56:10+00:00","text":"The issue with Cloud Build has been resolved for all affected users as of Tuesday, 2022-05-31 13:44 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-05-31T19:51:25+00:00","modified":"2022-05-31T19:51:25+00:00","when":"2022-05-31T19:51:25+00:00","text":"Summary: Cloud Build ListBuild HTTP calls having increased latency and timeouts\nDescription: We are experiencing an issue with Cloud Build beginning at Tuesday, 2022-05-31 11:04 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-05-31 13:59 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers making google.devtools.cloudbuild.v1.CloudBuild.ListBuilds HTTP calls may see elevated latency or timeouts.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]},{"created":"2022-05-31T19:50:19+00:00","modified":"2022-05-31T19:50:20+00:00","when":"2022-05-31T19:50:19+00:00","text":"Summary: Cloud Build ListBuild HTTP calls having increased latency and timeouts\nDescription: We are experiencing an issue with Cloud Build beginning at Tuesday, 2022-05-31 11:04 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-05-31 13:59 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Cloud Build ListBuild HTTP calls having increased latency and timeouts\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-31T20:56:10+00:00","modified":"2022-05-31T20:56:11+00:00","when":"2022-05-31T20:56:10+00:00","text":"The issue with Cloud Build has been resolved for all affected users as of Tuesday, 2022-05-31 13:44 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Build","id":"fw8GzBdZdqy4THau7e1y"}],"uri":"incidents/YkkPCVAzdyuNJJFS54kM","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"haJamsyevGnru71kfZRM","number":"7009434990327813350","begin":"2022-05-31T15:14:32+00:00","created":"2022-05-31T15:14:50+00:00","end":"2022-05-31T17:39:45+00:00","modified":"2022-05-31T17:39:45+00:00","external_desc":"Networking metrics in Cloud Monitoring partially unavailable in europe-west4, us-central*","updates":[{"created":"2022-05-31T17:39:45+00:00","modified":"2022-05-31T17:39:45+00:00","when":"2022-05-31T17:39:45+00:00","text":"The issue with Cloud Monitoring has been resolved for all affected users as of Tuesday, 2022-05-31 10:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-31T17:32:56+00:00","modified":"2022-05-31T17:32:58+00:00","when":"2022-05-31T17:32:56+00:00","text":"Summary: Networking metrics in Cloud Monitoring partially unavailable in europe-west4, us-central*\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-05-31 12:00 US/Pacific.\nDiagnosis: Customers will see unavailable Cloud Networking metrics in affected regions (in europe-west4, us-central*)\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]},{"created":"2022-05-31T15:57:46+00:00","modified":"2022-05-31T15:57:47+00:00","when":"2022-05-31T15:57:46+00:00","text":"Summary: Networking metrics in Cloud Monitoring partially unavailable in europe-west4, us-central*\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-05-31 10:34 US/Pacific.\nDiagnosis: Customers will see unavailable Cloud Networking metrics in affected regions (in europe-west4, us-central*)\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]},{"created":"2022-05-31T15:14:40+00:00","modified":"2022-05-31T15:14:52+00:00","when":"2022-05-31T15:14:40+00:00","text":"Summary: Networking metrics in Cloud Monitoring partially unavailable in europe-west4, us-central*\nDescription: We are experiencing an intermittent issue with Cloud Monitoring beginning at Tuesday, 2022-05-31 00:00 US/Pacific beginning at Tuesday, 2022-05-31 06:15 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-05-31 09:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers will see unavailable Cloud Networking metrics in affected regions (in europe-west4, us-central*)\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-31T17:39:45+00:00","modified":"2022-05-31T17:39:45+00:00","when":"2022-05-31T17:39:45+00:00","text":"The issue with Cloud Monitoring has been resolved for all affected users as of Tuesday, 2022-05-31 10:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"}],"uri":"incidents/haJamsyevGnru71kfZRM","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"ipisoukd9hNno6SbpWWE","number":"13688798534542981487","begin":"2022-05-27T20:56:06+00:00","created":"2022-05-27T20:56:18+00:00","end":"2022-05-27T21:04:41+00:00","modified":"2022-05-27T21:04:41+00:00","external_desc":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","updates":[{"created":"2022-05-27T21:04:40+00:00","modified":"2022-05-27T21:04:42+00:00","when":"2022-05-27T21:04:40+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-27T20:56:10+00:00","modified":"2022-05-27T20:56:20+00:00","when":"2022-05-27T20:56:10+00:00","text":"Summary: Europe : Eventarc GCS trigger mutations failing sporadically in Europe\nDescription: We are experiencing an intermittent issue with Eventarc beginning at Friday, 2022-05-26 20:56 US/Pacific. Users may receive an internal error when manipulating GCS triggers.\nOur engineering team continues to investigate the issue. Users can retry manipulating the trigger.\nWe will provide an update by Friday, 2022-05-27 14:20 US/Pacific with current details.\nDiagnosis: Eventarc users may receive an internal error when manipulating GCS triggers\nWorkaround: Users can retry manipulating the trigger","status":"SERVICE_INFORMATION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-27T21:04:40+00:00","modified":"2022-05-27T21:04:42+00:00","when":"2022-05-27T21:04:40+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"YaFawoMaXnqgY4keUBnW","service_name":"Eventarc","affected_products":[{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"}],"uri":"incidents/ipisoukd9hNno6SbpWWE","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"eMrkQFQCj1Xmf3trQRNt","number":"9948110675578981301","begin":"2022-05-27T20:42:25+00:00","created":"2022-05-27T21:26:15+00:00","end":"2022-05-27T22:12:21+00:00","modified":"2022-05-27T22:12:22+00:00","external_desc":"Europe: Customers may experience elevated errors with Google Cloud Storage API operation.","updates":[{"created":"2022-05-27T22:12:16+00:00","modified":"2022-05-27T22:12:22+00:00","when":"2022-05-27T22:12:16+00:00","text":"The issue with Eventarc, Google Cloud Storage has been resolved for all affected projects as of Friday, 2022-05-27 15:11 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-27T21:50:05+00:00","modified":"2022-05-27T21:50:07+00:00","when":"2022-05-27T21:50:05+00:00","text":"Summary: Europe: Customers may experience elevated errors with Google Cloud Storage API operation.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Friday, 2022-05-27 18:00 US/Pacific.\nWe will provide more information by Friday, 2022-05-27 18:00 US/Pacific.\nDiagnosis: Customers may experience elevated CLOUD_IAM_SERVICE_ERROR with GetProjectServiceAccount calls to GCS.\nEventarc users creating, updating, or deleting GCS triggers in European regions may get an internal error.\nWorkaround: Eventarc users can retry the create/update/delete operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Multi-region: eu","id":"eu"}]},{"created":"2022-05-27T21:26:14+00:00","modified":"2022-05-27T21:26:16+00:00","when":"2022-05-27T21:26:14+00:00","text":"Summary: Europe: Customers may experience elevated errors with Google Cloud Storage API operation.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Friday, 2022-05-27 17:30 US/Pacific.\nWe will provide more information by Friday, 2022-05-27 18:00 US/Pacific.\nDiagnosis: Customers may experience elevated CLOUD_IAM_SERVICE_ERROR with GetProjectServiceAccount calls to GCS.\nEventarc users creating, updating, or deleting GCS triggers in European regions may get an internal error.\nWorkaround: Eventarc users can retry the create/update/delete operation.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-05-27T22:12:16+00:00","modified":"2022-05-27T22:12:22+00:00","when":"2022-05-27T22:12:16+00:00","text":"The issue with Eventarc, Google Cloud Storage has been resolved for all affected projects as of Friday, 2022-05-27 15:11 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"}],"uri":"incidents/eMrkQFQCj1Xmf3trQRNt","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Multi-region: eu","id":"eu"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Madrid (europe-southwest1)","id":"europe-southwest1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Milan (europe-west8)","id":"europe-west8"},{"title":"Paris (europe-west9)","id":"europe-west9"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"qD5T5M49jVJvj4NXvYjK","number":"11614801264955688536","begin":"2022-05-27T19:59:44+00:00","created":"2022-05-27T20:22:44+00:00","end":"2022-05-27T20:33:58+00:00","modified":"2022-05-27T20:33:58+00:00","external_desc":"us-east4: Cloud Interconnects are experiencing latency spikes and packet loss.","updates":[{"created":"2022-05-27T20:33:58+00:00","modified":"2022-05-27T20:34:00+00:00","when":"2022-05-27T20:33:58+00:00","text":"The issue with Google Cloud Networking is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Northern Virginia (us-east4)","id":"us-east4"}]},{"created":"2022-05-27T20:22:43+00:00","modified":"2022-05-27T20:22:46+00:00","when":"2022-05-27T20:22:43+00:00","text":"Summary: us-east4: Cloud Interconnects are experiencing latency spikes and packet loss.\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Friday, 2022-05-27 11:03 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-05-27 14:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers with Cloud Interconnects in us-east4 may experience latency spikes and packet loss.\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-27T20:33:58+00:00","modified":"2022-05-27T20:34:00+00:00","when":"2022-05-27T20:33:58+00:00","text":"The issue with Google Cloud Networking is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Northern Virginia (us-east4)","id":"us-east4"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/qD5T5M49jVJvj4NXvYjK","currently_affected_locations":[],"previously_affected_locations":[{"title":"Northern Virginia (us-east4)","id":"us-east4"}]},{"id":"2uFuNdeK9xXSwY9w7cCj","number":"7247953702549949059","begin":"2022-05-26T12:14:30+00:00","created":"2022-05-26T13:52:30+00:00","end":"2022-05-26T15:49:34+00:00","modified":"2022-05-26T15:49:34+00:00","external_desc":"Google Cloud python libraries incompatible with 4.21.0 protobuf library release","updates":[{"created":"2022-05-26T15:49:33+00:00","modified":"2022-05-26T15:49:35+00:00","when":"2022-05-26T15:49:33+00:00","text":"The issue with Google Cloud Functions has been resolved for all affected users as of Thursday, 2022-05-26 08:29 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-26T15:18:35+00:00","modified":"2022-05-26T15:18:40+00:00","when":"2022-05-26T15:18:35+00:00","text":"Summary: Google Cloud python libraries incompatible with 4.21.0 protobuf library release\nDescription: We are experiencing an issue with the python Google Cloud Client libraries beginning on Wednesday, 2022-05-25 18:00 US/Pacific\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-05-26 08:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Impacted customers will see applications failing to initialize with a \"TypeError: Descriptors cannot not be created directly.\" error.\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-26T14:48:27+00:00","modified":"2022-05-26T14:48:32+00:00","when":"2022-05-26T14:48:27+00:00","text":"Summary: Google Cloud python libraries incompatible with 4.21.0 protobuf library release\nDescription: We are experiencing an issue with the python Google Cloud Client libraries beginning on Wednesday, 2022-05-25 18:00 US/Pacific\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-05-26 08:20 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Impacted customers will see applications failing to initialize with a \"TypeError: Descriptors cannot not be created directly.\" error.\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-26T13:52:28+00:00","modified":"2022-05-26T13:52:34+00:00","when":"2022-05-26T13:52:28+00:00","text":"Summary: Google Cloud python libraries incompatible with 4.21.0 protobuf library release\nDescription: We are experiencing an issue with Google Cloud Functions beginning on Wednesday, 2022-05-25 18:00 US/Pacific\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-05-26 07:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected Cloud Functions create and update function operations will fail\nWorkaround: Downgrade the protobuf package to 3.20.x by adding protobuf==3.20.1 to requirements.txt then retry the create/update function operation","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-05-26T15:49:33+00:00","modified":"2022-05-26T15:49:35+00:00","when":"2022-05-26T15:49:33+00:00","text":"The issue with Google Cloud Functions has been resolved for all affected users as of Thursday, 2022-05-26 08:29 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"oW4vJ7VNqyxTWNzSHopX","service_name":"Google Cloud Functions","affected_products":[{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"}],"uri":"incidents/2uFuNdeK9xXSwY9w7cCj","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"Gt6njQyniuxXViQULV2T","number":"18110704032802855247","begin":"2022-05-25T01:04:00+00:00","created":"2022-05-25T07:29:52+00:00","end":"2022-05-25T17:34:00+00:00","modified":"2022-06-02T17:21:24+00:00","external_desc":"Global: BigQuery may experience elevated query latencies or failures.","updates":[{"created":"2022-06-02T17:20:00+00:00","modified":"2022-06-02T17:21:19+00:00","when":"2022-06-02T17:20:00+00:00","text":"**INCIDENT REPORT**\n**Summary:**\nOn 24 May 2022 from 18:04 to 25 May 2022 10:34 Google BigQuery experienced elevated query latencies and job failures in four regions (US multiregion, europe-west1, asia-east1, and asia-east2) for a duration of 16 hours, 30 minutes. A product change roll out was identified as the root cause. The issue was mitigated by rolling back the changes. The rollback was performed in phases to prevent additional impact on the service, which caused some regions to recover earlier than others. Affected customers experienced elevated latencies or failures for QUERY, IMPORT and EXPORT jobs in BigQuery.\n**Root Cause:**\nOn 11 May 2022, BigQuery began a software rollout to improve audit logging for data that is imported from, queried, or exported to Google Cloud Storage (GCS). This rollout inadvertently contained a memory leak that gradually and incrementally manifested whenever these code paths were executed. This memory leak gradually consumed memory on BigQuery’s compute nodes until they were unable to accept new work to execute jobs, including queries, loads, and exports. The software rollout was deployed to all regions before the issue was detected and rolled back. Given the nature of the root cause, significant user load and time were required for the issue to manifest, meaning that the issue was not observed in internal testing, or in most of BigQuery's production regions.\n**Remediation and Prevention:**\nBigQuery will take the following actions to mitigate against such issues happening in the future: - In the short-term, enable comprehensive memory error detector coverage for the affected code path to detect possible memory leaks ahead of time. - In the long-term, ensure such analysis is enabled for all BigQuery code paths.\nIntroduce additional internal monitoring to detect memory pressure scenarios.\n**Detailed Description of Impact**\nIn affected regions, a portion of BigQuery compute nodes were unable to service incoming jobs due to insufficient memory. As a result customer jobs - including queries, loads, and exports - experienced elevated latencies and failures while waiting for nodes with sufficient memory to become available.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T21:54:12+00:00","modified":"2022-05-25T21:54:12+00:00","when":"2022-05-25T21:54:12+00:00","text":"**Mini Incident-Report:**\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 24 May 2022 18:04\n**Incident End:** 25 May 2022 12:19\n**Duration:** 18 hours, 15 minutes\n**Affected Services and Features:**\n* Google BigQuery\n**Regions/Zones:** Global\n**Description:**\nGoogle BigQuery experienced elevated query latencies and job failures globally for a duration of 18 hours, 15 minutes. From preliminary analysis, the root cause of the issue is a product change which had been rolled out globally. The issue was mitigated by rolling back the changes. The rollback was performed in a phased manner to prevent additional impact on the service and some regions would have recovered earlier than others.\n**Customer Impact:**\n* Affected customers would have experienced elevated query latencies or in some cases query failures.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T19:32:23+00:00","modified":"2022-05-25T19:32:25+00:00","when":"2022-05-25T19:32:23+00:00","text":"The issue with Google BigQuery is believed to be affecting a very small number of customers and our Engineering Team continues to work on mitigation across all regions.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T19:30:58+00:00","modified":"2022-05-25T19:31:00+00:00","when":"2022-05-25T19:30:58+00:00","text":"The issue with Google BigQuery is believed to be affecting a very small number of customers and our Engineering Team continues to work on mitigation across all regions.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T19:00:40+00:00","modified":"2022-05-25T19:00:42+00:00","when":"2022-05-25T19:00:40+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures.\nDescription: us multi-region, europe-west1, asia-east1, and asia-east2 regions have recovered and our engineering team continue to monitor the situation for full recovery.\nWe will provide more information by Wednesday, 2022-05-25 13:00 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T16:55:14+00:00","modified":"2022-05-25T16:55:22+00:00","when":"2022-05-25T16:55:14+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures.\nDescription: us multi-region, europe-west1, asia-east1, and asia-east2 regions have recovered and our engineering team is currently monitoring the situation for full recovery.\nWe will provide more information by Wednesday, 2022-05-25 12:00 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T15:51:15+00:00","modified":"2022-05-25T15:51:17+00:00","when":"2022-05-25T15:51:15+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures.\nDescription: The US multi- region is nearly completely recovered and the europe-west1, asia-east1, and asia-east2 regions have fully recovered. Other regions are currently not affected but precautionary actions are being taking globally by our engineering team.\nWe will provide more information by Wednesday, 2022-05-25 10:00 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Multi-region: eu","id":"eu"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Multi-region: us","id":"us"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T15:26:27+00:00","modified":"2022-05-25T15:26:33+00:00","when":"2022-05-25T15:26:27+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures.\nDescription: All Global locations including multi-region locations for BigQuery are impacted.\nEurope-west1 and Asia-east2 are currently mitigated\nMitigation work is currently underway for the remaining impacted regions by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 09:00 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T14:40:41+00:00","modified":"2022-05-25T14:40:48+00:00","when":"2022-05-25T14:40:41+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures.\nDescription: All Global locations including multi-region locations for BigQuery are impacted.\nEurope-west1 is currently mitigated\nMitigation work is currently underway for the remaining impacted regions by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 08:45 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T13:44:08+00:00","modified":"2022-05-25T13:44:12+00:00","when":"2022-05-25T13:44:08+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures.\nDescription: All Global locations including multi-region locations for BigQuery are impacted.\nEurope-west1 is currently mitigated\nMitigation work is currently underway for the remaining impacted regions by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 07:30 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Multi-region: eu","id":"eu"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Multi-region: us","id":"us"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T13:25:02+00:00","modified":"2022-05-25T13:25:09+00:00","when":"2022-05-25T13:25:02+00:00","text":"Summary: Global: BigQuery may experience elevated query latencies or failures\nDescription: Europe-west1 is currently mitigated\nMitigation work is currently underway for the remaining impacted regions by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 07:30 US/Pacific.\nDiagnosis: Customers will experience Increased Query latency and possible job failures.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-25T12:00:07+00:00","modified":"2022-05-25T12:00:18+00:00","when":"2022-05-25T12:00:07+00:00","text":"Summary: BigQuery degraded in us-multiregion, europe-west1, asia-northeast1, asia-southeast1, asia-southeast2, eu-canary, europe-west2, europe-west6, europe-west9, us-central1, us-west4. Customers will experience Increased Query latency and possible job failures.\nDescription: Europe-west1 is currently mitigated and mitigation work is currently underway for the remaining impacted regions by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 06:30 US/Pacific.\nDiagnosis: Increased Query latency and possible job failures\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-05-25T10:49:00+00:00","modified":"2022-05-25T10:49:06+00:00","when":"2022-05-25T10:49:00+00:00","text":"Summary: BigQuery degraded in us-multiregion and europe-west1. Customers will experience Increased Query latency and possible job failures.\nDescription: Europe-west1 is currently mitigated and mitigation work is currently underway for the remaining impacted regions by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 05:00 US/Pacific.\nDiagnosis: Increased Query latency and possible job failures\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-05-25T08:11:47+00:00","modified":"2022-05-25T08:11:48+00:00","when":"2022-05-25T08:11:47+00:00","text":"Summary: BigQuery degraded in us-multiregion and europe-west1. Customers will experience Increased Query latency and possible job failures.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 03:36 US/Pacific.\nDiagnosis: Increased Query latency and possible job failures\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-05-25T07:29:52+00:00","modified":"2022-05-25T07:29:54+00:00","when":"2022-05-25T07:29:52+00:00","text":"Summary: BigQuery degraded in multiple regions. Customers will experience Increased Query latency and possible job failures\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-05-25 01:36 US/Pacific.\nDiagnosis: Increased Query latency and possible job failures\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Multi-region: us","id":"us"}]}],"most_recent_update":{"created":"2022-06-02T17:20:00+00:00","modified":"2022-06-02T17:21:19+00:00","when":"2022-06-02T17:20:00+00:00","text":"**INCIDENT REPORT**\n**Summary:**\nOn 24 May 2022 from 18:04 to 25 May 2022 10:34 Google BigQuery experienced elevated query latencies and job failures in four regions (US multiregion, europe-west1, asia-east1, and asia-east2) for a duration of 16 hours, 30 minutes. A product change roll out was identified as the root cause. The issue was mitigated by rolling back the changes. The rollback was performed in phases to prevent additional impact on the service, which caused some regions to recover earlier than others. Affected customers experienced elevated latencies or failures for QUERY, IMPORT and EXPORT jobs in BigQuery.\n**Root Cause:**\nOn 11 May 2022, BigQuery began a software rollout to improve audit logging for data that is imported from, queried, or exported to Google Cloud Storage (GCS). This rollout inadvertently contained a memory leak that gradually and incrementally manifested whenever these code paths were executed. This memory leak gradually consumed memory on BigQuery’s compute nodes until they were unable to accept new work to execute jobs, including queries, loads, and exports. The software rollout was deployed to all regions before the issue was detected and rolled back. Given the nature of the root cause, significant user load and time were required for the issue to manifest, meaning that the issue was not observed in internal testing, or in most of BigQuery's production regions.\n**Remediation and Prevention:**\nBigQuery will take the following actions to mitigate against such issues happening in the future: - In the short-term, enable comprehensive memory error detector coverage for the affected code path to detect possible memory leaks ahead of time. - In the long-term, ensure such analysis is enabled for all BigQuery code paths.\nIntroduce additional internal monitoring to detect memory pressure scenarios.\n**Detailed Description of Impact**\nIn affected regions, a portion of BigQuery compute nodes were unable to service incoming jobs due to insufficient memory. As a result customer jobs - including queries, loads, and exports - experienced elevated latencies and failures while waiting for nodes with sufficient memory to become available.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/Gt6njQyniuxXViQULV2T","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Multi-region: eu","id":"eu"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Multi-region: us","id":"us"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"rrmfPRaESXcrpfxPo5y6","number":"17140268093845218737","begin":"2022-05-24T14:01:00+00:00","created":"2022-05-24T14:51:27+00:00","end":"2022-05-24T14:50:00+00:00","modified":"2022-05-24T18:52:35+00:00","external_desc":"Degraded serving for Google App Engine and Cloud Functions users in us-central1","updates":[{"created":"2022-05-24T18:36:18+00:00","modified":"2022-05-24T18:52:35+00:00","when":"2022-05-24T18:36:18+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 24 May 2022 07:01\n**Incident End:** 24 May 2022 07:50\n**Duration:** 49 minutes\n**Affected Services and Features:**\n* App Engine\n* Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Cloud Functions experienced elevated request latencies for a duration of 49 minutes. From preliminary analysis, the root cause of the issue is an unexpected surge in traffic which increased resource usage. The issue was mitigated by implementing safeguards to prevent resource starvation.\nWe continue to investigate the nature of the traffic surge and we will identify additional safeguards to prevent recurrence of the issue.\n**Customer Impact:**\n* Affected customers would have experienced elevated latencies for App Engine app serving and Cloud Function executions.\n* Less than 1% of the customers in us-central1 would have seen App Engine and Cloud Function requests failing with errors.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-24T15:03:45+00:00","modified":"2022-05-24T15:03:52+00:00","when":"2022-05-24T15:03:45+00:00","text":"The issue with Google App Engine has been resolved for all affected users as of Tuesday, 2022-05-24 07:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-24T14:51:18+00:00","modified":"2022-05-24T14:51:30+00:00","when":"2022-05-24T14:51:18+00:00","text":"Summary: Degraded serving for Google App Engine and Cloud Functions users in us-central1\nDescription: We are experiencing an issue with Google App Engine beginning at Tuesday, 2022-05-24 07:01 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-05-24 08:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers are seeing a high serving latency and request failures\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-05-24T18:36:18+00:00","modified":"2022-05-24T18:52:35+00:00","when":"2022-05-24T18:36:18+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 24 May 2022 07:01\n**Incident End:** 24 May 2022 07:50\n**Duration:** 49 minutes\n**Affected Services and Features:**\n* App Engine\n* Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Cloud Functions experienced elevated request latencies for a duration of 49 minutes. From preliminary analysis, the root cause of the issue is an unexpected surge in traffic which increased resource usage. The issue was mitigated by implementing safeguards to prevent resource starvation.\nWe continue to investigate the nature of the traffic surge and we will identify additional safeguards to prevent recurrence of the issue.\n**Customer Impact:**\n* Affected customers would have experienced elevated latencies for App Engine app serving and Cloud Function executions.\n* Less than 1% of the customers in us-central1 would have seen App Engine and Cloud Function requests failing with errors.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"kchyUtnkMHJWaAva8aYc","service_name":"Google App Engine","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/rrmfPRaESXcrpfxPo5y6","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"JqefFhK5oa22EzFGhtg3","number":"419743856759571945","begin":"2022-05-23T08:26:00+00:00","created":"2022-05-23T16:01:37+00:00","end":"2022-05-23T16:30:00+00:00","modified":"2022-05-23T21:55:04+00:00","external_desc":"Some customer VMs may be unreachable in us-central1-b.","updates":[{"created":"2022-05-23T21:55:03+00:00","modified":"2022-05-23T21:55:03+00:00","when":"2022-05-23T21:55:03+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 23 May 2021 01:26\n**Incident End:** 23 May 2021 09:30\n**Duration:** 8 hours, 4 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking\n**Regions/Zones:** us-central1-b\n**Description:**\nGoogle Cloud Networking experienced issues causing Google Compute Engine instances to be unreachable in us-central1-b for a duration of 8 hours, 4 minutes. From preliminary analysis, the root cause is a roll out in us-central1-b. The issue was mitigated by rolling back the changes.\n**Customer Impact:**\nAffected customers would have experienced issues with network traffic between Google Compute Engine instances within us-central1-b of the same VPC.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-23T16:35:56+00:00","modified":"2022-05-23T16:35:58+00:00","when":"2022-05-23T16:35:56+00:00","text":"The issue with Google Cloud Networking has been resolved for all affected users as of Monday, 2022-05-23 09:29 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-23T16:05:11+00:00","modified":"2022-05-23T16:05:13+00:00","when":"2022-05-23T16:05:11+00:00","text":"Summary: Some customer VMs may be unreachable in us-central1-b.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Monday, 2022-05-23 10:00 US/Pacific.\nWe will provide more information by Monday, 2022-05-23 10:30 US/Pacific.\nDiagnosis: VMs may be unreachable in us-central1-b.\nWorkaround: Customer can try migrating VMs to another cluster in us-central1-b.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-23T16:01:35+00:00","modified":"2022-05-23T16:01:39+00:00","when":"2022-05-23T16:01:35+00:00","text":"Summary: Some customer VMs may be unreachable in us-central1-b.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Monday, 2022-05-23 10:00 US/Pacific.\nWe will provide more information by Monday, 2022-05-23 10:00 US/Pacific.\nDiagnosis: VMs may be unreachable in us-central1-b.\nWorkaround: Customer can try migrating VMs to another cluster in us-central1-b.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-23T21:55:03+00:00","modified":"2022-05-23T21:55:03+00:00","when":"2022-05-23T21:55:03+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 23 May 2021 01:26\n**Incident End:** 23 May 2021 09:30\n**Duration:** 8 hours, 4 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking\n**Regions/Zones:** us-central1-b\n**Description:**\nGoogle Cloud Networking experienced issues causing Google Compute Engine instances to be unreachable in us-central1-b for a duration of 8 hours, 4 minutes. From preliminary analysis, the root cause is a roll out in us-central1-b. The issue was mitigated by rolling back the changes.\n**Customer Impact:**\nAffected customers would have experienced issues with network traffic between Google Compute Engine instances within us-central1-b of the same VPC.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/JqefFhK5oa22EzFGhtg3","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"eo76pxZiDgWVz4z3kmUv","number":"14038663271156697675","begin":"2022-05-20T20:47:00+00:00","created":"2022-05-20T21:44:17+00:00","end":"2022-05-20T21:07:00+00:00","modified":"2022-06-02T19:11:24+00:00","external_desc":"Global : Cloud Networking faced severe packet loss","updates":[{"created":"2022-06-02T19:11:24+00:00","modified":"2022-06-02T19:11:24+00:00","when":"2022-06-02T19:11:24+00:00","text":"**INCIDENT REPORT**\n**SUMMARY:**\nOn Friday, 20 May 2022 at 13:47 US/Pacific, Google Cloud Networking experienced intermittent packet loss for traffic between multiple cloud regions for a duration of 20 minutes. The issue was identified and mitigated automatically by 14:07 US/Pacific.\nWe understand this issue has affected our valued customers and users, and we apologize to those who were affected.\n**ROOT CAUSE:**\nGoogle’s production backbone is a global network that enables connectivity for all user-facing traffic via Points of Presence (POPs) or internet exchanges.\nA failure of a component on a fiber path from one of the central US gateway campuses in Google’s production backbone led to a decrease in available network bandwidth between the gateway and multiple edge locations, causing packet loss while the backbone automatically moved traffic onto remaining paths.\nThe network topology in this region is being augmented, and the second-best path has not completed its augmentation. This meant some traffic needed to reroute onto the third-best path, which led to an extended traffic migration period. This disruption was more severe than we had anticipated, and is the subject of remediation actions below.\n**REMEDIATION AND PREVENTION:**\nGoogle’s automated repair mechanisms detected the decrease in available network bandwidth on Friday, 20 May 2022 at 13:47 US/Pacific and automatically routed the traffic through alternate links. The traffic rerouting completed on Friday, 20 May 2022 at 14:06 US/Pacific, mitigating the issue.\nWhile our automated mechanisms worked as intended and recovered the traffic without manual intervention, we understand that the scope of impact caused by this event affected our customers.\nWe have been working on optimizing our global network to minimize the time spent automatically reconfiguring around failures like this (known as \"convergence time\"). While we have made progress, efforts to improve are ongoing. We continue to ensure that the current technology is optimally configured to minimize the frequency and severity of these issues.\nIn this network region, we intend to complete the augmentation by 11 July 2022, which will return the network to the intended topology where any single fiber path failure can be rerouted quickly onto the second-best path.\nWe will also build automatic analysis to ensure that the network topology during augmentation always supports fast convergence.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\n**DETAILED DESCRIPTION OF IMPACT:**\n- Google Cloud Networking - Affected customers would have observed packet loss for ingress and egress traffic to and from US Central Cloud regions from 13:47 to 14:07 US/Pacific.\n- Cloud VPN - Affected customers would have experienced latency and errors for cross region VPN with global routing from 13:47 to 14:07 US/Pacific. Intra-region traffic for Classic and HA VPN would not have been affected.\n- Cloud Router - Affected customers would have observed delays in global routing propagation from 13:47 to 14:07 US/Pacific.\n- Cloud Interconnect - Affected customers would have experienced latency and errors from 13:47 to 14:07 US/Pacific.\n- Google Cloud Storage - Affected customers would have experienced elevated latency, HTTP 500 errors, or transient API errors from 13:47 to 14:17 US/Pacific.\n- Cloud SQL - Affected customers would have experienced failures for Export, Update, and Delete operations as well as delayed data replication in us-west1 from 13:53 to 14:13 US/Pacific.\n- Messages - Affected customers would have experienced service availability issues from 13:49 to 14:05 US/Pacific.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-23T21:33:36+00:00","modified":"2022-05-23T21:33:36+00:00","when":"2022-05-23T21:33:36+00:00","text":"**Mini Incident Report**\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 20 May 2022 13:47\n**Incident End:** 20 May 2022 14:07\n**Duration:** 20 minutes\n**Affected Services and Features:**\n* Google Cloud Networking\n* Cloud VPN\n* Cloud Router\n* Cloud Interconnect\n* Google Cloud Storage\n* Cloud SQL\n* Messages\n**Regions/Zones:** Multiple regions\n**Description:**\nGoogle Cloud Networking experienced intermittent packet loss for some transit traffic affecting inter-region connectivity for multiple cloud regions, which lasted 20 minutes. From preliminary analysis, the root cause is a hardware issue on an optical (amplification) component affecting Google's user facing backbone. The issue was identified and mitigated by our automated repair mechanism without manual intervention.\n**Customer Impact:** * Google Cloud Networking - Affected customers would have observed packet loss for Ingress and egress traffic to and from central US cloud regions. * Cloud VPN - Affected customers would have experienced latency and errors for cross region VPN with global routing. Intra-region traffic for Classic and HA VPN would not have been affected.\n* Cloud Router - Affected customers would have observed delays in global routing propagation.\n* Cloud Interconnect - Affected customers would have experienced latency and errors.\n* Google Cloud Storage - Affected customers would have experienced elevated latency, HTTP 500 errors, or transient API errors. * Cloud SQL - Affected customers would have experienced failures for Export, Update, and Delete operations as well as delayed data replication in us-west1.\n* Messages - Affected customers would have experienced service availability issues.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-20T21:52:20+00:00","modified":"2022-05-20T21:52:27+00:00","when":"2022-05-20T21:52:20+00:00","text":"The issue with Google Cloud Networking has been resolved for all affected users as of Friday, 2022-05-20 14:07 US/Pacific.\nAffected customers would have experienced high latency, timeouts and errors.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-20T21:44:16+00:00","modified":"2022-05-20T21:44:20+00:00","when":"2022-05-20T21:44:16+00:00","text":"Summary: Global : Cloud Networking faced severe packet loss\nDescription: We've received a report of an issue with Google Cloud Networking as of Friday, 2022-05-20 13:47 US/Pacific.\nWe will provide more information by Friday, 2022-05-20 14:45 US/Pacific.\nDiagnosis: Customers may have encountered elevated latency errors\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-06-02T19:11:24+00:00","modified":"2022-06-02T19:11:24+00:00","when":"2022-06-02T19:11:24+00:00","text":"**INCIDENT REPORT**\n**SUMMARY:**\nOn Friday, 20 May 2022 at 13:47 US/Pacific, Google Cloud Networking experienced intermittent packet loss for traffic between multiple cloud regions for a duration of 20 minutes. The issue was identified and mitigated automatically by 14:07 US/Pacific.\nWe understand this issue has affected our valued customers and users, and we apologize to those who were affected.\n**ROOT CAUSE:**\nGoogle’s production backbone is a global network that enables connectivity for all user-facing traffic via Points of Presence (POPs) or internet exchanges.\nA failure of a component on a fiber path from one of the central US gateway campuses in Google’s production backbone led to a decrease in available network bandwidth between the gateway and multiple edge locations, causing packet loss while the backbone automatically moved traffic onto remaining paths.\nThe network topology in this region is being augmented, and the second-best path has not completed its augmentation. This meant some traffic needed to reroute onto the third-best path, which led to an extended traffic migration period. This disruption was more severe than we had anticipated, and is the subject of remediation actions below.\n**REMEDIATION AND PREVENTION:**\nGoogle’s automated repair mechanisms detected the decrease in available network bandwidth on Friday, 20 May 2022 at 13:47 US/Pacific and automatically routed the traffic through alternate links. The traffic rerouting completed on Friday, 20 May 2022 at 14:06 US/Pacific, mitigating the issue.\nWhile our automated mechanisms worked as intended and recovered the traffic without manual intervention, we understand that the scope of impact caused by this event affected our customers.\nWe have been working on optimizing our global network to minimize the time spent automatically reconfiguring around failures like this (known as \"convergence time\"). While we have made progress, efforts to improve are ongoing. We continue to ensure that the current technology is optimally configured to minimize the frequency and severity of these issues.\nIn this network region, we intend to complete the augmentation by 11 July 2022, which will return the network to the intended topology where any single fiber path failure can be rerouted quickly onto the second-best path.\nWe will also build automatic analysis to ensure that the network topology during augmentation always supports fast convergence.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\n**DETAILED DESCRIPTION OF IMPACT:**\n- Google Cloud Networking - Affected customers would have observed packet loss for ingress and egress traffic to and from US Central Cloud regions from 13:47 to 14:07 US/Pacific.\n- Cloud VPN - Affected customers would have experienced latency and errors for cross region VPN with global routing from 13:47 to 14:07 US/Pacific. Intra-region traffic for Classic and HA VPN would not have been affected.\n- Cloud Router - Affected customers would have observed delays in global routing propagation from 13:47 to 14:07 US/Pacific.\n- Cloud Interconnect - Affected customers would have experienced latency and errors from 13:47 to 14:07 US/Pacific.\n- Google Cloud Storage - Affected customers would have experienced elevated latency, HTTP 500 errors, or transient API errors from 13:47 to 14:17 US/Pacific.\n- Cloud SQL - Affected customers would have experienced failures for Export, Update, and Delete operations as well as delayed data replication in us-west1 from 13:53 to 14:13 US/Pacific.\n- Messages - Affected customers would have experienced service availability issues from 13:49 to 14:05 US/Pacific.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/eo76pxZiDgWVz4z3kmUv","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"3NTkKG1x382cHtxQGM76","number":"11053303972152724257","begin":"2022-05-09T08:47:55+00:00","created":"2022-05-09T08:48:00+00:00","end":"2022-05-09T08:48:33+00:00","modified":"2022-05-09T08:48:33+00:00","external_desc":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","updates":[{"created":"2022-05-09T08:48:32+00:00","modified":"2022-05-09T08:48:33+00:00","when":"2022-05-09T08:48:32+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-05-09T08:48:00+00:00","modified":"2022-05-09T08:48:01+00:00","when":"2022-05-09T08:48:00+00:00","text":"Summary: Google Infrastructure Configuration Server operation requests failing\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Sunday, 2022-05-08 23:24 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 03:00 US/Pacific with current details\nDiagnosis: Google Infrastructure Configuration Server operation requests failing for multiple actions to create, set or\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-05-09T08:48:32+00:00","modified":"2022-05-09T08:48:33+00:00","when":"2022-05-09T08:48:32+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/3NTkKG1x382cHtxQGM76","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"jmCkyGvzEvbgUuhdYTTW","number":"10151776288952327323","begin":"2022-05-09T08:18:39+00:00","created":"2022-05-09T08:18:44+00:00","end":"2022-05-09T08:42:41+00:00","modified":"2022-05-09T08:42:41+00:00","external_desc":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","updates":[{"created":"2022-05-09T08:42:40+00:00","modified":"2022-05-09T08:42:41+00:00","when":"2022-05-09T08:42:40+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-09T08:18:44+00:00","modified":"2022-05-09T08:18:45+00:00","when":"2022-05-09T08:18:44+00:00","text":"Summary: Google App Engine Flex failing deployments in multiple regions with DEADLINE_EXCEEDED\nDescription: We are experiencing an intermittent issue with Google App Engine beginning at Monday, 2022-05-08 23:38 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 01:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Users may observe error, DEADLINE_EXCEEDED - \"Timed out waiting for the app infrastructure to become healthy.\"\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2022-05-09T08:42:40+00:00","modified":"2022-05-09T08:42:41+00:00","when":"2022-05-09T08:42:40+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"kchyUtnkMHJWaAva8aYc","service_name":"Google App Engine","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/jmCkyGvzEvbgUuhdYTTW","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"SmJPisFk1m1KH7UTJrM5","number":"1736553551166578884","begin":"2022-05-09T08:05:00+00:00","created":"2022-05-09T08:28:59+00:00","end":"2022-05-09T08:31:30+00:00","modified":"2022-05-09T08:31:30+00:00","external_desc":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","updates":[{"created":"2022-05-09T08:31:30+00:00","modified":"2022-05-09T08:31:31+00:00","when":"2022-05-09T08:31:30+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-09T08:28:58+00:00","modified":"2022-05-09T08:28:59+00:00","when":"2022-05-09T08:28:58+00:00","text":"Summary: Composer creations fail in multiple regions\nDescription: We are experiencing an issue with Google Cloud Composer beginning at Monday, 2022-05-09 00:39 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 02:10 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Users are unable to create Composer environments with Private IP.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2022-05-09T08:31:30+00:00","modified":"2022-05-09T08:31:31+00:00","when":"2022-05-09T08:31:30+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"YxkG5FfcC42cQmvBCk4j","service_name":"Google Cloud Composer","affected_products":[{"title":"Google Cloud Composer","id":"YxkG5FfcC42cQmvBCk4j"}],"uri":"incidents/SmJPisFk1m1KH7UTJrM5","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"GVJPxjAqPSvxFMh3CmjQ","number":"18030607443765012729","begin":"2022-05-09T06:59:24+00:00","created":"2022-05-09T07:30:41+00:00","end":"2022-05-09T08:37:22+00:00","modified":"2022-05-09T08:37:22+00:00","external_desc":"We've received a report of an issue with Persistent Disk.","updates":[{"created":"2022-05-09T08:37:22+00:00","modified":"2022-05-09T08:37:23+00:00","when":"2022-05-09T08:37:22+00:00","text":"The issue with Persistent Disk is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-09T07:30:35+00:00","modified":"2022-05-09T07:30:41+00:00","when":"2022-05-09T07:30:35+00:00","text":"Summary: We've received a report of an issue with Persistent Disk.\nDescription: We are experiencing an issue with Persistent Disk.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 01:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-09T08:37:22+00:00","modified":"2022-05-09T08:37:23+00:00","when":"2022-05-09T08:37:22+00:00","text":"The issue with Persistent Disk is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"}],"uri":"incidents/GVJPxjAqPSvxFMh3CmjQ","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"2Hd52dn3PqYGTD5zdp7v","number":"12687481699252051222","begin":"2022-05-09T06:55:00+00:00","created":"2022-05-09T08:37:29+00:00","end":"2022-05-09T08:42:00+00:00","modified":"2022-05-26T19:57:20+00:00","external_desc":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/eWat683pNnkMT7orVDBV","updates":[{"created":"2022-05-09T08:42:39+00:00","modified":"2022-05-09T08:42:40+00:00","when":"2022-05-09T08:42:39+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/eWat683pNnkMT7orVDBV","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-09T08:42:31+00:00","modified":"2022-05-09T08:42:31+00:00","when":"2022-05-09T08:42:31+00:00","text":"Summary: Google Infrastructure Configuration Server operation requests failing\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Sunday, 2022-05-08 23:24 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 03:00 US/Pacific with current details\nDiagnosis: Diagnosis: Customer's might experience following:\n1) External HTTP/S Load Balancing, Cloud CDN - config changes will be accepted \u0026 fail to propagate\n2) Cloud Armor rules may also be affected\n3) Appengine flex customers may see apps fail to deploy\n4) Traffic Director may see configs fail to propagate\n5) VM instance group healthchecks \u0026 functionality like autoscaling/autohealing may be affected\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-05-09T08:37:29+00:00","modified":"2022-05-09T08:37:30+00:00","when":"2022-05-09T08:37:29+00:00","text":"Summary: Google Infrastructure Configuration Server operation requests failing\nDescription: We are experiencing an issue with Google Cloud DNS, Service Directory beginning at Sunday, 2022-05-08 23:24 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 03:00 US/Pacific with current details\nDiagnosis: Diagnosis: Customer's might experience following:\n1) External HTTP/S Load Balancing, Cloud CDN - config changes will be accepted \u0026 fail to propagate\n2) Cloud Armor rules may also be affected\n3) Appengine flex customers may see apps fail to deploy\n4) Traffic Director may see configs fail to propagate\n5) VM instance group healthchecks \u0026 functionality like autoscaling/autohealing may be affected\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-09T08:42:39+00:00","modified":"2022-05-09T08:42:40+00:00","when":"2022-05-09T08:42:39+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/eWat683pNnkMT7orVDBV","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud DNS","id":"TUZUsWSJUVJGW97Jq2sH"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Service Directory","id":"vmq8TsEZwitKYM6V9BaM"},{"title":"Cloud CDN","id":"ckSRJf2vQwQy188ULGy3"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"},{"title":"Cloud Armor","id":"Kakg69gTC3xFyeJCY2va"},{"title":"Access Approval","id":"duAxoF9U3MBmoAihC67y"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Anthos Service Mesh","id":"D6ayar99EiNLHhwcn77V"}],"uri":"incidents/2Hd52dn3PqYGTD5zdp7v","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"eWat683pNnkMT7orVDBV","number":"6487971899846239496","begin":"2022-05-09T06:24:00+00:00","created":"2022-05-09T08:28:43+00:00","end":"2022-05-09T09:42:00+00:00","modified":"2022-05-10T22:40:46+00:00","external_desc":"Google Cloud Infrastructure is failing to push cloud configs","updates":[{"created":"2022-05-10T22:40:22+00:00","modified":"2022-05-10T22:40:22+00:00","when":"2022-05-10T22:40:22+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 08 May 2022 23:24\n**Incident End:** 09 May 2022 02:42\n**Duration:** 3 hours, 18 minutes\n**Affected Services and Features:**\n* Google Cloud Networking\n* Google App Engine\n* Google Cloud Composer\n**Regions/Zones:** Global\n**Description:**\nMultiple Google Cloud services experienced issues from 8 May 2022 23:24 to 9 May 2022 02:42 US/Pacific. From preliminary analysis, the root cause of the issue was network issues that caused tests to fail which led the configuration management system to treat the config as a bad config and prevented propagating configuration changes to Google Cloud infrastructure backend servers.\n**Customer Impact:**\n* Google Cloud Networking - Customers modifying backend configuration for load balanced products or health check configuration may have seen their configuration accepted but not propagate, or fail after a timeout\n* Google App Engine - Customers may have observed the error, DEADLINE_EXCEEDED - “Timed out waiting for the app infrastructure to become healthy” when attempting to deploy updates.\n* Google Cloud Composer - Customers would have been unable to create Composer 1 and some Composer 2 \"Private IP\" environments (only those using VPC peering, environments using Private Service Connect were unaffected).","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-05-09T09:49:51+00:00","modified":"2022-05-09T09:49:52+00:00","when":"2022-05-09T09:49:51+00:00","text":"The issue with Google App Engine, Google Cloud Composer, Google Cloud DNS, Google Cloud Networking, Google Compute Engine, Service Directory has been resolved for all affected users as of Monday, 2022-05-09 02:42 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-05-09T09:27:45+00:00","modified":"2022-05-09T09:27:46+00:00","when":"2022-05-09T09:27:45+00:00","text":"Summary: Google Cloud Infrastructure is failing to push cloud configs\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2022-05-09 04:00 US/Pacific.\nDiagnosis: Customers may experience the following: 1) External HTTP/S Load Balancing, Cloud CDN - config changes will be accepted but fail to propagate 2) Cloud Armor rules may also be affected 3) App Engine Flex customers may see apps fail to deploy 4) Traffic Director may see configs fail to propagate 5) VM instance group health checks \u0026 functionality like autoscaling/autohealing may be affected.\nProduct Impact:\nGoogle App Engine Flex:\nImpact/Diagnosis: Users may observe error, DEADLINE_EXCEEDED. “Timed out waiting for the app infrastructure to become healthy”. This occurs when attempting to deploy major version updates.\nWorkaround: None.\nGoogle Cloud Networking\nImpact/Diagnosis: Cloud customers modifying backend configuration for load balanced products, or health check configuration, may see their configuration accepted but not propagate, or fail after a timeout.\nWorkaround: None\nCloud Composer:\nImpact: Users are unable to create Composer 1 and Composer 2 \"Private IP\" environments.\nWorkaround: No workaround is available for Composer 1. For Composer 2 “Private IP” the workaround is to create environments with Private Service Connect. Composer 2 “Public IP” environment creations should be successful.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-05-09T08:28:42+00:00","modified":"2022-05-09T08:28:43+00:00","when":"2022-05-09T08:28:42+00:00","text":"Summary: GICS is failing to push cloud configs\nDescription: We are experiencing an issue with Google Cloud DNS, Service Directory beginning at Sunday, 2022-05-08 23:24 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-05-09 03:00 US/Pacific with current details.\nDiagnosis: Customer's might experience following:\n1) External HTTP/S Load Balancing, Cloud CDN - config changes will be accepted \u0026 fail to propagate\n2) Cloud Armor rules may also be affected\n3) Appengine flex customers may see apps fail to deploy\n4) Traffic Director may see configs fail to propagate\n5) VM instance group healthchecks \u0026 functionality like autoscaling/autohealing may be affected\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-05-10T22:40:22+00:00","modified":"2022-05-10T22:40:22+00:00","when":"2022-05-10T22:40:22+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 08 May 2022 23:24\n**Incident End:** 09 May 2022 02:42\n**Duration:** 3 hours, 18 minutes\n**Affected Services and Features:**\n* Google Cloud Networking\n* Google App Engine\n* Google Cloud Composer\n**Regions/Zones:** Global\n**Description:**\nMultiple Google Cloud services experienced issues from 8 May 2022 23:24 to 9 May 2022 02:42 US/Pacific. From preliminary analysis, the root cause of the issue was network issues that caused tests to fail which led the configuration management system to treat the config as a bad config and prevented propagating configuration changes to Google Cloud infrastructure backend servers.\n**Customer Impact:**\n* Google Cloud Networking - Customers modifying backend configuration for load balanced products or health check configuration may have seen their configuration accepted but not propagate, or fail after a timeout\n* Google App Engine - Customers may have observed the error, DEADLINE_EXCEEDED - “Timed out waiting for the app infrastructure to become healthy” when attempting to deploy updates.\n* Google Cloud Composer - Customers would have been unable to create Composer 1 and some Composer 2 \"Private IP\" environments (only those using VPC peering, environments using Private Service Connect were unaffected).","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Cloud DNS","id":"TUZUsWSJUVJGW97Jq2sH"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud Composer","id":"YxkG5FfcC42cQmvBCk4j"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Service Directory","id":"vmq8TsEZwitKYM6V9BaM"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"},{"title":"Cloud CDN","id":"ckSRJf2vQwQy188ULGy3"},{"title":"Traffic Director","id":"NroZwL2UMMionesUGP87"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"}],"uri":"incidents/eWat683pNnkMT7orVDBV","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"x7sHBZr8vu8tyJoq8Ahq","number":"1062169737220912229","begin":"2022-05-06T18:01:01+00:00","created":"2022-05-06T18:01:06+00:00","end":"2022-05-06T22:30:25+00:00","modified":"2022-05-09T18:38:58+00:00","external_desc":"Customers may experience elevated latency for inter-region in multiple asia regions.","updates":[{"created":"2022-05-06T22:30:25+00:00","modified":"2022-05-06T22:30:26+00:00","when":"2022-05-06T22:30:25+00:00","text":"The issue with Cloud Memorystore, Google Cloud Networking, Google Cloud SQL, Google Compute Engine, Google Kubernetes Engine has been resolved for all affected projects as of Friday, 2022-05-06 15:25 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"}]},{"created":"2022-05-06T22:20:50+00:00","modified":"2022-05-09T18:38:36+00:00","when":"2022-05-06T22:20:50+00:00","text":"Summary: Customers may experience elevated latency for inter-region in multiple asia regions.\nDescription: We believe the issue with Cloud Memorystore, Google Cloud Networking, Google Cloud SQL, Google Compute Engine, Google Kubernetes Engine is partially resolved.\nFull resolution is expected to complete by Friday, 2022-05-06 16:30 US/Pacific.\nWe will provide an update by Friday, 2022-05-06 16:45 US/Pacific with current details.\nDiagnosis: Customers may experience elevated inter-region latency between following regions\n* Traffic between asia-east1 and asia-northeast3\n* Traffic between asia-east2 and asia-east1\n* Traffic between asia-east2 and asia-northeast3\n* Traffic between asia-southeast1 and asia-east1\n* Traffic between asia-southeast2 and asia-east1\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"}]},{"created":"2022-05-06T18:22:34+00:00","modified":"2022-05-09T18:38:48+00:00","when":"2022-05-06T18:22:34+00:00","text":"Summary: Customers may experience elevated latency for inter-region in multiple asia regions.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Friday, 2022-05-06 16:00 US/Pacific.\nWe will provide more information by Friday, 2022-05-06 16:15 US/Pacific.\nDiagnosis: Customers may experience elevated inter-region latency between following regions\n* Traffic between asia-east1 and asia-northeast3\n* Traffic between asia-east2 and asia-east1\n* Traffic between asia-east2 and asia-northeast3\n* Traffic between asia-southeast1 and asia-east1\n* Traffic between asia-southeast2 and asia-east1\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"}]},{"created":"2022-05-06T18:01:05+00:00","modified":"2022-05-09T18:38:58+00:00","when":"2022-05-06T18:01:05+00:00","text":"Summary: Customers may experience elevated latency for inter-region in multiple asia regions.\nDescription: We are experiencing an issue with Google Cloud Networking, Google Kubernetes Engine, Google Compute Engine, Google Cloud SQL, Cloud Memorystore beginning at Friday, 2022-05-06 10:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-05-06 11:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience elevated inter-region latency between following regions\n* Traffic between asia-east1 and asia-northeast3\n* Traffic between asia-east2 and asia-east1\n* Traffic between asia-east2 and asia-northeast3\n* Traffic between asia-southeast1 and asia-east1\n* Traffic between asia-southeast2 and asia-east1\nWorkaround: None at this time.","status":"SERVICE_INFORMATION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"}]}],"most_recent_update":{"created":"2022-05-06T22:30:25+00:00","modified":"2022-05-06T22:30:26+00:00","when":"2022-05-06T22:30:25+00:00","text":"The issue with Cloud Memorystore, Google Cloud Networking, Google Cloud SQL, Google Compute Engine, Google Kubernetes Engine has been resolved for all affected projects as of Friday, 2022-05-06 15:25 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"}]},"status_impact":"SERVICE_INFORMATION","severity":"low","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Cloud Memorystore","id":"LGPLu3M5pcUAKU1z6eP3"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"}],"uri":"incidents/x7sHBZr8vu8tyJoq8Ahq","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"}]},{"id":"6nV9kudxEmgDnwAG1uJY","number":"16191319608876327775","begin":"2022-05-06T13:24:37+00:00","created":"2022-05-06T13:24:42+00:00","end":"2022-05-06T13:25:18+00:00","modified":"2022-05-06T13:25:18+00:00","external_desc":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","updates":[{"created":"2022-05-06T13:25:17+00:00","modified":"2022-05-06T13:25:18+00:00","when":"2022-05-06T13:25:17+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-06T13:24:42+00:00","modified":"2022-05-06T13:24:42+00:00","when":"2022-05-06T13:24:42+00:00","text":"Summary: Users can't connect to their instance in us-central1-b through proxy-server\nDescription: We are experiencing an issue with Google Cloud SQL beginning at Friday, 2022-05-06 02:00 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV where we will provide the next update.\nDiagnosis: None at this time.Users can't connect to their instance in us-central1-b through proxy-server. Other connectivity ways seem to be unaffected.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2022-05-06T13:25:17+00:00","modified":"2022-05-06T13:25:18+00:00","when":"2022-05-06T13:25:17+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"hV87iK5DcEXKgWU2kDri","service_name":"Google Cloud SQL","affected_products":[{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"}],"uri":"incidents/6nV9kudxEmgDnwAG1uJY","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"1YzVoQe8tR83ubvbAnDd","number":"4016492199676528941","begin":"2022-05-06T11:00:01+00:00","created":"2022-05-06T12:18:46+00:00","end":"2022-05-06T13:13:16+00:00","modified":"2022-05-06T13:13:16+00:00","external_desc":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","updates":[{"created":"2022-05-06T13:13:15+00:00","modified":"2022-05-06T13:13:16+00:00","when":"2022-05-06T13:13:15+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","status":"AVAILABLE","affected_locations":[]},{"created":"2022-05-06T12:18:38+00:00","modified":"2022-05-06T12:18:49+00:00","when":"2022-05-06T12:18:38+00:00","text":"Summary: Hung tasks in Cloud Filestore in us-central1-b. Investigation ongoing\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1.\nOur engineering team continues to investigate the issue.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Slow and possible hung NFS\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2022-05-06T13:13:15+00:00","modified":"2022-05-06T13:13:16+00:00","when":"2022-05-06T13:13:15+00:00","text":"This incident is being merged with an existing incident. All future updates will be provided there: https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"jog4nyYkquiLeSK5s26q","service_name":"Cloud Filestore","affected_products":[{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"}],"uri":"incidents/1YzVoQe8tR83ubvbAnDd","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"YUJH3JbojVM5WE4jxyrc","number":"10598063297897699427","begin":"2022-05-06T09:15:37+00:00","created":"2022-05-06T11:35:38+00:00","end":"2022-05-06T12:45:00+00:00","modified":"2022-05-06T12:45:01+00:00","external_desc":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","updates":[{"created":"2022-05-06T12:44:54+00:00","modified":"2022-05-06T12:45:02+00:00","when":"2022-05-06T12:44:54+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T11:35:33+00:00","modified":"2022-05-06T11:35:39+00:00","when":"2022-05-06T11:35:33+00:00","text":"Summary: Elevated tail latencies on Persistent Disk standard devices\nDescription: We are experiencing an issue with Persistent Disk beginning at Friday, 2022-05-06 01:20 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-05-06 05:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some I/O operations in Persistent Disk Standard devices are stuck for a long time (\u003e1 min)\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-05-06T12:44:54+00:00","modified":"2022-05-06T12:45:02+00:00","when":"2022-05-06T12:44:54+00:00","text":"This issue is believed to be affecting a very small number of projects and our Engineering Team is working on it. If you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved. No further updates will be provided here. We thank you for your patience while we are working on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"}],"uri":"incidents/YUJH3JbojVM5WE4jxyrc","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"4Qvmd4q81VnA9RirCMqV","number":"2022535118131364015","begin":"2022-05-06T08:30:00+00:00","created":"2022-05-06T12:01:47+00:00","end":"2022-05-06T19:06:00+00:00","modified":"2022-05-16T23:13:40+00:00","external_desc":"We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b","updates":[{"created":"2022-05-16T23:12:31+00:00","modified":"2022-05-16T23:12:31+00:00","when":"2022-05-16T23:12:31+00:00","text":"**INCIDENT REPORT**\n**Summary:**\nOn 6 May 2022 at 01:30 US/Pacific, multiple Google Cloud services experienced issues in the us-central1 region. These issues mostly were isolated to us-central1-b for zonal services, but some regional services experienced degradation until their traffic could be shifted away from the impacted zone. Most Google Cloud services recovered automatically, after the underlying problem was resolved.\nWe sincerely apologize for the impact to your service or application. We completed an internal investigation and are taking immediate steps to improve the quality and reliability of our services. If you believe that your services experienced an SLA violation as a result of this incident, please [contact us](https://support.google.com/cloud/contact/cloud_platform_sla).\n**Root Cause:**\nGoogle Cloud systems are built on a zonal distributed storage system called [Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system), which replicates data across a large number of individual storage servers called D Servers. In this incident, a background job responsible for repacking storage objects began to retry those repack operations more aggressively as part of its normal operations. This subsequently increased the load on the Colossus system in the zone, including the number of open connections to the D Servers.\nThe sudden increase in connection load to D Servers caused a small number of servers to unexpectedly crash due to high memory pressure. This led our automated management systems to remove them from the serving fleet for Colossus. This further reduced the number of D Servers available to handle the rising traffic loads and increased the traffic latency within the Colossus system in the impacted zone.\nThis significant increase in latency subsequently impacted our customers’ performance across a range of Google Cloud services that are built atop Colossus, including Persistent Disk, BigQuery, and many others.\nThis zonal incident impacted some regional services due to the specific failure mode. When a Colossus cluster is marked down, the regional services receive proactive notification and automatically shift traffic away from the cluster. Since this cluster was still up, but with variable latency for some operations, the regional services received no proactive notification and were unable to automatically shift traffic away from the cluster. Therefore, the impact to a number of regional services was extended as they had to manually remove the impacted cluster from serving.\n**Remediation and Prevention:**\nGoogle engineers were alerted to the issue on Friday, 6 May, 2022 at 01:54 US/Pacific and immediately started an investigation.\nGoogle engineers stopped the background traffic. To increase traffic capacity, Google engineers re-added the impacted D Servers to the serving fleet, mitigating the issue at 12:06 US/Pacific.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We are taking the following steps to prevent this or similar issues from happening again:\n* Investigate and add additional protections in the D Servers to decrease memory pressure during high network traffic load periods.\n* Improve the retry logic for the storage object repacking job to ensure that it cannot overload the Colossus system within a zone.\n* Extend the automated D Server management systems to better handle crash loop conditions and quickly restore D Servers to production once they become healthy.\n* Google's regional services are designed to tolerate zonal failure while staying within their service level objectives. The nature of this failure was not properly handled by some regional services. We are committed to investigating the behavior of each regional service impacted in this outage to ensure that fault tolerance gaps are properly addressed.\n**Detailed Description of Impact:**\nSome customers may have experienced high latency or errors in multiple Google Cloud services in the impacted region.\n* **BigQuery [us-central1 and US multi-region]:** Customers saw increased query, import, and export latencies and errors. The overall duration of impact was 6 hours 5 minutes in us-central1 and 4 hours 34 minutes in US multi-region.\n* **Cloud Bigtable [us-central1-b zone]:** A small number of Customers in us-central1-b experienced elevated latency and errors as well as replication delays for a duration of 10 hours, 36 minutes. A very minor percentage of the affected customers for Cloud Bigtable had residual impact for additional 6 hours, 40 minutes.\n* **Cloud Pub/Sub [us-central1 region]:** Customers may have seen missing backlog stats metrics for subscriptions against topics with messages published to us-central1 for a duration of 2 hours, 41 minutes. Since the impact was based on the message publish region, the subscribers could have been in regions other than us-central1.\n* **Google Cloud Load Balancer (GCLB) [us-central1-b zone]:** New load balancer creations and modifications or deletions of existing components with backends in us-central1-b may have been delayed or not taken effect until the outage was resolved. The total impact duration for GCLB is 4 hours, 46 minutes.\n* **Google Compute Engine (GCE) [us-central1-b zone]:** Customers may have seen issues with instance availability in us-central1-b due to some input/output (I/O) operations in Persistent Disk Standard disks being stuck for over one minute. Additionally, Regional Persistent Disk Standard disks with a replica in us-central1-b may have been briefly affected due to delays in failover. A small number of instances may have experienced brief loss of network connectivity to other Google Cloud services following live migration events. The total impact duration for GCE is around 5 hours, 25 minutes.\n* **Cloud Datastream [us-central1 region]:** Customers may have seen streams enter into \"Failed\" state on the Datastream UI, noticed no new data ingested by Datastream into Google Cloud Storage buckets, had duplicate data loaded into Google Cloud Storage, or metrics not being reported. This impacted a whole region for a duration of 7 hours 40 minutes, because the cluster over provisioning was not at a high enough level, and losing one zone on the underlying Kafka cluster caused the cluster to be at 100% utilization, until it was able to fully copy the data to a new zone.\n* **Cloud Filestore [us-central1-b zone]:** Many Filestore instance creation operations failed. Additionally, a small number of instances were unresponsive for the duration of the incident. Some instances suffered performance impact.\n* **Cloud Memorystore [us-central1-b zone]:** Redis nodes in us-central1-b may have been unavailable for a duration of 3 hours, 54 minutes.\n* **Cloud SQL [us-central1-b zone]:** Customers may not have been able to connect to their instance in us-central1-b through the Cloud SQL Auth proxy for a duration of 3 hours, 5 minutes.\n* **Google Kubernetes Engine (GKE) [us-central1-b zone]:** Customers may have experienced issues interacting with their clusters' control planes. New workloads may not have been scheduled. Auto scaling may not have been operational.The total impact duration for GKE is 7 hours, 11 minutes.\n* **Apigee [us-central1 region]:** Customers may have seen errors for their API traffic with Datastore Errors. Apigee is internally redundant across zones within the region, but due to the high latency failure mode in us-central1-b, the engineers were not able to remove the impacted zone from the regional cluster. The total impact duration for Apigee is 2 hours, 55 minutes.\n* **Dataflow [us-central1 region]:** New Dataflow jobs may have failed to start in us-central1-b. Jobs already running in us-central1-b may have been stuck or delayed, but restarting the jobs would have automatically routed them to a healthy zone starting at 05:00 US/Pacific if the customer was using auto zone placement. The total impact duration for dataflow is 2 hours, 55 minutes.\n* **Cloud Data Fusion (CDF) [us-central1 region]:** Customers may have experienced DataFusion instance creation failures, instance availability issues and higher data processing pipeline failures. This was because Persistent Disk (PD) issues caused DataFusion backend services to become unhealthy and the total impact duration was about 7 hours.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T23:12:13+00:00","modified":"2022-05-16T23:13:40+00:00","when":"2022-05-06T23:12:13+00:00","text":"**Mini Incident Report**\nWe apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 06 May 2022 01:30\n**Incident End:** 06 May 2022 12:06\n**Duration:** 10 hours, 36 minutes\n**Affected Services and Features:** * BigQuery * Cloud Pub/Sub * Google Cloud Load Balancer (GCLB) * Google Compute Engine (GCE) * Datastream * Cloud Filestore * Cloud Memorystore * Cloud SQL * Apigee * Cloud Dataflow * Cloud Data Fusion (CDF) * Cloud Bigtable * Google Kubernetes Engine (GKE)\n**Regions/Zones:** us-central1-b\n**Description:**\nMultiple Google Cloud services experienced issues in the us-central1 region beginning Friday, 6 May 2022 at 01:30 PT. These issues were predominantly isolated to us-central1-b for zonal services, but some regional services experienced degradation until their traffic could be shifted away from the impacted zone. Most services recovered automatically after the underlying problem was resolved.\nThe issues were triggered by an unexpected increase in normally occurring background traffic in the Google Cloud distributed storage infrastructure[1] within the us-central1-b zone. The system automatically directed load away from backend file servers that were impacted by this load increase. This subsequently reduced the overall traffic capacity in the zone. Google engineers mitigated the issue by stopping the background traffic and marking the impacted file servers as available in order to increase capacity.\n**Customer Impact:**\n**How Customers Experienced the Issue:** Some customers may have experienced high latency or errors in multiple Google Cloud services in the impacted region.\n* **BigQuery:** Customers may have seen increased query delays and/or failures.\n* **Cloud Bigtable:** Customers may have experienced elevated latency and errors.\n* **Cloud Pub/Sub:** Customers may have seen missing metrics for backlog statistics.\n* **Google Cloud Load Balancer (GCLB):** New load balancer creations, as well as modifications or deletions of existing components, with backends in us-central1-b may have been delayed or not taken effect until the outage was resolved.\n* **Google Compute Engine (GCE):** Customers may have seen issues with instance availability in us-central1-b due to some input/output (I/O) operations in Persistent Disk being stuck for over one minute. A small number of instances may have experienced brief loss of network reachability to other Google Cloud services following live migration events.\n* **Datastream:** Customers may have seen streams enter into \"Failed\" state on the Datastream UI, noticed no new data ingested by Datastream into Google Cloud Storage bucket, or metrics not being reported.\n* **Cloud Filestore:** Customers may have experienced hung tasks in Filestore instances.\n* **Cloud Memorystore:** Redis nodes in us-central1-b may have been unavailable.\n* **Cloud SQL:** Customers may not have been able to connect to their instance in us-central1-b through the proxy-server.\n* **Google Kubernetes Engine (GKE):** Customers may have experienced issues interacting with the control plane. New workloads may not have been scheduled. Auto scaling may not have been operational.\n* **Apigee:** Customers may have seen errors for their API traffic with Datastore Errors.\n* **Dataflow:** Some Dataflow batch and streaming jobs in us-central1 may have been stuck or delayed.\n* **Cloud Data Fusion (CDF):** CDF operations, like instance creation and pipeline launch, may have failed in the us-central1 region due to an issue on Compute Engine.\n[1] https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T19:09:51+00:00","modified":"2022-05-06T19:09:53+00:00","when":"2022-05-06T19:09:51+00:00","text":"The issue with Cloud Data Fusion, Cloud Filestore, Cloud Memorystore, Google BigQuery, Google Cloud Dataflow, Google Cloud Networking, Google Cloud Pub/Sub, Google Cloud SQL, Google Kubernetes Engine, Persistent Disk, Apigee has been resolved for all affected projects as of Friday, 2022-05-06 12:06 US/Pacific.\nProducts with Narrow Impact: * Google Cloud Bigtable: Outage is currently affecting less than 2% of customers us-central1-b. Mitigation is ongoing and support will continue to work with the affected customers through resolution.\nWe will publish an Incident Report once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T18:16:49+00:00","modified":"2022-05-06T18:16:53+00:00","when":"2022-05-06T18:16:49+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with multiple cloud services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed, and most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 12:30 US/Pacific.\nProducts Recovered: * BigQuery Engine * Cloud Pub/Sub * Cloud Networking * Compute Engine * Datastream * Cloud Filestore * Cloud Memorystore * Cloud SQL * Cloud Data Fusion * Apigee * Dataflow * GKE\nProducts Still Recovering: * Google Cloud Bigtable: Mitigation is ongoing for a small number of customers who are experiencing elevated latency and errors.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T17:57:45+00:00","modified":"2022-05-06T17:57:46+00:00","when":"2022-05-06T17:57:45+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with multiple cloud services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed, and most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 11:30 US/Pacific.\nProducts Recovered: * BigQuery Engine * Cloud Pub/Sub * Cloud Networking * Compute Engine * Datastream * Cloud Filestore * Cloud Memorystore * Cloud SQL * Cloud Data Fusion * Apigee * Dataflow * GKE\nProducts Still Recovering: * Google Cloud Bigtable: Mitigation is ongoing for a small number of customers who are experiencing elevated latency and errors.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T17:28:20+00:00","modified":"2022-05-06T17:28:22+00:00","when":"2022-05-06T17:28:20+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 11:00 US/Pacific.\nProducts Recovered: * BigQuery Engine * Cloud Pub/Sub * Cloud Networking * Compute Engine * Datastream * Cloud Filestore * Cloud Memorystore * Cloud SQL * Cloud Data Fusion * Apigee * Dataflow * GKE\nProducts Still Recovering: * Google Cloud Bigtable: A small number of customers may still be experiencing elevated latency and errors.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T16:55:11+00:00","modified":"2022-05-06T16:55:12+00:00","when":"2022-05-06T16:55:11+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 10:30 US/Pacific.\nProducts Recovered: * BigQuery Engine:Cloud Pub/Sub * Cloud Networking * Compute Engine * Datastream * Cloud Filestore * Cloud Memorystore * Cloud SQL * Cloud Data Fusion * Apigee * Dataflow * GKE\nProducts Still Recovering: * Google Cloud Bigtable: Customers may be experiencing elevated latency and errors.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T16:26:48+00:00","modified":"2022-05-06T16:26:49+00:00","when":"2022-05-06T16:26:48+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 10:00 US/Pacific.\nProducts Recovered: * BigQuery Engine:Cloud Pub/Sub * Cloud Networking * Compute Engine * Datastream * Cloud Filestore * Cloud Memorystore * Cloud SQL * Apigee * Dataflow\nProducts Still Recovering: * Google Cloud Bigtable: Customers may be experiencing elevated latency and errors. * Google Kubernetes Engine: Customers may experience issues interacting with the control plane. New workloads won’t be scheduled. Auto scaling may not be operational. * Cloud Data Fusion: CDF operations like instance creation, pipeline launch might fail in us-central1 region due to a Compute Engine issue.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T15:56:44+00:00","modified":"2022-05-06T15:56:46+00:00","when":"2022-05-06T15:56:44+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and we see that most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 09:30 US/Pacific.\nProducts Recovered:\n- BigQuery Engine:Cloud Pub/Sub, Cloud Networking, Compute Engine, Datastream, Cloud Filestore, Cloud Memorystore, Cloud SQL, Apigee, Dataflow\nProducts Still Recovering:\n- Google Cloud BigTable: Customers may be experiencing elevated latency and errors.\n- Google Kubernetes Engine: Customers may experience issues interacting with the control plane. New workloads won’t be scheduled. Auto scaling may not be operational.\n- Cloud Data Fusion: CDF operations like instance creation, pipeline launch might fail in us-central1 region due to an issue on Compute Engine.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T15:27:58+00:00","modified":"2022-05-06T15:27:59+00:00","when":"2022-05-06T15:27:58+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and we see that most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 09:00 US/Pacific.\nProducts Recovered:\nBigQuery Engine:Cloud Pub/Sub, Cloud Networking, Compute Engine, Datastream, Cloud Filestore, Cloud Memorystore, Cloud SQL, Apigee, Dataflow\nProducts Still Recovering:\n- Google Cloud BigTable: Customers may be experiencing issues.\n- Google Kubernetes Engine: Customers may experience issues interacting with the control plane. New workloads won’t be scheduled. Auto scaling may not be operational.\n- Cloud Data Fusion: CDF operations like instance creation, pipeline launch might fail in us-central1 region due to an issue on Compute Engine.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T15:01:50+00:00","modified":"2022-05-06T15:01:51+00:00","when":"2022-05-06T15:01:50+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, Google Kubernetes Engine (GKE), Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion (CDF) beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and we see that most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 08:30 US/Pacific.\nProducts Recovered:\nBigQuery Engine:Cloud Pub/Sub, Cloud Networking, Compute Engine, Datastream, Cloud Filestore, Cloud Memorystore, Cloud SQL, Apigee, Dataflow\nProducts Still Recovering:\n- Google Kubernetes Engine: Customers may experience issues interacting with the control plane. New workloads won’t be scheduled. Auto scaling may not be operational.\n- Cloud Data Fusion: CDF operations like instance creation, pipeline launch might fail in us-central1 region due to an issue on Compute Engine.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T14:25:49+00:00","modified":"2022-05-06T14:25:50+00:00","when":"2022-05-06T14:25:49+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, GKE, Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services, Cloud Data Fusion beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation is completed and we see that most of the affected services have recovered.\nWe will provide more information by Friday, 2022-05-06 08:00 US/Pacific.\nProduct Impact:\n- BigQuery Engine: Customers may see increased query latencies and/or failures.\n- Cloud Pub/Sub: Customer may see missing metrics for backlog statistics.\n- Cloud Networking: Customers may see connectivity issues.\n- Compute Engine: Customers may see issues with VM availability in us-central1-b.\n- Datastream: Customers may see streams enter into \"Failed\" state on the Datastream UI, notice no new data ingested by Datastream into GCS bucket or metrics not being reported.\n- Cloud Filestore: Customers may experience many hung tasks in filestore VMs.\n- Cloud Memorystore: Redis nodes in us-central1-b may be unavailable.\n- Cloud SQL: Customers may not be able to connect to their instance in us-central1-b through proxy-server.\n- Apigee: Customers may see 5XX errors for their API traffic with Datastore Errors.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T13:54:30+00:00","modified":"2022-05-06T13:54:32+00:00","when":"2022-05-06T13:54:30+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, GKE, Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation work is currently underway by our engineering team. We see partial recovery for some services.\nWe will provide more information by Friday, 2022-05-06 07:30 US/Pacific.\nProduct Impact:\n- BigQuery Engine: Customers may see increased query latencies and/or failures.\n- Cloud Pub/Sub: Customer may see missing metrics for backlog statistics\n- Cloud Networking : Customers may see connectivity issues.\n- Compute Engine : Customers may see issues with VM availability in us-central1-b\n- Datastream: Customers may see streams enter into \"Failed\" state on the Datastream UI, notice no new data ingested by Datastream into GCS bucket or metrics not being reported.\n- Cloud Filestore : Customers may experience many hung tasks in filestore VMs.\n- Cloud Memorystore: Redis nodes in us-central1-b may be unavailable\n- Cloud SQL: Customers may not be able to connect to their instance in us-central1-b through proxy-server.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T13:20:50+00:00","modified":"2022-05-06T13:20:51+00:00","when":"2022-05-06T13:20:50+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including BigQuery, Cloud Networking, Cloud SQL, GKE Control Plane, Cloud Filestore, Cloud Bigtable, Cloud Memorystore, Apigee, Cloud Dataflow services beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation work is currently underway by our engineering team.\nWe will provide more information by Friday, 2022-05-06 07:00 US/Pacific.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T13:04:08+00:00","modified":"2022-05-06T13:04:09+00:00","when":"2022-05-06T13:04:08+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services including Bigquery, Cloud Networking, Cloud SQL, GKE beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1-b.\nMitigation work is currently underway by our engineering team.\nWe will provide more information by Friday, 2022-05-06 06:30 US/Pacific.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T12:58:39+00:00","modified":"2022-05-06T12:58:40+00:00","when":"2022-05-06T12:58:39+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1-b\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Friday, 2022-05-06 06:30 US/Pacific.\nDiagnosis: Customers might see connectivity issues in us-central1-b\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T12:45:08+00:00","modified":"2022-05-06T12:45:09+00:00","when":"2022-05-06T12:45:08+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1. Our engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-05-06 06:30 US/Pacific with current details. with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some I/O operations in Persistent Disk Standard devices are stuck for a long time (\u003e1 min)\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T12:01:47+00:00","modified":"2022-05-06T12:01:48+00:00","when":"2022-05-06T12:01:47+00:00","text":"Summary: We are experiencing an issue with Persistent Disk affecting multiple services in us-central1\nDescription: We are experiencing an issue with Persistent Disk affecting multiple services beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-05-06 06:30 US/Pacific with current details. with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some I/O operations in Persistent Disk Standard devices are stuck for a long time (\u003e1 min)\nWorkaround: Move the workloads to a different zone if possible","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-05-16T23:12:31+00:00","modified":"2022-05-16T23:12:31+00:00","when":"2022-05-16T23:12:31+00:00","text":"**INCIDENT REPORT**\n**Summary:**\nOn 6 May 2022 at 01:30 US/Pacific, multiple Google Cloud services experienced issues in the us-central1 region. These issues mostly were isolated to us-central1-b for zonal services, but some regional services experienced degradation until their traffic could be shifted away from the impacted zone. Most Google Cloud services recovered automatically, after the underlying problem was resolved.\nWe sincerely apologize for the impact to your service or application. We completed an internal investigation and are taking immediate steps to improve the quality and reliability of our services. If you believe that your services experienced an SLA violation as a result of this incident, please [contact us](https://support.google.com/cloud/contact/cloud_platform_sla).\n**Root Cause:**\nGoogle Cloud systems are built on a zonal distributed storage system called [Colossus](https://cloud.google.com/blog/products/storage-data-transfer/a-peek-behind-colossus-googles-file-system), which replicates data across a large number of individual storage servers called D Servers. In this incident, a background job responsible for repacking storage objects began to retry those repack operations more aggressively as part of its normal operations. This subsequently increased the load on the Colossus system in the zone, including the number of open connections to the D Servers.\nThe sudden increase in connection load to D Servers caused a small number of servers to unexpectedly crash due to high memory pressure. This led our automated management systems to remove them from the serving fleet for Colossus. This further reduced the number of D Servers available to handle the rising traffic loads and increased the traffic latency within the Colossus system in the impacted zone.\nThis significant increase in latency subsequently impacted our customers’ performance across a range of Google Cloud services that are built atop Colossus, including Persistent Disk, BigQuery, and many others.\nThis zonal incident impacted some regional services due to the specific failure mode. When a Colossus cluster is marked down, the regional services receive proactive notification and automatically shift traffic away from the cluster. Since this cluster was still up, but with variable latency for some operations, the regional services received no proactive notification and were unable to automatically shift traffic away from the cluster. Therefore, the impact to a number of regional services was extended as they had to manually remove the impacted cluster from serving.\n**Remediation and Prevention:**\nGoogle engineers were alerted to the issue on Friday, 6 May, 2022 at 01:54 US/Pacific and immediately started an investigation.\nGoogle engineers stopped the background traffic. To increase traffic capacity, Google engineers re-added the impacted D Servers to the serving fleet, mitigating the issue at 12:06 US/Pacific.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We are taking the following steps to prevent this or similar issues from happening again:\n* Investigate and add additional protections in the D Servers to decrease memory pressure during high network traffic load periods.\n* Improve the retry logic for the storage object repacking job to ensure that it cannot overload the Colossus system within a zone.\n* Extend the automated D Server management systems to better handle crash loop conditions and quickly restore D Servers to production once they become healthy.\n* Google's regional services are designed to tolerate zonal failure while staying within their service level objectives. The nature of this failure was not properly handled by some regional services. We are committed to investigating the behavior of each regional service impacted in this outage to ensure that fault tolerance gaps are properly addressed.\n**Detailed Description of Impact:**\nSome customers may have experienced high latency or errors in multiple Google Cloud services in the impacted region.\n* **BigQuery [us-central1 and US multi-region]:** Customers saw increased query, import, and export latencies and errors. The overall duration of impact was 6 hours 5 minutes in us-central1 and 4 hours 34 minutes in US multi-region.\n* **Cloud Bigtable [us-central1-b zone]:** A small number of Customers in us-central1-b experienced elevated latency and errors as well as replication delays for a duration of 10 hours, 36 minutes. A very minor percentage of the affected customers for Cloud Bigtable had residual impact for additional 6 hours, 40 minutes.\n* **Cloud Pub/Sub [us-central1 region]:** Customers may have seen missing backlog stats metrics for subscriptions against topics with messages published to us-central1 for a duration of 2 hours, 41 minutes. Since the impact was based on the message publish region, the subscribers could have been in regions other than us-central1.\n* **Google Cloud Load Balancer (GCLB) [us-central1-b zone]:** New load balancer creations and modifications or deletions of existing components with backends in us-central1-b may have been delayed or not taken effect until the outage was resolved. The total impact duration for GCLB is 4 hours, 46 minutes.\n* **Google Compute Engine (GCE) [us-central1-b zone]:** Customers may have seen issues with instance availability in us-central1-b due to some input/output (I/O) operations in Persistent Disk Standard disks being stuck for over one minute. Additionally, Regional Persistent Disk Standard disks with a replica in us-central1-b may have been briefly affected due to delays in failover. A small number of instances may have experienced brief loss of network connectivity to other Google Cloud services following live migration events. The total impact duration for GCE is around 5 hours, 25 minutes.\n* **Cloud Datastream [us-central1 region]:** Customers may have seen streams enter into \"Failed\" state on the Datastream UI, noticed no new data ingested by Datastream into Google Cloud Storage buckets, had duplicate data loaded into Google Cloud Storage, or metrics not being reported. This impacted a whole region for a duration of 7 hours 40 minutes, because the cluster over provisioning was not at a high enough level, and losing one zone on the underlying Kafka cluster caused the cluster to be at 100% utilization, until it was able to fully copy the data to a new zone.\n* **Cloud Filestore [us-central1-b zone]:** Many Filestore instance creation operations failed. Additionally, a small number of instances were unresponsive for the duration of the incident. Some instances suffered performance impact.\n* **Cloud Memorystore [us-central1-b zone]:** Redis nodes in us-central1-b may have been unavailable for a duration of 3 hours, 54 minutes.\n* **Cloud SQL [us-central1-b zone]:** Customers may not have been able to connect to their instance in us-central1-b through the Cloud SQL Auth proxy for a duration of 3 hours, 5 minutes.\n* **Google Kubernetes Engine (GKE) [us-central1-b zone]:** Customers may have experienced issues interacting with their clusters' control planes. New workloads may not have been scheduled. Auto scaling may not have been operational.The total impact duration for GKE is 7 hours, 11 minutes.\n* **Apigee [us-central1 region]:** Customers may have seen errors for their API traffic with Datastore Errors. Apigee is internally redundant across zones within the region, but due to the high latency failure mode in us-central1-b, the engineers were not able to remove the impacted zone from the regional cluster. The total impact duration for Apigee is 2 hours, 55 minutes.\n* **Dataflow [us-central1 region]:** New Dataflow jobs may have failed to start in us-central1-b. Jobs already running in us-central1-b may have been stuck or delayed, but restarting the jobs would have automatically routed them to a healthy zone starting at 05:00 US/Pacific if the customer was using auto zone placement. The total impact duration for dataflow is 2 hours, 55 minutes.\n* **Cloud Data Fusion (CDF) [us-central1 region]:** Customers may have experienced DataFusion instance creation failures, instance availability issues and higher data processing pipeline failures. This was because Persistent Disk (PD) issues caused DataFusion backend services to become unhealthy and the total impact duration was about 7 hours.","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"},{"title":"Apigee","id":"9Y13BNFy4fJydvjdsN3X"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Cloud Memorystore","id":"LGPLu3M5pcUAKU1z6eP3"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"},{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"},{"title":"Cloud Data Fusion","id":"rLKDHeeaBiXTeutF1air"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/4Qvmd4q81VnA9RirCMqV","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"pQohCqBLfFapHrkjY5Mh","number":"10148657515826785856","begin":"2022-05-06T08:20:00+00:00","created":"2022-05-06T10:49:53+00:00","end":"2022-05-06T19:09:00+00:00","modified":"2022-05-06T19:36:48+00:00","external_desc":"Elevated tail latencies on Persistent Disk standard devices","updates":[{"created":"2022-05-06T19:36:42+00:00","modified":"2022-05-06T19:36:42+00:00","when":"2022-05-06T19:36:42+00:00","text":"The issue with Persistent Disk has been resolved for all affected projects as of Friday, 2022-05-06 12:09 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.\nFor additional details, please refer to https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T12:47:42+00:00","modified":"2022-05-06T12:47:53+00:00","when":"2022-05-06T12:47:42+00:00","text":"We are experiencing an issue with Persistent Disk affecting multiple services beginning at Friday, 2022-05-06 01:20 US/Pacific in us-central1.\nFor further information please refer to https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"created":"2022-05-06T10:49:54+00:00","modified":"2022-05-06T10:49:54+00:00","when":"2022-05-06T10:49:54+00:00","text":"We are experiencing an issue with Persistent Disk beginning at Friday, 2022-05-06 01:20 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2022-05-06 04:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nCUSTOMER SYMPTOMS\nSome I/O operations in Persistent Disk Standard devices are stuck for a long time (\u003e1 min)\nWORKAROUND\nMove the workloads to a different zone if possible","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]}],"most_recent_update":{"created":"2022-05-06T19:36:42+00:00","modified":"2022-05-06T19:36:42+00:00","when":"2022-05-06T19:36:42+00:00","text":"The issue with Persistent Disk has been resolved for all affected projects as of Friday, 2022-05-06 12:09 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.\nFor additional details, please refer to https://status.cloud.google.com/incidents/4Qvmd4q81VnA9RirCMqV","status":"AVAILABLE","affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"SzESm2Ux129pjDGKWD68","service_name":"Persistent Disk","affected_products":[{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"}],"uri":"incidents/pQohCqBLfFapHrkjY5Mh","currently_affected_locations":[],"previously_affected_locations":[{"title":"Iowa (us-central1)","id":"us-central1"}]},{"id":"TFyk1uenaojF2iJAnAAL","number":"901619274871354280","begin":"2022-04-28T14:27:00+00:00","created":"2022-04-28T15:18:18+00:00","end":"2022-04-28T15:51:00+00:00","modified":"2022-05-05T16:47:59+00:00","external_desc":"Google Cloud Support experiencing issues with case creation, case viewing and case search","updates":[{"created":"2022-05-05T16:47:59+00:00","modified":"2022-05-05T16:47:59+00:00","when":"2022-05-05T16:47:59+00:00","text":"The Incident Report is now available on the Google Cloud Status Dashboard:\nhttps://status.cloud.google.com/incidents/mvpNTsgUmf2LL7PdgmyF","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-29T01:22:08+00:00","modified":"2022-04-29T01:22:08+00:00","when":"2022-04-29T01:22:08+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 28 April 2022 07:27\n**Incident End:** 28 April 2022 08:51\n**Duration:** 1 hour, 24 minutes\n**Affected Services and Features:**\nGoogle Cloud Support - Cloud Console\n**Regions/Zones:** Global\n**Description:**\n- Google Cloud Support was unavailable via the Cloud Console and Admin Console for a duration of 1 hour, 24 minutes. From preliminary analysis, the root cause of the issue was due to elevated errors and latency from several backend component modules which failed to start in Google App Engine, due to an overload in the Google App Engine infrastructure.\n**Customer Impact:**\n- Google Cloud Support - customers were unable to create, view or search support cases in Google Cloud Support Center.\n**Additional details:**\nThe issue was fully resolved once the root cause change was rolled back from the Google App Engine infrastructure and request levels returned to normal, allowing the affected backend component modules to start.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-28T16:04:26+00:00","modified":"2022-04-28T16:04:27+00:00","when":"2022-04-28T16:04:26+00:00","text":"The issue with Google Cloud Support has been resolved for all affected users as of Thursday, 2022-04-28 09:04 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-28T15:40:14+00:00","modified":"2022-04-28T15:40:14+00:00","when":"2022-04-28T15:40:14+00:00","text":"Summary: Google Cloud Support experiencing issues with case creation, case viewing and case search\nDescription: We are experiencing an issue with Google Cloud Support beginning at Thursday, 2022-04-28 07:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-04-28 09:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create, view and search cases using Cloud Console or Google Cloud Support Center.\nWorkaround: Google Cloud customers can use https://support.google.com/cloud/contact/prod_issue to open the cases","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-28T15:18:17+00:00","modified":"2022-04-28T15:18:19+00:00","when":"2022-04-28T15:18:17+00:00","text":"Summary: Google Cloud Support experiencing issues with case creation, case viewing and case search\nDescription: We are experiencing an issue with Google Cloud Support beginning at Thursday, 2022-04-28 07:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-04-28 08:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are unable to create, view and search cases using Cloud Console or Google Cloud Support Center.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-05-05T16:47:59+00:00","modified":"2022-05-05T16:47:59+00:00","when":"2022-05-05T16:47:59+00:00","text":"The Incident Report is now available on the Google Cloud Status Dashboard:\nhttps://status.cloud.google.com/incidents/mvpNTsgUmf2LL7PdgmyF","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"bGThzF7oEGP5jcuDdMuk","service_name":"Google Cloud Support","affected_products":[{"title":"Google Cloud Support","id":"bGThzF7oEGP5jcuDdMuk"}],"uri":"incidents/TFyk1uenaojF2iJAnAAL","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"mvpNTsgUmf2LL7PdgmyF","number":"9101450190999181209","begin":"2022-04-28T14:00:00+00:00","created":"2022-04-28T15:37:05+00:00","end":"2022-04-28T15:32:00+00:00","modified":"2022-05-05T15:26:36+00:00","external_desc":"Google App Engine Increased Latency in us-central1","updates":[{"created":"2022-05-05T15:26:36+00:00","modified":"2022-05-05T15:26:36+00:00","when":"2022-05-05T15:26:36+00:00","text":"**INCIDENT REPORT**\n**Summary:**\nOn 28 April 2022, from 07:00 to 08:32 US/Pacific, Google App Engine and Google Cloud Functions experienced increased latency and reduced availability for a duration of 1 hour and 32 minutes in one zone in the us-central1 region. Additionally, customers were unable to create, view, or search support cases in the Google Cloud Support Center and Google Admin Console for 1 hour and 24 minutes. We sincerely apologize for the impact to your service or application. We have completed an internal investigation and are taking immediate steps to improve our service’s quality and reliability.\n**Root Cause:**\nThe Serverless stack relies on a file serving service for container images. The issue was triggered when the file serving component of the Serverless stack experienced a sudden increase in traffic. A bug, introduced in a recent configuration change to the file serving service, was surfaced by the sudden increase in traffic, causing many threads to get stuck. This led to resource exhaustion on the affected file servers, causing some tasks to crash and preventing new requests from completing.\n**Remediation and Prevention:**\nGoogle engineers were alerted to the issue on Thursday, 28 April 2022, at 07:11 US/Pacific and immediately started an investigation. Once the affected component was identified, engineers added additional capacity to the component in the zone that was experiencing the degradation. This mitigated the resource exhaustion, and the service recovered at 08:20 US/Pacific.\nTo prevent recurrence of the issue, engineers rolled back the configuration change to the previous stable version and implemented an automated release block to prevent any unintended release of a version that included the bug.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We are taking the following steps to prevent this or similar issues from happening again:\nTo improve resolution time for future issues of this type, we are calibrating our alerting system to give us an earlier and more precise notification which will allow us to diagnose the issue more quickly.\nWe are also evaluating what kinds of load/stress tests could deterministically detect this kind of issue in the future, so that such regressions are caught automatically before they are deployed to production.\n**Detailed Description of Impact:**\nOn 28th April 2022 from 7:00 PT to 8:32 PT\n**Google App Engine**\nGoogle App Engine experienced increased latency and reduced availability in one zone in us-central1 for a period of 1 hour and 32 minutes. Customers may have experienced increased latency or higher error rate for App Engine projects.\n**Google Cloud Functions**\nGoogle Cloud Functions experienced increased latency and reduced availability in one zone in us-central1 for a period of 1 hour and 32 minutes. “Google Cloud Functions” customers updating their functions (e.g. deploying a new version) may have experienced increased latency or failures, notably failing health checks.\n**Google Cloud Support and Google Workspace Support**\nCustomers were unable to create, view, or search support cases in the Google Cloud Support Center or Google Admin Console for 1 hour and 24 minutes. In addition, a small number of customers had degraded access to phone support, being redirected to a queue requiring additional manual authentication with the support agent.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-04-29T06:05:46+00:00","modified":"2022-04-29T06:05:46+00:00","when":"2022-04-29T06:05:46+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 28 April 2022 07:00\n**Incident End:** 28 April 2022 08:32\n**Duration:** 1 hours, 32 minutes\n**Affected Services and Features:**\nGoogle App Engine\nGoogle Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Google Cloud Functions experienced increased latency and reduced availability in us-central1 for a period of 1 hour and 32 minutes. From the preliminary investigation root cause is related to a bug that caused one component of App Engine to crash under heavy load in us-central1.\n**Customer Impact:**\nGoogle App Engine\nCustomers may have experienced increased latency or higher error rate for App Engine projects.\nGoogle Cloud Functions\nCustomers updating their functions (e.g. deploying a new version) may have experienced increased latency or failures, notably failing health checks.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-04-28T15:52:32+00:00","modified":"2022-04-28T15:52:32+00:00","when":"2022-04-28T15:52:32+00:00","text":"The issue with Google App Engine has been resolved for all affected projects as of Thursday, 2022-04-28 08:52 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-04-28T15:43:41+00:00","modified":"2022-04-28T15:43:41+00:00","when":"2022-04-28T15:43:41+00:00","text":"Summary: Google App Engine Increased Latency in us-central1\nDescription: We are experiencing an issue with Google App Engine beginning at Thursday, 2022-04-28 07:00 US/Pacific.\nOur engineers believe the issue is mitigated and are validating.\nWe will provide an update by Thursday, 2022-04-28 09:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may have experienced increased latency for App Engine projects in us-central1.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-04-28T15:37:04+00:00","modified":"2022-04-28T15:37:06+00:00","when":"2022-04-28T15:37:04+00:00","text":"Summary: Google App Engine Increased Latency in us-central1\nDescription: We are experiencing an issue with Google App Engine beginning at Thursday, 2022-04-28 07:00 US/Pacific.\nOur engineers believe the issue is mitigated and are validating.\nWe will provide an update by Thursday, 2022-04-28 09:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may have experienced increased latency for App Engine projects in uc-central1.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-05-05T15:26:36+00:00","modified":"2022-05-05T15:26:36+00:00","when":"2022-05-05T15:26:36+00:00","text":"**INCIDENT REPORT**\n**Summary:**\nOn 28 April 2022, from 07:00 to 08:32 US/Pacific, Google App Engine and Google Cloud Functions experienced increased latency and reduced availability for a duration of 1 hour and 32 minutes in one zone in the us-central1 region. Additionally, customers were unable to create, view, or search support cases in the Google Cloud Support Center and Google Admin Console for 1 hour and 24 minutes. We sincerely apologize for the impact to your service or application. We have completed an internal investigation and are taking immediate steps to improve our service’s quality and reliability.\n**Root Cause:**\nThe Serverless stack relies on a file serving service for container images. The issue was triggered when the file serving component of the Serverless stack experienced a sudden increase in traffic. A bug, introduced in a recent configuration change to the file serving service, was surfaced by the sudden increase in traffic, causing many threads to get stuck. This led to resource exhaustion on the affected file servers, causing some tasks to crash and preventing new requests from completing.\n**Remediation and Prevention:**\nGoogle engineers were alerted to the issue on Thursday, 28 April 2022, at 07:11 US/Pacific and immediately started an investigation. Once the affected component was identified, engineers added additional capacity to the component in the zone that was experiencing the degradation. This mitigated the resource exhaustion, and the service recovered at 08:20 US/Pacific.\nTo prevent recurrence of the issue, engineers rolled back the configuration change to the previous stable version and implemented an automated release block to prevent any unintended release of a version that included the bug.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We are taking the following steps to prevent this or similar issues from happening again:\nTo improve resolution time for future issues of this type, we are calibrating our alerting system to give us an earlier and more precise notification which will allow us to diagnose the issue more quickly.\nWe are also evaluating what kinds of load/stress tests could deterministically detect this kind of issue in the future, so that such regressions are caught automatically before they are deployed to production.\n**Detailed Description of Impact:**\nOn 28th April 2022 from 7:00 PT to 8:32 PT\n**Google App Engine**\nGoogle App Engine experienced increased latency and reduced availability in one zone in us-central1 for a period of 1 hour and 32 minutes. Customers may have experienced increased latency or higher error rate for App Engine projects.\n**Google Cloud Functions**\nGoogle Cloud Functions experienced increased latency and reduced availability in one zone in us-central1 for a period of 1 hour and 32 minutes. “Google Cloud Functions” customers updating their functions (e.g. deploying a new version) may have experienced increased latency or failures, notably failing health checks.\n**Google Cloud Support and Google Workspace Support**\nCustomers were unable to create, view, or search support cases in the Google Cloud Support Center or Google Admin Console for 1 hour and 24 minutes. In addition, a small number of customers had degraded access to phone support, being redirected to a queue requiring additional manual authentication with the support agent.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"kchyUtnkMHJWaAva8aYc","service_name":"Google App Engine","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/mvpNTsgUmf2LL7PdgmyF","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"HoppX9SAc7jshpz6H52u","number":"15296038728160858841","begin":"2022-04-26T08:47:00+00:00","created":"2022-04-26T09:32:28+00:00","end":"2022-04-26T09:40:00+00:00","modified":"2022-04-26T17:23:37+00:00","external_desc":"We are experiencing Cloud Networking Control Plane issues","updates":[{"created":"2022-04-26T17:23:07+00:00","modified":"2022-04-26T17:23:07+00:00","when":"2022-04-26T17:23:07+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 26 April 2022 01:47\n**Incident End:** 26 April 2022 02:40\n**Duration:** 53 minutes\n**Affected Services and Features:** Google Cloud Networking\n**Regions/Zones:** Global/All regions\n**Description:**\nGoogle Cloud Networking control plane experienced elevated latency. All Google Cloud Load Balancer config to Google Front End stalled globally for a duration of 53 minutes.From the preliminary analysis, the root cause of the issue seems to be a race condition where unprocessed configuration deletes caused conflicts with newer configuration changes, leading to the configuration pipeline stall.\n**Customer Impact:**\nCustomers might have experienced issues while creating new forwarding rules for Global Layer 7 External load balancer or edit or delete existing ones.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-04-26T09:52:04+00:00","modified":"2022-04-26T09:52:10+00:00","when":"2022-04-26T09:52:04+00:00","text":"The issue with Google Cloud Networking has been resolved for all affected users as of Tuesday, 2022-04-26 02:40 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-04-26T09:32:22+00:00","modified":"2022-04-26T09:41:56+00:00","when":"2022-04-26T09:32:22+00:00","text":"Summary: We are experiencing Cloud Networking Control Plane issues\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Tuesday, 2022-04-26 02:06 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-04-26 03:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-04-26T17:23:07+00:00","modified":"2022-04-26T17:23:07+00:00","when":"2022-04-26T17:23:07+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 26 April 2022 01:47\n**Incident End:** 26 April 2022 02:40\n**Duration:** 53 minutes\n**Affected Services and Features:** Google Cloud Networking\n**Regions/Zones:** Global/All regions\n**Description:**\nGoogle Cloud Networking control plane experienced elevated latency. All Google Cloud Load Balancer config to Google Front End stalled globally for a duration of 53 minutes.From the preliminary analysis, the root cause of the issue seems to be a race condition where unprocessed configuration deletes caused conflicts with newer configuration changes, leading to the configuration pipeline stall.\n**Customer Impact:**\nCustomers might have experienced issues while creating new forwarding rules for Global Layer 7 External load balancer or edit or delete existing ones.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/HoppX9SAc7jshpz6H52u","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Delhi (asia-south2)","id":"asia-south2"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"yatPQDmQ5uXqb6PzwanU","number":"13941430952028707728","begin":"2022-04-23T02:10:00+00:00","created":"2022-04-23T16:27:43+00:00","end":"2022-04-23T17:21:00+00:00","modified":"2022-04-25T20:06:04+00:00","external_desc":"Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub","updates":[{"created":"2022-04-25T20:05:32+00:00","modified":"2022-04-25T20:05:32+00:00","when":"2022-04-25T20:05:32+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 22 April 2022 19:10 PT\n**Incident End:** 23 April 2022 10:21 PT\n**Duration:** 15 hours,11 minutes\n**Affected Services and Features:**\nGoogle Cloud Pub/Sub - Google Cloud Monitoring\n**Regions/Zones:** Global Locale\n**Description:**\nGoogle Cloud Pub/Sub customers experienced issues with metrics in Google Cloud Monitoring for a duration of 15 hours, 11 minutes. The issue was caused by a configuration change to the backend for Cloud Monitoring that affected Cloud Pub/Sub metric recording. The issue was mitigated by reverting this change.\n**Customer Impact:** - Cloud Pub/Sub metrics in Cloud Monitoring for times during the incident may be missing or underreported. - The metric values lost in this timeframe will not be recoverable. - Any alerting based on these metrics might have fired erroneously or not fired when they should have during the time of the incident. - Any auto-scaling of Google Kubernetes Engine (GKE) based on these metrics may not have functioned as expected during the time of the incident. - Cloud Pub/Sub administrative, publish, and subscribe operations were not affected by the incident.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-23T17:40:46+00:00","modified":"2022-04-23T17:40:48+00:00","when":"2022-04-23T17:40:46+00:00","text":"The issue with Google Cloud Pub/Sub monitoring has been resolved for all affected projects as of Saturday, 2022-04-23 10:21 US/Pacific.\nWe will publish an analysis of this incident once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-23T17:27:03+00:00","modified":"2022-04-23T17:27:05+00:00","when":"2022-04-23T17:27:03+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We believe the issue with Google Cloud Pub/Sub monitoring was partially resolved as of 10:20 US/Pacific and are continuing to monitor the recovery of the service.\nWe do not have an ETA for full resolution at this point.\nWe will provide more information by Saturday, 2022-04-23 11:30 US/Pacific.\nDiagnosis: Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values. Any alerting based on these metrics may fire erroneously.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-23T16:53:35+00:00","modified":"2022-04-23T16:53:38+00:00","when":"2022-04-23T16:53:35+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Google Cloud Pub/Sub beginning on Friday, 2022-04-22 19:10 US/Pacific.\nThere is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Saturday, 2022-04-23 10:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values. Any alerting based on these metrics may fire erroneously.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-23T16:27:43+00:00","modified":"2022-04-23T16:27:46+00:00","when":"2022-04-23T16:27:43+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Google Cloud Pub/Sub beginning on Friday, 2022-04-22 19:10 US/Pacific.\nThere is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Saturday, 2022-04-23 10:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values. Any alerting based on these metrics may fire erroneously.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\ninstance/cpu/utilization\ninstance/network/received_bytes_count\ninstance/network/sent_bytes_count","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-04-25T20:05:32+00:00","modified":"2022-04-25T20:05:32+00:00","when":"2022-04-25T20:05:32+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 22 April 2022 19:10 PT\n**Incident End:** 23 April 2022 10:21 PT\n**Duration:** 15 hours,11 minutes\n**Affected Services and Features:**\nGoogle Cloud Pub/Sub - Google Cloud Monitoring\n**Regions/Zones:** Global Locale\n**Description:**\nGoogle Cloud Pub/Sub customers experienced issues with metrics in Google Cloud Monitoring for a duration of 15 hours, 11 minutes. The issue was caused by a configuration change to the backend for Cloud Monitoring that affected Cloud Pub/Sub metric recording. The issue was mitigated by reverting this change.\n**Customer Impact:** - Cloud Pub/Sub metrics in Cloud Monitoring for times during the incident may be missing or underreported. - The metric values lost in this timeframe will not be recoverable. - Any alerting based on these metrics might have fired erroneously or not fired when they should have during the time of the incident. - Any auto-scaling of Google Kubernetes Engine (GKE) based on these metrics may not have functioned as expected during the time of the incident. - Cloud Pub/Sub administrative, publish, and subscribe operations were not affected by the incident.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"dFjdLh2v6zuES6t9ADCB","service_name":"Google Cloud Pub/Sub","affected_products":[{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"}],"uri":"incidents/yatPQDmQ5uXqb6PzwanU","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"BNZwWJfvtjg3HxKA3bjY","number":"11838922764773661140","begin":"2022-04-21T00:59:00+00:00","created":"2022-04-21T17:44:44+00:00","end":"2022-04-21T05:18:00+00:00","modified":"2022-04-22T00:51:31+00:00","external_desc":"We are experiencing a decrease in availability of Cloud ML Vision, increase in latency.","updates":[{"created":"2022-04-22T00:51:20+00:00","modified":"2022-04-22T00:51:20+00:00","when":"2022-04-22T00:51:20+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 20 April 2021 17:59\n**Incident End:** 20 April 2021 22:18\n**Duration:** 4 hours, 19 minutes\n**Affected Services and Features:**\nCloud Vision - Vision API\n**Regions/Zones:** Global\n**Description:**\nCloud Vision API experienced increased latency and decreased availability for Cloud ML Vision text detection features for a duration of 4 hours, 19 minutes.\nFrom preliminary analysis, the root cause of the issue was due to a recent update in the Optical Character Recognition [OCR] Model [1] which resulted in performance issues on Cloud Vision API.\n[1] - https://cloud.google.com/vision/docs/ocr\nThe issue was resolved on Wednesday, 20 April 22:18 once a rollback of the change was completed.\n**Customer Impact:**\n- Affected customers may have experienced increased latency and elevated errors.\n- Some features of Cloud Vision API were temporarily unavailable.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-21T17:46:53+00:00","modified":"2022-04-21T17:46:53+00:00","when":"2022-04-21T17:46:53+00:00","text":"The issue with Cloud Vision has been resolved for all affected users as of Wednesday, 2022-04-20 22:17:21 US/Pacific. We thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-21T17:44:45+00:00","modified":"2022-04-21T17:44:45+00:00","when":"2022-04-21T17:44:45+00:00","text":"We are experiencing an issue with Cloud Vision beginning at Wednesday, 2022-04-20 17:59 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-04-21 00:35 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-04-22T00:51:20+00:00","modified":"2022-04-22T00:51:20+00:00","when":"2022-04-22T00:51:20+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 20 April 2021 17:59\n**Incident End:** 20 April 2021 22:18\n**Duration:** 4 hours, 19 minutes\n**Affected Services and Features:**\nCloud Vision - Vision API\n**Regions/Zones:** Global\n**Description:**\nCloud Vision API experienced increased latency and decreased availability for Cloud ML Vision text detection features for a duration of 4 hours, 19 minutes.\nFrom preliminary analysis, the root cause of the issue was due to a recent update in the Optical Character Recognition [OCR] Model [1] which resulted in performance issues on Cloud Vision API.\n[1] - https://cloud.google.com/vision/docs/ocr\nThe issue was resolved on Wednesday, 20 April 22:18 once a rollback of the change was completed.\n**Customer Impact:**\n- Affected customers may have experienced increased latency and elevated errors.\n- Some features of Cloud Vision API were temporarily unavailable.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"pDkm4vkHrT54mm7iSVHm","service_name":"Cloud Vision","affected_products":[{"title":"Cloud Vision","id":"pDkm4vkHrT54mm7iSVHm"}],"uri":"incidents/BNZwWJfvtjg3HxKA3bjY","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"waYi5P1JfEPrJh5tPNQU","number":"6524406606269626845","begin":"2022-04-18T03:52:00+00:00","created":"2022-04-18T16:49:41+00:00","end":"2022-04-19T00:33:00+00:00","modified":"2022-04-20T14:32:50+00:00","external_desc":"Cloud Logging is experiencing elevated latencies for log querying. Customers using global storage buckets or storage buckets in us-central1 to store their logs may see latencies on queries run against Cloud Logging. - Issue Resolved.","updates":[{"created":"2022-04-20T14:32:33+00:00","modified":"2022-04-20T14:32:33+00:00","when":"2022-04-20T14:32:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 17 April 2022 20:52\n**Incident End:** 18 April 2022 17:33\n**Duration:** 20 hours, 41 minutes\n**Affected Services and Features:**\nGoogle Cloud Logging\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Logging experienced elevated error rates and high latency in log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not have been able to see their recently written logs for a duration of 20 hours, 41 minutes.\nFrom preliminary analysis, an unexpected increase in uncached NOT_FOUND errors while validating log bucket configurations caused a large increase in the overall volume of configuration reads. This in turn caused latency of configuration reads and therefore validations to increase, leading to an ingestion backlog and a delay in log ingestion to the storage layer of our query backend.\nThe issue was resolved at 17:33 US/Pacific once the ingestion backlog cleared.\n**Customer Impact:**\n- Customers using global storage buckets or storage buckets in us-central1 to store their logs may not have been able to see their recently written logs for most of the duration of the event.\n- Customers may have experienced high latency or failures up to 90% on reads from Google Cloud Logging and other query requests.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-19T00:43:20+00:00","modified":"2022-04-19T00:43:20+00:00","when":"2022-04-19T00:43:20+00:00","text":"The issue on Cloud Logging Ingestion and querying has been resolved for all affected projects as of Monday, 2022-04-18 17:33 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T23:55:47+00:00","modified":"2022-04-18T23:55:48+00:00","when":"2022-04-18T23:55:47+00:00","text":"Summary: Cloud Logging is experiencing elevated latencies for log querying. Customers using global storage buckets or storage buckets in us-central1 to store their logs may see latencies on queries run against Cloud Logging.\nDescription: Our engineering team continues to investigate the issue with Log Querying on Cloud Logging. Log ingestion backlogs have cleared but customers may experience issues with querying logs.\nWe will provide an update by Monday, 2022-04-18 18:00 US/Pacific with current details.\nDiagnosis: Customers may experience high latency or failures when listing log entries and for other query requests.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T22:47:30+00:00","modified":"2022-04-18T22:47:31+00:00","when":"2022-04-18T22:47:30+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: Our engineering team has determined that further investigation is required to mitigate the issue with log querying.\nLog ingestion backlog has cleared but customers may experience issues with querying the logs.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2022-04-18 17:00 US/Pacific.\nDiagnosis:\n* Customers may experience high latency or failures when listing log entries and for other query requests\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T21:48:41+00:00","modified":"2022-04-18T21:48:44+00:00","when":"2022-04-18T21:48:41+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: Following the fix roll out, log ingestion backlog has cleared but customers may experience issue with querying the logs.\nMitigation work is currently underway by our engineering team for the issue with log querying.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2022-04-18 16:00 US/Pacific.\nDiagnosis:\n* Customers may experience high latency or failures when listing log entries and for other query requests\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T20:51:48+00:00","modified":"2022-04-18T20:51:49+00:00","when":"2022-04-18T20:51:48+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: Following the fix roll out, log ingestion backlog has cleared but customers may experience issue with querying the logs.\nOur engineering team is working to fix the log querying issue.\nWe will provide more information by Monday, 2022-04-18 15:00 US/Pacific.\nDiagnosis:\n* Customers may experience high latency or failures when listing log entries and for other query requests\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T19:53:12+00:00","modified":"2022-04-18T19:53:13+00:00","when":"2022-04-18T19:53:12+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: Following the fix roll out, log ingestion backlog has cleared but customers may experience issue with querying the logs.\nOur engineering team is working to fix the log querying issue.\nWe will provide more information by Monday, 2022-04-18 14:00 US/Pacific.\nDiagnosis:\n* Log ingestion backlog has cleared but customers may experience issue with querying the logs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T18:56:02+00:00","modified":"2022-04-18T18:56:03+00:00","when":"2022-04-18T18:56:02+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: A fix roll out is completed and customers may still experience delays while the log ingestion backlog is being cleared.\nThe backlog is expected to clear by Monday, 2022-04-18 15:00 US/Pacific.\nWe will provide more information by Monday, 2022-04-18 13:00 US/Pacific.\nDiagnosis:\n* There is a high latency in log ingestion with median latency of around 2 hours and 99% latency of 4.5 hours.\n* Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T17:53:53+00:00","modified":"2022-04-18T17:53:54+00:00","when":"2022-04-18T17:53:53+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Monday, 2022-04-18 12:00 US/Pacific.\nWe will provide more information by Monday, 2022-04-18 12:00 US/Pacific.\nDiagnosis: * There is a high latency in log ingestion with median latency of around 2 hours and 99% latency of 4.5 hours.\n* Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-18T16:49:40+00:00","modified":"2022-04-18T16:49:42+00:00","when":"2022-04-18T16:49:40+00:00","text":"Summary: Cloud Logging is experiencing elevated latency for log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Monday, 2022-04-18 12:00 US/Pacific.\nWe will provide more information by Monday, 2022-04-18 11:00 US/Pacific.\nDiagnosis: * There is a high latency in log ingestion in our backends with median latency of around 2 hours and 99% latency of 4.5 hours.\n* Customers using global storage buckets or storage buckets in us-central1 to store their logs may not be able to see their recently written logs for a few hours.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-04-20T14:32:33+00:00","modified":"2022-04-20T14:32:33+00:00","when":"2022-04-20T14:32:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 17 April 2022 20:52\n**Incident End:** 18 April 2022 17:33\n**Duration:** 20 hours, 41 minutes\n**Affected Services and Features:**\nGoogle Cloud Logging\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Logging experienced elevated error rates and high latency in log ingestion. Customers using global storage buckets or storage buckets in us-central1 to store their logs may not have been able to see their recently written logs for a duration of 20 hours, 41 minutes.\nFrom preliminary analysis, an unexpected increase in uncached NOT_FOUND errors while validating log bucket configurations caused a large increase in the overall volume of configuration reads. This in turn caused latency of configuration reads and therefore validations to increase, leading to an ingestion backlog and a delay in log ingestion to the storage layer of our query backend.\nThe issue was resolved at 17:33 US/Pacific once the ingestion backlog cleared.\n**Customer Impact:**\n- Customers using global storage buckets or storage buckets in us-central1 to store their logs may not have been able to see their recently written logs for most of the duration of the event.\n- Customers may have experienced high latency or failures up to 90% on reads from Google Cloud Logging and other query requests.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/waYi5P1JfEPrJh5tPNQU","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"ukkfXQc8CEeFZbSTYQi7","number":"14166479295409213890","begin":"2022-03-31T19:15:00+00:00","created":"2022-03-31T21:17:41+00:00","end":"2022-03-31T22:13:00+00:00","modified":"2022-04-01T18:49:41+00:00","external_desc":"us multiregion: Elevated errors on Cloud KMS requests.","updates":[{"created":"2022-04-01T18:49:24+00:00","modified":"2022-04-01T18:49:24+00:00","when":"2022-04-01T18:49:24+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 2022-03-31 12:15\n**Incident End:** 2022-03-31 15:13\n**Duration:** 2 hours, 58 minutes\n**Affected Services and Features:**\nGoogle Cloud Key Management Service (KMS)\nGoogle Cloud Storage (GCS)\n**Regions/Zones:** Multiregional US\n**Description:**\nA Cloud Key Management Service (KMS) job experienced multiple errors due to task crashes in one metro of the US multiregion for 2 Hours 58 Minutes. From the preliminary analysis, the root cause of was identified as a map-reduce-style batch job with a huge fast ramp-up of ReadObjects to Google Cloud Storage (GCS), which overloaded the KMS jobs (that are a dependency of GCS).\n**Customer Impact:**\n- The affected customers observed errors in Google Cloud Storage for one project.\n- Multiple tasks failed with Memory-Exceed Error.\n- There were a tiny non-zero amount of errors for some projects.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-03-31T22:14:05+00:00","modified":"2022-03-31T22:14:07+00:00","when":"2022-03-31T22:14:05+00:00","text":"The issue with Cloud Key Management Service has been resolved for all affected users as of Thursday, 2022-03-31 15:13 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-03-31T21:58:09+00:00","modified":"2022-03-31T21:58:10+00:00","when":"2022-03-31T21:58:09+00:00","text":"Summary: us multiregion: Elevated errors on Cloud KMS requests.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-03-31 16:30 US/Pacific.\nDiagnosis: Affected customers are seeing elevated errors on Cloud KMS requests in the us multiregion.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"created":"2022-03-31T21:17:40+00:00","modified":"2022-03-31T21:17:43+00:00","when":"2022-03-31T21:17:40+00:00","text":"Summary: us multiregion: Elevated errors on Cloud KMS requests.\nDescription: We are experiencing an issue with Cloud Key Management Service beginning at Thursday, 2022-03-31 12:15 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 15:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are seeing elevated errors on Cloud KMS requests in the us multiregion.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Multi-region: us","id":"us"}]}],"most_recent_update":{"created":"2022-04-01T18:49:24+00:00","modified":"2022-04-01T18:49:24+00:00","when":"2022-04-01T18:49:24+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 2022-03-31 12:15\n**Incident End:** 2022-03-31 15:13\n**Duration:** 2 hours, 58 minutes\n**Affected Services and Features:**\nGoogle Cloud Key Management Service (KMS)\nGoogle Cloud Storage (GCS)\n**Regions/Zones:** Multiregional US\n**Description:**\nA Cloud Key Management Service (KMS) job experienced multiple errors due to task crashes in one metro of the US multiregion for 2 Hours 58 Minutes. From the preliminary analysis, the root cause of was identified as a map-reduce-style batch job with a huge fast ramp-up of ReadObjects to Google Cloud Storage (GCS), which overloaded the KMS jobs (that are a dependency of GCS).\n**Customer Impact:**\n- The affected customers observed errors in Google Cloud Storage for one project.\n- Multiple tasks failed with Memory-Exceed Error.\n- There were a tiny non-zero amount of errors for some projects.","status":"AVAILABLE","affected_locations":[{"title":"Multi-region: us","id":"us"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"67cSySTL7dwJZo9JWUGU","service_name":"Cloud Key Management Service","affected_products":[{"title":"Cloud Key Management Service","id":"67cSySTL7dwJZo9JWUGU"}],"uri":"incidents/ukkfXQc8CEeFZbSTYQi7","currently_affected_locations":[],"previously_affected_locations":[{"title":"Multi-region: us","id":"us"}]},{"id":"RmPhfQT9RDGwWLCXS2sC","number":"3617221773064871579","begin":"2022-03-31T18:07:00+00:00","created":"2022-03-31T18:41:46+00:00","end":"2022-03-31T22:13:00+00:00","modified":"2022-04-01T19:27:03+00:00","external_desc":"Global: Delays creating or modifying load balancer configurations","updates":[{"created":"2022-04-01T19:26:50+00:00","modified":"2022-04-01T19:26:50+00:00","when":"2022-04-01T19:26:50+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 31 March 2022 11:07\n**Incident End:** 31 March 2022 15:13\n**Duration:** 4 hours, 6 minutes,\n**Affected Services and Features:**\n* Global External HTTP/S load balancing\n* Cloud CDN\n* Cloud Armor\n* Appengine flex\n**Regions/Zones:** Global Locale\n**Description:**\nCloud Load Balancers experienced delays creating or modifying configurations for 4 hours 6 minutes. From the preliminary analysis, the root cause of the issue is a rollout on the configuration management system that manages and pushes configuration for multiple Google infrastructure systems. The roll out caused an increase in the task counts which overloaded the configuration management system as all tasks pulled the configuration at the same time.\n**Customer Impact:**\n* Affected customers would have experienced delays in creating or modifying the configuration of Global External HTTP(S) load balancers, Cloud Armor and Cloud CDN.\n* App Engine Flexible apps might have failed to deploy.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-03-31T21:56:04+00:00","modified":"2022-03-31T21:56:06+00:00","when":"2022-03-31T21:56:04+00:00","text":"The issue with Cloud Armor, Google App Engine, Google Cloud Networking has been resolved for all affected users as of Thursday, 2022-03-31 14:55 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-03-31T20:17:13+00:00","modified":"2022-03-31T20:17:15+00:00","when":"2022-03-31T20:17:13+00:00","text":"Summary: Global: Delays creating or modifying load balancer configurations\nDescription: Mitigation efforts are reducing the backlog of delayed configuration changes, which are slowly being processed.\nWe will provide more information by Thursday, 2022-03-31 15:15 US/Pacific.\nDiagnosis: Affected customers will see delays in creating or modifying the configuration of Global External HTTP(S) load balancers, Cloud Armor and Cloud CDN. App Engine Flexible apps may fail to deploy. Configuration changes will be accepted, but not take effect until the issue is resolved. Existing configurations and deployments are not impacted.\nWorkaround: Configuration changes will eventually take effect. Failed App Engine Flexible deployments can be retried.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-03-31T19:43:10+00:00","modified":"2022-03-31T19:43:13+00:00","when":"2022-03-31T19:43:10+00:00","text":"Summary: Global: Delays creating or modifying load balancer configurations\nDescription: Our engineering team is exploring multiple mitigations options currently.\nWe will provide more information by Thursday, 2022-03-31 15:00 US/Pacific.\nDiagnosis: Affected customers will see delays in creating or modifying the configuration of Global External HTTP(S) load balancers, Cloud Armor and Cloud CDN. App Engine Flexible apps may fail to deploy. Configuration changes will be accepted, but not take effect until the issue is resolved. Existing configurations and deployments are not impacted.\nWorkaround: Configuration changes will eventually take effect. Failed App Engine Flexible deployments can be retried.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-03-31T19:11:56+00:00","modified":"2022-03-31T19:12:03+00:00","when":"2022-03-31T19:11:56+00:00","text":"Summary: Global: Unable to create or modify some Cloud Networking configurations\nDescription: Mitigation work is still underway by our engineering team.\nCurrent data indicates that approximately 15% of projects globally are affected by this issue.\nWe will provide more information by Thursday, 2022-03-31 14:00 US/Pacific.\nDiagnosis: Affected customers are not able to create or change External HTTP(S) load balancers. Cloud Armor and Cloud CDN have similar symptoms. App Engine Flexible apps may fail to deploy. In general, config change will be accepted, but will not take effect. Existing configurations are not impacted.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T18:51:39+00:00","modified":"2022-03-31T18:51:42+00:00","when":"2022-03-31T18:51:39+00:00","text":"Summary: Global: Unable to create or modify some Cloud Networking configurations\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-03-31 13:00 US/Pacific.\nDiagnosis: Affected customers are not able to create or change External HTTP(S) load balancers. Cloud Armor and Cloud CDN have similar symptoms. App Engine Flexible apps may fail to deploy. In general, config change will be accepted, but will not take effect. Existing configurations are not impacted.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T18:41:39+00:00","modified":"2022-03-31T18:41:50+00:00","when":"2022-03-31T18:41:39+00:00","text":"Summary: Global: Unable to create or modify regional external HTTP(S) load balancers\nDescription: We are experiencing an issue with Google Cloud Networking beginning at Thursday, 2022-03-31 11:07 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 12:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers are not able to create or change Regional external HTTP(S) load balancer balancers. Existing balancers are not impacted.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-04-01T19:26:50+00:00","modified":"2022-04-01T19:26:50+00:00","when":"2022-04-01T19:26:50+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 31 March 2022 11:07\n**Incident End:** 31 March 2022 15:13\n**Duration:** 4 hours, 6 minutes,\n**Affected Services and Features:**\n* Global External HTTP/S load balancing\n* Cloud CDN\n* Cloud Armor\n* Appengine flex\n**Regions/Zones:** Global Locale\n**Description:**\nCloud Load Balancers experienced delays creating or modifying configurations for 4 hours 6 minutes. From the preliminary analysis, the root cause of the issue is a rollout on the configuration management system that manages and pushes configuration for multiple Google infrastructure systems. The roll out caused an increase in the task counts which overloaded the configuration management system as all tasks pulled the configuration at the same time.\n**Customer Impact:**\n* Affected customers would have experienced delays in creating or modifying the configuration of Global External HTTP(S) load balancers, Cloud Armor and Cloud CDN.\n* App Engine Flexible apps might have failed to deploy.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Armor","id":"Kakg69gTC3xFyeJCY2va"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Cloud CDN","id":"ckSRJf2vQwQy188ULGy3"}],"uri":"incidents/RmPhfQT9RDGwWLCXS2sC","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Global","id":"global"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"B1hD4KAtcxiyAWkcANfV","number":"17742360388109155603","begin":"2022-03-31T15:30:00+00:00","created":"2022-03-31T18:40:18+00:00","end":"2022-03-31T22:54:00+00:00","modified":"2022-04-18T20:16:50+00:00","external_desc":"Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub","updates":[{"created":"2022-04-18T20:16:50+00:00","modified":"2022-04-18T20:16:50+00:00","when":"2022-04-18T20:16:50+00:00","text":"**Summary**\nOn Thursday March 31st, starting at 08:30 PT, Cloud Pub/Sub metrics were missing or were underreported in Cloud Monitoring for some Cloud Pub/Sub customers for a duration of 7 hours, 24 minutes. Google apologizes to customers who were affected by this outage and is taking steps to ensure that this type of outage does not reoccur.\n**Root Cause**\nOur investigation found the cause was a backend configuration change to our Cloud Monitoring service. This configuration changed the computation of some metrics not directly related to, but shared by, Cloud Pub/Sub. This configuration change progressively rolled out across all Google Cloud regions over two hours.\nThis configuration change increased the latency of requests to record metrics sent from Cloud Pub/Sub to Cloud Monitoring and, in some cases, resulted in failures due to write operations timing out.\n**Remediation and Prevention**\nEngineers were able to mitigate the issue by reverting the change that caused the issue, restoring services for all customers at 15:54 US/Pacific.\nWe are taking the following actions to ensure this does not happen again:\n- Improving the monitoring of Cloud Pub/Sub metrics reporting to allow for quicker error detection.\n- Making Cloud Pub/Sub metrics reporting operations more resilient to high latency.\n- Improving internal visibility and vetting of Cloud Monitoring backend configuration changes.\n**Detailed Description of Impact**\nOn Thursday March 31st, between 08:30 and 15:54 US/Pacific time:\n**Cloud Pub/Sub Metrics in Cloud Reporting**\n- The metric values lost during this timeframe are not recoverable.\n- Any alerting based on these metrics might have fired erroneously or not fired when it should have.\n- Auto scaling of Google Kubernetes Engine (GKE) based on these metrics may not have functioned as expected.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-04-01T20:13:25+00:00","modified":"2022-04-01T20:13:25+00:00","when":"2022-04-01T20:13:25+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support or help article https://support.google.com/a/answer/1047213.\n(All Times US/Pacific)\n**Incident Start:** 31 March 2022 08:30\n**Incident End:** 31 March 2022 15:54\n**Duration:** 7 hours, 24 minutes\n**Affected Services and Features:**\nGoogle Cloud Pub/Sub, Google Cloud Monitoring\n**Regions/Zones:** Global Locale\n**Description:**\nGoogle Cloud Pub/Sub customers experienced issues with metrics in Google Cloud Monitoring for a duration of 7 hours, 24 minutes. The issue was caused by a configuration change to the backend for Cloud Monitoring that affected Cloud Pub/Sub metric recording. The issue was mitigated by reverting this change.\n**Customer Impact:**\n- Cloud Pub/Sub metrics in Cloud Monitoring for times during the incident may be missing or underreported. - The metric values lost in this timeframe will not be recoverable.\n- Any alerting based on these metrics might have fired erroneously or not fired when they should have during the time of the incident.\n- Any auto scaling of Google Kubernetes Engine (GKE) based on these metrics may not have functioned as expected during the time of the incident.\n- Cloud Pub/Sub administrative, publish, and subscribe operations were not affected by the incident.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T23:11:12+00:00","modified":"2022-03-31T23:11:14+00:00","when":"2022-03-31T23:11:12+00:00","text":"The issue with Google Cloud Pub/Sub monitoring has been resolved for all affected projects as of Thursday, 2022-03-31 15:54 US/Pacific.\nWe will publish an analysis of this incident once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T22:23:34+00:00","modified":"2022-03-31T22:23:36+00:00","when":"2022-03-31T22:23:34+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: ​​We believe the issue with Google Cloud Pub/Sub monitoring was partially resolved as of 14:57, and are continuing to monitor the recovery of the service.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Thursday, 2022-03-31 16:30 US/Pacific with current details.\nDiagnosis: - Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values.\n- Any alerting based on these metrics may fire erroneously.\n- Any auto scaling of GKE based on these metrics may not function as expected due to lack of or underreported values.\nPublish and subscribe metrics are currently affected for publishers and subscribers in following regions:\n- asia-east1\n- europe-north1\n- europe-west4\n- us-central1\n- us-central2\n- us-east1\n- us-east4\n- us-east7\n- us-west1\n- us-west4\nBacklog metrics for subscriptions and snapshot metrics in all regions are no longer affected in any region.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T21:49:14+00:00","modified":"2022-03-31T21:49:15+00:00","when":"2022-03-31T21:49:14+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring metrics for Cloud Pub/Sub beginning Thursday, 2022-03-31 09:30 US/Pacific. There is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 15:25 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: - Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values.\n- Any alerting based on these metrics may fire erroneously.\n- Any auto scaling of GKE based on these metrics may not function as expected due to lack of or underreported values.\nPublish and subscribe metrics are currently affected for publishers and subscribers in following regions:\n- asia-east1\n- europe-north1\n- europe-west4\n- us-central1\n- us-central2\n- us-east1\n- us-east4\n- us-east7\n- us-west1\n- us-west4\nBacklog metrics for subscriptions and snapshot metrics in all regions are no longer affected in any region.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T21:20:01+00:00","modified":"2022-03-31T21:20:01+00:00","when":"2022-03-31T21:20:01+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring metrics for Cloud Pub/Sub beginning Thursday, 2022-03-31 09:30 US/Pacific. There is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 14:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: - Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values.\n- Any alerting based on these metrics may fire erroneously.\n- Any auto scaling of GKE based on these metrics may not function as expected due to lack of or underreported values.\nPublish and subscribe metrics are currently affected for publishers and subscribers in following regions:\n- asia-east1\n- europe-north1\n- europe-west4\n- us-central1\n- us-central2\n- us-east1\n- us-east4\n- us-east7\n- us-west1\n- us-west4\nBacklog metrics for subscriptions and snapshot metrics in all regions are currently affected.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T20:49:56+00:00","modified":"2022-03-31T20:49:57+00:00","when":"2022-03-31T20:49:56+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring metrics for Cloud Pub/Sub beginning Thursday, 2022-03-31 09:30 US/Pacific. There is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 14:25 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: - Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values.\n- Any alerting based on these metrics may fire erroneously.\n- Any auto scaling of GKE based on these metrics may not function as expected due to lack of or underreported values.\nPublish and subscribe metrics are currently affected for publishers and subscribers in following regions:\n- asia-east1\n- europe-north1\n- europe-west4\n- us-central1\n- us-central2\n- us-east1\n- us-east4\n- us-east7\n- us-west1\n- us-west4\nBacklog metrics for subscriptions and snapshot metrics in all regions are currently affected.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T20:18:51+00:00","modified":"2022-03-31T20:18:52+00:00","when":"2022-03-31T20:18:51+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring and Cloud Pub/Sub beginning Thursday, 2022-03-31 09:30 US/Pacific. There is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 13:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: - Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values.\n- Any alerting based on these metrics may fire erroneously.\n- Any auto scaling of GKE based on these metrics may not function as expected due to lack of or underreported values.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T19:52:27+00:00","modified":"2022-03-31T19:52:33+00:00","when":"2022-03-31T19:52:27+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring and Cloud Pub/Sub beginning Thursday, 2022-03-31 09:30 US/Pacific. There is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 13:25 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values. Any alerting based on these metrics may fire erroneously.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T19:23:08+00:00","modified":"2022-03-31T19:23:09+00:00","when":"2022-03-31T19:23:08+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable or underreported for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring and Cloud Pub/Sub beginning Thursday, 2022-03-31 09:30 US/Pacific. There is no known impact on Cloud Pub/Sub administrative, publish, or subscribe operations at this time.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 12:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers impacted by this issue may see Cloud Monitoring metrics for Cloud Pub/Sub that show no or underreported values. Any alerting based on these metrics may fire erroneously.\nWorkaround: Non-Cloud-Pub/Sub metrics and logs on publish and subscriber clients can be used as a proxy to ensure that publishing and subscribing is still behaving as expected. For example, metrics available for clients running on GCE include:\n- instance/cpu/utilization\n- instance/network/received_bytes_count\n- instance/network/sent_bytes_count","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T18:58:09+00:00","modified":"2022-03-31T18:58:10+00:00","when":"2022-03-31T18:58:09+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable for Cloud Pub/Sub\nDescription: We are experiencing an issue with Cloud Monitoring and Cloud Pub/Sub beginning at Thursday, 2022-03-31 09:30 US/Pacific.\nEngineering is continuing to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 12:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience unavailable Metrics in Cloud Monitoring for Cloud Pub/Sub\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-31T18:40:17+00:00","modified":"2022-03-31T18:40:19+00:00","when":"2022-03-31T18:40:17+00:00","text":"Summary: Global: Cloud Monitoring Metrics may be unavailable\nDescription: We are experiencing an issue with Cloud Monitoring beginning at Thursday, 2022-03-31 09:30 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-03-31 12:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience unavailable Metrics in Cloud Monitoring\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-04-18T20:16:50+00:00","modified":"2022-04-18T20:16:50+00:00","when":"2022-04-18T20:16:50+00:00","text":"**Summary**\nOn Thursday March 31st, starting at 08:30 PT, Cloud Pub/Sub metrics were missing or were underreported in Cloud Monitoring for some Cloud Pub/Sub customers for a duration of 7 hours, 24 minutes. Google apologizes to customers who were affected by this outage and is taking steps to ensure that this type of outage does not reoccur.\n**Root Cause**\nOur investigation found the cause was a backend configuration change to our Cloud Monitoring service. This configuration changed the computation of some metrics not directly related to, but shared by, Cloud Pub/Sub. This configuration change progressively rolled out across all Google Cloud regions over two hours.\nThis configuration change increased the latency of requests to record metrics sent from Cloud Pub/Sub to Cloud Monitoring and, in some cases, resulted in failures due to write operations timing out.\n**Remediation and Prevention**\nEngineers were able to mitigate the issue by reverting the change that caused the issue, restoring services for all customers at 15:54 US/Pacific.\nWe are taking the following actions to ensure this does not happen again:\n- Improving the monitoring of Cloud Pub/Sub metrics reporting to allow for quicker error detection.\n- Making Cloud Pub/Sub metrics reporting operations more resilient to high latency.\n- Improving internal visibility and vetting of Cloud Monitoring backend configuration changes.\n**Detailed Description of Impact**\nOn Thursday March 31st, between 08:30 and 15:54 US/Pacific time:\n**Cloud Pub/Sub Metrics in Cloud Reporting**\n- The metric values lost during this timeframe are not recoverable.\n- Any alerting based on these metrics might have fired erroneously or not fired when it should have.\n- Auto scaling of Google Kubernetes Engine (GKE) based on these metrics may not have functioned as expected.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"}],"uri":"incidents/B1hD4KAtcxiyAWkcANfV","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"4rRjbE16mteQwUeXPZwi","number":"8134027662519725646","begin":"2022-03-29T21:00:00+00:00","created":"2022-03-30T01:14:53+00:00","end":"2022-03-29T22:27:00+00:00","modified":"2022-03-30T17:30:12+00:00","external_desc":"We've received a report of an issue with Cloud Talent Solution - SearchJobs API queries.","updates":[{"created":"2022-03-30T17:30:12+00:00","modified":"2022-03-30T17:30:12+00:00","when":"2022-03-30T17:30:12+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case\nusing https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 29 March 2022 14:00\n**Incident End:** 29 March 2022 15:27\n**Duration:** 1 hour, 27 minutes\n**Affected Services and Features:**\nCloud Talent Solution - Job Search\n**Regions/Zones:** Global Locale\n**Description:**\nCloud Talent Solution Job Search experienced elevated error rate for a duration of 1 hour, 27 minutes. From preliminary analysis, the root cause of the issue is reduced availability on a dependency component that is responsible for enhancing search results.\n**Customer Impact:**\nAffected customers would have experienced deadline exceeded 504 or service unavailable 503 errors when performing Job search.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-30T01:23:02+00:00","modified":"2022-03-30T01:23:02+00:00","when":"2022-03-30T01:23:02+00:00","text":"We experienced an issue with Cloud Talent Solution - SearchJobs API, beginning at Tuesday, 2022-03-29 14:00 US/Pacific.\nCustomers may have experienced 504 - Deadline exceeded errors, on SearchJobs API queries.\nThe issue has been resolved for all affected projects as of Tuesday, 2022-03-29 15:27 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},{"created":"2022-03-30T01:16:59+00:00","modified":"2022-03-30T01:44:33+00:00","when":"2022-03-30T01:16:59+00:00","text":"Summary: We've received a report of an issue with Cloud Talent Solution - SearchJobs API queries.\nDescription: This is a retrospective update for a reported issue with Cloud Talent Solution - SearchJobs API queries, reported between 2022-03-29 14:00 - 15:27, US/Pacific.\nWe will provide more information by Tuesday, 2022-03-29 18:30 US/Pacific.\nDiagnosis: Customers may have experienced 504 - Deadline exceeded errors, on SearchJobs API queries.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Global","id":"global"}]}],"most_recent_update":{"created":"2022-03-30T17:30:12+00:00","modified":"2022-03-30T17:30:12+00:00","when":"2022-03-30T17:30:12+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case\nusing https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 29 March 2022 14:00\n**Incident End:** 29 March 2022 15:27\n**Duration:** 1 hour, 27 minutes\n**Affected Services and Features:**\nCloud Talent Solution - Job Search\n**Regions/Zones:** Global Locale\n**Description:**\nCloud Talent Solution Job Search experienced elevated error rate for a duration of 1 hour, 27 minutes. From preliminary analysis, the root cause of the issue is reduced availability on a dependency component that is responsible for enhancing search results.\n**Customer Impact:**\nAffected customers would have experienced deadline exceeded 504 or service unavailable 503 errors when performing Job search.","status":"AVAILABLE","affected_locations":[{"title":"Global","id":"global"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Talent Solution - Job Search","id":"VXydUfMqEtvZxXJFm56a"},{"title":"Cloud Machine Learning","id":"z9PfKanGZYvYNUbnKzRJ"}],"uri":"incidents/4rRjbE16mteQwUeXPZwi","currently_affected_locations":[],"previously_affected_locations":[{"title":"Global","id":"global"}]},{"id":"2j8xsJMSyDhmgfJriGeR","number":"5259740469836333814","begin":"2022-03-28T22:30:00+00:00","created":"2022-03-29T04:53:00+00:00","end":"2022-03-29T09:58:00+00:00","modified":"2022-03-29T23:36:25+00:00","external_desc":"Cloud Spanner instance creation failure","updates":[{"created":"2022-03-29T23:36:23+00:00","modified":"2022-03-29T23:36:23+00:00","when":"2022-03-29T23:36:23+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case\nusing https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 28 March 2022 15:30\n**Incident End:** 29 March 2022 2:58\n**Duration:** 11 hours, 28 minutes\n**Affected Services and Features:**\nCloud Spanner - Creation of new instance , Scaling existing instances.\n**Regions/Zones:** us-east1, europe-central2, multi-region location nam6\n**Description:**\nCloud Spanner experienced delays in creation and scaling of instances in us-east1, europe-central2, and multi-region location nam6 [1] for a duration of 11 hours, 28 minutes. The cause of this was a rollout of a new feature that affected the ability to create or modify instance compute capacity (via instance scaling or new instance creation). This issue was discovered internally and mitigation was performed via updating the configuration back to the previous settings.\n[1] https://cloud.google.com/spanner/docs/instance-configurations#available-configurations-multi-region\n**Customer Impact:**\nAffected customers would have experienced the following operations being stuck or failing with a timeout:\n* Creation of new Cloud Spanner instances\n* Scaling existing Cloud Spanner instances","status":"AVAILABLE","affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T10:46:17+00:00","modified":"2022-03-29T10:46:24+00:00","when":"2022-03-29T10:46:17+00:00","text":"The issue with Cloud Spanner has been resolved for all affected users as of Tuesday, 2022-03-29 03:13 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T09:56:46+00:00","modified":"2022-03-29T09:56:56+00:00","when":"2022-03-29T09:56:46+00:00","text":"Summary: Cloud Spanner instance creation failure\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2022-03-29 04:30 US/Pacific.\nWe will provide more information by Tuesday, 2022-03-29 04:30 US/Pacific.\nDiagnosis: The impacted users might experience timeout failures while:\n1) Creating a new Cloud Spanner instance\n2) Scaling existing Cloud Spanner instance\nWorkaround: As a workaround, users may try to create new cloud spanner instance in a different region","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T09:06:41+00:00","modified":"2022-03-29T09:06:42+00:00","when":"2022-03-29T09:06:41+00:00","text":"Summary: Cloud Spanner instance creation failure\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2022-03-29 03:30 US/Pacific.\nWe will provide more information by Tuesday, 2022-03-29 03:30 US/Pacific.\nDiagnosis: The impacted users might experience timeout failures while:\n1) Creating a new Cloud Spanner instance\n2) Scaling existing Cloud Spanner instance\nWorkaround: As a workaround, users may try to create new cloud spanner instance in a different region","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T07:15:48+00:00","modified":"2022-03-29T07:15:54+00:00","when":"2022-03-29T07:15:48+00:00","text":"Summary: Cloud Spanner instance creation failure\nDescription: We are experiencing an issue with Cloud Spanner beginning at Monday, 2022-03-28 15:30 US/Pacific. Updating and creating a new Cloud Spanner instance in us-east1 and europe-central2 might fail with the timeout errors.\nOur engineering team continues to investigate the issue and working towards the mitigation.\nWe will provide an update by Tuesday, 2022-03-29 05:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: The impacted users might experience timeout failures while:\n1) Creating a new Cloud Spanner instance\n2) Scaling existing Cloud Spanner instance\nWorkaround: As a workaround, users may try to create new cloud spanner instance in a different region","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T05:48:39+00:00","modified":"2022-03-29T05:48:40+00:00","when":"2022-03-29T05:48:39+00:00","text":"Summary: Cloud Spanner instance creation failure\nDescription: We are experiencing an issue with Cloud Spanner beginning at Monday, 2022-03-28 15:30 US/Pacific. Updating and creating a new Cloud Spanner instance in us-east1 might be failing with timeout errors.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-03-29 01:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: The impacted users might experience timeout failures while:\n1) Creating a new Cloud Spanner instance\n2) Scaling existing Cloud Spanner instance\nWorkaround: As a workaround, users may try to create new cloud spanner instance in a different region","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T05:29:01+00:00","modified":"2022-03-29T05:29:07+00:00","when":"2022-03-29T05:29:01+00:00","text":"Summary: Cloud Spanner instance creation failure\nDescription: We are experiencing an issue with Cloud Spanner beginning at Monday, 2022-03-28 15:30 US/Pacific. A new Cloud Spanner instance creation in us-east1 might be failing with timeout errors.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-03-29 01:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: The impacted users might experience timeout failures while creating a new Cloud Spanner instance\nWorkaround: As a workaround, users may try to create new cloud spanner instance in a different region","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"created":"2022-03-29T04:52:54+00:00","modified":"2022-03-29T04:53:00+00:00","when":"2022-03-29T04:52:54+00:00","text":"Summary: Cloud Spanner instance creation failure\nDescription: We are experiencing an issue with Cloud Spanner beginning at Monday, 2022-03-28 19:00 US/Pacific. A new Cloud Spanner instance creation in us-east1 might be failing with timeout errors.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2022-03-28 23:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: The impacted users might experience timeout failures while creating a new Cloud Spanner instance\nWorkaround: As a workaround, users may try to create new instance in a different region","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"South Carolina (us-east1)","id":"us-east1"}]}],"most_recent_update":{"created":"2022-03-29T23:36:23+00:00","modified":"2022-03-29T23:36:23+00:00","when":"2022-03-29T23:36:23+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case\nusing https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 28 March 2022 15:30\n**Incident End:** 29 March 2022 2:58\n**Duration:** 11 hours, 28 minutes\n**Affected Services and Features:**\nCloud Spanner - Creation of new instance , Scaling existing instances.\n**Regions/Zones:** us-east1, europe-central2, multi-region location nam6\n**Description:**\nCloud Spanner experienced delays in creation and scaling of instances in us-east1, europe-central2, and multi-region location nam6 [1] for a duration of 11 hours, 28 minutes. The cause of this was a rollout of a new feature that affected the ability to create or modify instance compute capacity (via instance scaling or new instance creation). This issue was discovered internally and mitigation was performed via updating the configuration back to the previous settings.\n[1] https://cloud.google.com/spanner/docs/instance-configurations#available-configurations-multi-region\n**Customer Impact:**\nAffected customers would have experienced the following operations being stuck or failing with a timeout:\n* Creation of new Cloud Spanner instances\n* Scaling existing Cloud Spanner instances","status":"AVAILABLE","affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"EcNGGUgBtBLrtm4mWvqC","service_name":"Cloud Spanner","affected_products":[{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"}],"uri":"incidents/2j8xsJMSyDhmgfJriGeR","currently_affected_locations":[],"previously_affected_locations":[{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"South Carolina (us-east1)","id":"us-east1"}]},{"id":"MtMwhU6SXrpBeg5peXqY","number":"17330021626924647123","begin":"2022-03-25T07:00:00+00:00","created":"2022-03-25T08:22:51+00:00","end":"2022-03-25T07:16:00+00:00","modified":"2022-03-25T19:59:34+00:00","external_desc":"Global: We experienced Google Cloud Networking issues.","updates":[{"created":"2022-03-25T19:59:31+00:00","modified":"2022-03-25T19:59:31+00:00","when":"2022-03-25T19:59:31+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 25 March 2022 00:00\n**Incident End:** 25 March 2022 00:16\n**Duration:** 16 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking\n**Regions/Zones:** Multiple regions\n**Description:**\nGoogle Cloud Networking experienced packet loss due to transient failures on several fiber paths between two metro locations in the US northwest region and US central region. This packet loss led to disruption of traffic between multiple cloud regions globally. Most of the transient failures were cleared by 25 March 2022, 00:08 US/Pacific mitigating the issue for global regions.\nAn unrelated transient fiber failure in the Asia region caused an additional minor disruption that was limited only to the APAC region between 25 March 2022 00:10 US/Pacific and 25 March 2022 00:16 US/Pacific.\n**Customer Impact:**\nCustomers may have experienced issues with transit traffic between multiple Google Cloud regions. Following are the cloud regions which were affected the most:\n* Traffic FROM us-east2 TO asia-east1\n* Traffic FROM all europe regions TO us-west1, asia-east1and asia-south2\n* Traffic FROM us-east1, us-east2, us-east4 TO us-west1, asia-east1and asia-south2\n* Traffic FROM us-central1 TO us-west1, asia-east1and asia-south2","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-03-25T08:38:09+00:00","modified":"2022-03-25T08:38:09+00:00","when":"2022-03-25T08:38:09+00:00","text":"We experienced an issue with Google Cloud Networking beginning at 2022-03-25 00:00 US/Pacific to 2022-03-25 00:08 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"created":"2022-03-25T08:22:51+00:00","modified":"2022-03-25T08:22:52+00:00","when":"2022-03-25T08:22:51+00:00","text":"Summary: Global: We experienced Google Cloud Networking issues.\nDescription: We experienced an issue with Google Cloud Networking , beginning at Friday, 2022-03-25 00:00 US/Pacific to 2022-03-25 00:08.\nOur engineering team continues to monitor the issue.\nWe will provide an update by Friday, 2022-03-25 02:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]}],"most_recent_update":{"created":"2022-03-25T19:59:31+00:00","modified":"2022-03-25T19:59:31+00:00","when":"2022-03-25T19:59:31+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 25 March 2022 00:00\n**Incident End:** 25 March 2022 00:16\n**Duration:** 16 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking\n**Regions/Zones:** Multiple regions\n**Description:**\nGoogle Cloud Networking experienced packet loss due to transient failures on several fiber paths between two metro locations in the US northwest region and US central region. This packet loss led to disruption of traffic between multiple cloud regions globally. Most of the transient failures were cleared by 25 March 2022, 00:08 US/Pacific mitigating the issue for global regions.\nAn unrelated transient fiber failure in the Asia region caused an additional minor disruption that was limited only to the APAC region between 25 March 2022 00:10 US/Pacific and 25 March 2022 00:16 US/Pacific.\n**Customer Impact:**\nCustomers may have experienced issues with transit traffic between multiple Google Cloud regions. Following are the cloud regions which were affected the most:\n* Traffic FROM us-east2 TO asia-east1\n* Traffic FROM all europe regions TO us-west1, asia-east1and asia-south2\n* Traffic FROM us-east1, us-east2, us-east4 TO us-west1, asia-east1and asia-south2\n* Traffic FROM us-central1 TO us-west1, asia-east1and asia-south2","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/MtMwhU6SXrpBeg5peXqY","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Hong Kong (asia-east2)","id":"asia-east2"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Mumbai (asia-south1)","id":"asia-south1"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Toronto (northamerica-northeast2)","id":"northamerica-northeast2"},{"title":"São Paulo (southamerica-east1)","id":"southamerica-east1"},{"title":"Santiago (southamerica-west1)","id":"southamerica-west1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"},{"title":"Los Angeles (us-west2)","id":"us-west2"},{"title":"Salt Lake City (us-west3)","id":"us-west3"},{"title":"Las Vegas (us-west4)","id":"us-west4"}]},{"id":"R9vAbtGnhzo6n48SnqTj","number":"2948654908633925955","begin":"2022-03-22T22:30:00+00:00","created":"2022-03-23T00:14:43+00:00","end":"2022-03-23T00:15:00+00:00","modified":"2022-03-28T15:30:50+00:00","external_desc":"Customers connecting from Mexico and Central America may experience issues with accessing GCP services","updates":[{"created":"2022-03-28T15:30:50+00:00","modified":"2022-03-28T15:30:50+00:00","when":"2022-03-28T15:30:50+00:00","text":"**Summary**\nOn Tuesday, 22 March 2022, Google Cloud Networking experienced congestion on network infrastructure to and from the network edge locations [1] in Queretaro, Mexico, for a duration of 1 hour and 45 minutes, following multiple fiber cuts between the United States and Mexico. Affected customers may have experienced high latency, high retransmits and elevated errors.\nWe would like to apologize for the length and severity of this incident. We are taking immediate steps to prevent a recurrence and improve reliability in the future.\n[1] - https://cloud.google.com/vpc/docs/edge-locations\n**Root Cause**\nGoogle’s wide-area network is primarily comprised of infrastructure owned and operated by Google, although some physical links use fiber infrastructure which is owned and operated by third parties. Google has multiple resilient fiber paths and network points of presence serving users in Mexico.\nMultiple simultaneous fiber cuts on diversely-routed paths, at the same time as a planned maintenance event on another diversely-routed path, resulted in a capacity shortfall in Google’s wide-area network between Queretaro, Mexico and the United States. This multiple failure event represents a rare case in which the resulting capacity shortfall was visible to Google's users.\nTraffic was subsequently redirected to alternate links; however, there was insufficient capacity to serve all the redirected traffic, resulting in packet loss for user-facing network traffic to Google from users in Mexico.\n**Remediation and Prevention**\nGoogle’s automated repair mechanism detected the congestion on Tuesday, 22 March 2022, at 15:42 US/Pacific and redirected the traffic through alternate edge locations. However, the alternate link did not have sufficient capacity to serve all the redirected traffic, resulting in packet loss. The congestion was cleared when traffic was manually routed around the failed links on Tuesday, 22 March 2022, at 17:15 US/Pacific.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. However, this incident was the result of multiple concurrent failures on links which are designed to provide resilience; such failures, whilst very rare, are expected at a low frequency. We are taking the following steps to ensure we respond quickly to similar events in the future:\n* Improving the reliability of tooling that manually reroutes traffic in extreme multiple-failure scenarios.\n* Improving the diagnostic systems that help Google's Cloud Networking teams decide how to deal with extreme multiple-failure scenarios.\nWe appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\n**Detailed Description of Impact**\nOn Tuesday, 22 March 2022 from 15:30 to 17:15 US/Pacific, customers reaching Google services from Mexico may have experienced network slowness and packet loss, and errors returned from our services. This may have resulted in customers not being able to access services running on Google Cloud and Workspace services including:\n* Google Meet\n* Gmail\n* Google Drive","status":"AVAILABLE","affected_locations":[]},{"created":"2022-03-23T17:25:19+00:00","modified":"2022-03-23T17:25:19+00:00","when":"2022-03-23T17:25:19+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 22 March 2022 15:30\n**Incident End:** 22 March 2022 17:15\n**Duration:** 1 hour, 45 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking - Cloud VPN, Cloud Interconnect, Cloud Load Balancers\nWorkspace Products - Gmail, Meet, Drive and Other Workspace products\n**Regions/Zones:** Mexico\n**Description:**\nGoogle Cloud Networking experienced congestion on network infrastructure to and from the network edge locations in Queretaro, Mexico, for a duration of 1 hour and 45 minutes, following a fiber cut between the United States \u0026 Mexico.\nGoogle’s automated repair mechanism detected the congestion and redirected the traffic through alternate edge locations. However, the alternate link did not have sufficient capacity to serve all the redirected traffic, resulting in packet loss for user-facing network traffic to Google. When traffic was manually routed to take other paths, the congestion was cleared.\n**Customer Impact:**\nCustomers reaching Google services, including GCP and Workspace, from Mexico may have experienced high latency, high packet loss, high retransmits, and errors returned from our services, resulting in customers not being able to access those services.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-03-23T01:28:48+00:00","modified":"2022-03-23T01:28:49+00:00","when":"2022-03-23T01:28:48+00:00","text":"The issue with Cloud Interconnect, Cloud Load Balancing, Cloud NAT, Cloud Networking, Cloud Router, Cloud VPN has been resolved for all affected users as of Tuesday, 2022-03-22 17:15 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-03-23T00:49:12+00:00","modified":"2022-03-23T00:49:12+00:00","when":"2022-03-23T00:49:12+00:00","text":"Summary: Customers connecting from Mexico and Central America may experience issues with accessing GCP services\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-03-22 18:30 US/Pacific.\nDiagnosis: Customers may experience delays in accessing GCP services from Mexico and Central America.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-03-23T00:15:29+00:00","modified":"2022-03-23T00:15:30+00:00","when":"2022-03-23T00:15:29+00:00","text":"Summary: Customers connecting from South-Central America may experience issues with accessing GCP services\nDescription: We are experiencing an issue with Cloud Interconnect, Cloud Load Balancing, Cloud NAT, Cloud Networking, Cloud Router, Cloud VPN beginning at Tuesday, 2022-03-22 16:15 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-03-22 17:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience delays in accessing GCP services from South-Central America.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-03-23T00:14:37+00:00","modified":"2022-03-23T00:14:43+00:00","when":"2022-03-23T00:14:37+00:00","text":"Summary: Customers connecting from South-Central America may experience issues with accessing GCP services\nDescription: We are experiencing an issue with Cloud Interconnect, Cloud Load Balancing, Cloud NAT, Cloud Networking, Cloud Router, Cloud VPN beginning at Tuesday, 2022-03-22 16:15 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-03-22 20:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience delays in accessing GCP services from South-Central America.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-03-28T15:30:50+00:00","modified":"2022-03-28T15:30:50+00:00","when":"2022-03-28T15:30:50+00:00","text":"**Summary**\nOn Tuesday, 22 March 2022, Google Cloud Networking experienced congestion on network infrastructure to and from the network edge locations [1] in Queretaro, Mexico, for a duration of 1 hour and 45 minutes, following multiple fiber cuts between the United States and Mexico. Affected customers may have experienced high latency, high retransmits and elevated errors.\nWe would like to apologize for the length and severity of this incident. We are taking immediate steps to prevent a recurrence and improve reliability in the future.\n[1] - https://cloud.google.com/vpc/docs/edge-locations\n**Root Cause**\nGoogle’s wide-area network is primarily comprised of infrastructure owned and operated by Google, although some physical links use fiber infrastructure which is owned and operated by third parties. Google has multiple resilient fiber paths and network points of presence serving users in Mexico.\nMultiple simultaneous fiber cuts on diversely-routed paths, at the same time as a planned maintenance event on another diversely-routed path, resulted in a capacity shortfall in Google’s wide-area network between Queretaro, Mexico and the United States. This multiple failure event represents a rare case in which the resulting capacity shortfall was visible to Google's users.\nTraffic was subsequently redirected to alternate links; however, there was insufficient capacity to serve all the redirected traffic, resulting in packet loss for user-facing network traffic to Google from users in Mexico.\n**Remediation and Prevention**\nGoogle’s automated repair mechanism detected the congestion on Tuesday, 22 March 2022, at 15:42 US/Pacific and redirected the traffic through alternate edge locations. However, the alternate link did not have sufficient capacity to serve all the redirected traffic, resulting in packet loss. The congestion was cleared when traffic was manually routed around the failed links on Tuesday, 22 March 2022, at 17:15 US/Pacific.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. However, this incident was the result of multiple concurrent failures on links which are designed to provide resilience; such failures, whilst very rare, are expected at a low frequency. We are taking the following steps to ensure we respond quickly to similar events in the future:\n* Improving the reliability of tooling that manually reroutes traffic in extreme multiple-failure scenarios.\n* Improving the diagnostic systems that help Google's Cloud Networking teams decide how to deal with extreme multiple-failure scenarios.\nWe appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\n**Detailed Description of Impact**\nOn Tuesday, 22 March 2022 from 15:30 to 17:15 US/Pacific, customers reaching Google services from Mexico may have experienced network slowness and packet loss, and errors returned from our services. This may have resulted in customers not being able to access services running on Google Cloud and Workspace services including:\n* Google Meet\n* Gmail\n* Google Drive","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Cloud NAT","id":"hCNpnTQHkUCCGxJy35Yq"}],"uri":"incidents/R9vAbtGnhzo6n48SnqTj","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"aA3kbJm5nwvVTKnYbrWM","number":"551739384385711524","begin":"2022-03-18T22:20:00+00:00","created":"2022-03-18T23:53:02+00:00","end":"2022-03-18T22:28:00+00:00","modified":"2022-03-29T03:49:06+00:00","external_desc":"Cloud Networking: Up to 40% packet loss between affected zones","updates":[{"created":"2022-03-29T03:49:06+00:00","modified":"2022-03-29T03:49:06+00:00","when":"2022-03-29T03:49:06+00:00","text":"**Summary**\nOn Friday, 18 March 2022 at 15:20 US/Pacific, Google Cloud Networking experienced intermittent packet loss for traffic between multiple cloud regions for a duration of 8 minutes. The issue was identified and mitigated automatically by 15:28 US/Pacific.\nWe understand this issue has affected our valued customers and users, and we apologize to those who were affected.\n**Root Cause**\nGoogle’s production backbone is a global network that enables connectivity for all user-facing traffic via Points of Presence (POPs) or internet exchanges.\nA rare hardware failure of a component on the fiber paths from one of the transpacific gateway campuses in Google’s production backbone led to a decrease in available network bandwidth between the gateway and multiple edge locations, causing packet loss.\n**Remediation and Prevention**\nGoogle’s automated repair mechanisms detected the decrease in available network bandwidth on Friday, 18 March 2022 at 15:20 US/Pacific and automatically routed the traffic through alternate links. The traffic rerouting completed on Friday, 18 March 2022 at 15:28 US/Pacific, mitigating the issue.\nWhile our automated mechanisms worked as intended and recovered the traffic without manual intervention, we understand that the scope of impact caused by this rare event affected our customers. We have been working on optimizing our global network to minimize the time spent automatically reconfiguring around failures like this (known as \"convergence time\"). While we have made progress, efforts to improve still further remain ongoing. We continue to ensure that the current technology is optimally configured to minimize the frequency and severity of these issues.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\n**Detailed Description of Impact**\nCustomers may have observed packet loss for traffic routed via transpacific links on Google's backbone. This could include traffic from or to any of the following cloud regions:\n* asia-east1\n* asia-northeast1\n* asia-northeast2\n* asia-northeast3\n* asia-southeast1\n* asia-southeast2\n* australia-southeast1\n* australia-southeast2\n* us-west1\n* us-central1\n* us-east1\n* us-east4\n* northamerica-northeast1\n* europe-west1\n* europe-west2\n* europe-west3\n* europe-west4\n* europe-west6\n* europe-west8\n* europe-west9\n* europe-central2\n* europe-north1","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"}]},{"created":"2022-03-21T17:36:33+00:00","modified":"2022-03-23T16:13:24+00:00","when":"2022-03-21T17:36:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 18 March 2022 15:20\n**Incident End:** 18 March 2022 15:28\n**Duration:** 8 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking\n**Regions/Zones:**\nasia-east1, asia-northeast1, asia-northeast2, asia-northeast3, asia-southeast1, asia-southeast2,\naustrailia-southeast1, austrailia-southeast2\nus-west1, us-central1, us-east1, us-east4, northamerica-northeast1,\neurope-west1, europe-west2, europe-west3, europe-west4, europe-west6, europe-west8, europe-west9, europe-central2, europe-north1\n**Description:**\nGoogle Cloud Networking experienced intermittent packet loss for transit traffic in multiple cloud regions for 8 minutes. From preliminary analysis, the root cause is a hardware issue on a component of Google Cloud’s networking equipment. The issue was identified and mitigated automatically.\n**Customer Impact:**\nCustomers may have observed packet loss for transit traffic in the above mentioned cloud regions.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"}]},{"created":"2022-03-19T00:01:18+00:00","modified":"2022-03-23T16:13:04+00:00","when":"2022-03-19T00:01:18+00:00","text":"We experienced an issue with Cloud Networking beginning at Friday, 2022-03-18 15:20 US/Pacific.\nSelf-diagnosis:\nCustomers may have experienced up to 40% packet loss between VMs in the following affected regions:\nasia-east1 asia-northeast1 asia-northeast2 asia-northeast3 asia-southeast1 asia-southeast2 austrailia-southeast1 austrailia-southeast2 us-west1 us-central1 us-east1 us-east4 northamerica-northeast1 europe-west1 europe-west2 europe-west3 europe-west4 europe-west6 europe-west8 europe-west9 europe-southwest1 europe-central2 europe-north1\nThe issue has been resolved for all affected projects as of Friday, 2022-03-18 15:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"}]},{"created":"2022-03-18T23:53:02+00:00","modified":"2022-03-21T18:42:22+00:00","when":"2022-03-18T23:53:02+00:00","text":"Summary: Cloud Networking: Up to 40% packet loss between affected zones\nDescription: We are investigating a potential issue with Cloud Networking.\nWe will provide more information by Friday, 2022-03-18 17:25 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"}]}],"most_recent_update":{"created":"2022-03-29T03:49:06+00:00","modified":"2022-03-29T03:49:06+00:00","when":"2022-03-29T03:49:06+00:00","text":"**Summary**\nOn Friday, 18 March 2022 at 15:20 US/Pacific, Google Cloud Networking experienced intermittent packet loss for traffic between multiple cloud regions for a duration of 8 minutes. The issue was identified and mitigated automatically by 15:28 US/Pacific.\nWe understand this issue has affected our valued customers and users, and we apologize to those who were affected.\n**Root Cause**\nGoogle’s production backbone is a global network that enables connectivity for all user-facing traffic via Points of Presence (POPs) or internet exchanges.\nA rare hardware failure of a component on the fiber paths from one of the transpacific gateway campuses in Google’s production backbone led to a decrease in available network bandwidth between the gateway and multiple edge locations, causing packet loss.\n**Remediation and Prevention**\nGoogle’s automated repair mechanisms detected the decrease in available network bandwidth on Friday, 18 March 2022 at 15:20 US/Pacific and automatically routed the traffic through alternate links. The traffic rerouting completed on Friday, 18 March 2022 at 15:28 US/Pacific, mitigating the issue.\nWhile our automated mechanisms worked as intended and recovered the traffic without manual intervention, we understand that the scope of impact caused by this rare event affected our customers. We have been working on optimizing our global network to minimize the time spent automatically reconfiguring around failures like this (known as \"convergence time\"). While we have made progress, efforts to improve still further remain ongoing. We continue to ensure that the current technology is optimally configured to minimize the frequency and severity of these issues.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions. We appreciate your patience and apologize again for the impact to your organization. We thank you for your business.\n**Detailed Description of Impact**\nCustomers may have observed packet loss for traffic routed via transpacific links on Google's backbone. This could include traffic from or to any of the following cloud regions:\n* asia-east1\n* asia-northeast1\n* asia-northeast2\n* asia-northeast3\n* asia-southeast1\n* asia-southeast2\n* australia-southeast1\n* australia-southeast2\n* us-west1\n* us-central1\n* us-east1\n* us-east4\n* northamerica-northeast1\n* europe-west1\n* europe-west2\n* europe-west3\n* europe-west4\n* europe-west6\n* europe-west8\n* europe-west9\n* europe-central2\n* europe-north1","status":"AVAILABLE","affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"}]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/aA3kbJm5nwvVTKnYbrWM","currently_affected_locations":[],"previously_affected_locations":[{"title":"Taiwan (asia-east1)","id":"asia-east1"},{"title":"Tokyo (asia-northeast1)","id":"asia-northeast1"},{"title":"Osaka (asia-northeast2)","id":"asia-northeast2"},{"title":"Seoul (asia-northeast3)","id":"asia-northeast3"},{"title":"Singapore (asia-southeast1)","id":"asia-southeast1"},{"title":"Jakarta (asia-southeast2)","id":"asia-southeast2"},{"title":"Sydney (australia-southeast1)","id":"australia-southeast1"},{"title":"Melbourne (australia-southeast2)","id":"australia-southeast2"},{"title":"Warsaw (europe-central2)","id":"europe-central2"},{"title":"Finland (europe-north1)","id":"europe-north1"},{"title":"Belgium (europe-west1)","id":"europe-west1"},{"title":"London (europe-west2)","id":"europe-west2"},{"title":"Frankfurt (europe-west3)","id":"europe-west3"},{"title":"Netherlands (europe-west4)","id":"europe-west4"},{"title":"Zurich (europe-west6)","id":"europe-west6"},{"title":"Montréal (northamerica-northeast1)","id":"northamerica-northeast1"},{"title":"Iowa (us-central1)","id":"us-central1"},{"title":"South Carolina (us-east1)","id":"us-east1"},{"title":"Northern Virginia (us-east4)","id":"us-east4"},{"title":"Oregon (us-west1)","id":"us-west1"}]},{"id":"LuGcJVjNTeC5Sb9pSJ9o","number":"5384612291846020564","begin":"2022-03-08T18:07:00+00:00","created":"2022-03-08T20:36:16+00:00","end":"2022-03-08T20:42:00+00:00","modified":"2022-03-15T23:23:16+00:00","external_desc":"global: Elevated HTTP 500s errors for a small number of customers with load balancers on Traffic Director-managed backends","updates":[{"created":"2022-03-15T23:23:16+00:00","modified":"2022-03-15T23:23:16+00:00","when":"2022-03-15T23:23:16+00:00","text":"**INCIDENT REPORT**\n**Summary**\nOn Tuesday, 8 March 2022, Traffic Director users, who had service mesh with Traffic Director-managed backends, experienced elevated service errors for 2 hours and 35 minutes. To our Traffic Director customers who were impacted during this service disruption, we sincerely apologize. We have conducted an internal investigation and are taking steps to improve our service.\n**Root Cause**\nTraffic Director has a configuration pipeline that distributes customer configurations to Traffic Director infrastructure globally. The configuration pipeline has been undergoing a multi-stage, multi-year architectural rewrite to address a number of limitations in the previous architecture. One step to move to the new architecture has been to migrate existing Traffic Director configurations to a different format. This data migration has been ongoing since November 2021. The Traffic Director code was updated to handle both old and new formats of the configuration.\nThe data format migration was not fully completed; it encountered sporadic failures that were believed to only affect test configurations of new unreleased features. These failures masked the true completion state for full migration of all customer data. As a result, some customer data was an inconsistent state of old and new formats.\nOn 8 March 2022 at 10:05 PT, a change to the Traffic Director code that processes the configuration was updated. The code change assumed that the configuration data format migration was fully completed. In fact, the data migration had not completed. The failures were believed to only affect test configurations of new unreleased features, but they also affected a certain set of customer configurations that met all of the following criteria:\n1. The project was part of a Shared VPC.\n2. The project had at least one configuration migrated to the new format, which would be true if: a. The customer modified the project configuration after 12 November 2021; or b. The internal migration tool successfully processed the configuration.\n3. The project had at least one configuration that was not migrated to the new format: a. The customer had not modified the project since 12 November 2021; or b.The internal migration tool failed to process the configuration.\n4. The hostname of interest was part of the migrated configuration or the data plane requested configuration from the Traffic Director based on the migrated configuration.\nIt would inadvertently delete the configurations which caused the downstream clients to lose their programming and deconfigure the data plane.\nThe change was rolled back, the configurations were recovered, and service was restored for all users.\n**Remediation and Prevention**\nGoogle engineers were alerted to the issue through a customer support case on 8 March 2022 at 10:24 US/Pacific and immediately started an investigation.\nOnce the nature and scope of the issue became clear, Google engineers rolled back the Traffic Director configuration rollout to prevent additional customer impact.\nAt 12:42 US/Pacific, engineers forced a reprogramming of configurations, which mitigated the issue.\nWe sincerely apologize for the length and severity of this incident. Our team at Google is taking immediate steps to prevent a recurrence and improve reliability in the future.\n**Detailed Description of Impact**\nOn Tuesday, 8 March 2022 from 10:07 to 12:42 US/Pacific, customers using Traffic Director-managed backends experienced elevated service errors. Affected customers would have seen their Traffic Director managed clients deprogrammed as the configuration was removed. The effect of the deprogramming for users behind Google Cloud Load Balancers (GCLB) would have been visible as 500 errors. Some affected customers were able to mitigate the issue for themselves through the workaround of moving to backends that were not Traffic Director managed.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-03-09T17:25:35+00:00","modified":"2022-03-11T00:21:25+00:00","when":"2022-03-09T17:25:35+00:00","text":"Mini Incident Report (Full Incident Report To Follow)\nWe apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 08 March 2022 10:07\n**Incident End:** 08 March 2022 12:42\n**Duration:** 2 hours, 35 minutes\n**Affected Services and Features:**\nTraffic Director\n**Regions/Zones:** Global\n**Description:**\nTraffic Director customers who used shared Virtual Private Cloud (VPC) experienced elevated service errors for 2 hours and 35 minutes, due to local proxies, gRPC clients, and other services being unable to retrieve their xDS configurations. From preliminary analysis, the root cause of the issue was related to a bug introduced in a recent Traffic Director programming pipeline rollout.\n**Customer Impact:**\nAffected customers would have seen their Traffic Director-managed clients deprogrammed as the configuration was removed. The effect of the deprogramming for users behind GCLB would have been visible as 500 errors. Some affected customers were able to mitigate the issue for themselves through the workaround of moving to backends that were not Traffic Director-managed.\n**Additional details:**\nOur engineers rolled back the Traffic Director configuration rollout and forced a reprogramming of configurations, which mitigated the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-03-08T22:23:50+00:00","modified":"2022-03-08T22:23:57+00:00","when":"2022-03-08T22:23:50+00:00","text":"The issue with Traffic Director has been confirmed to be caused by a recent release; the release has been rolled back and customers can now start using Traffic Director. We have identified a probable root cause and will be publishing an Incident Report within the next several days.\nThe issue with Cloud Load Balancing, Cloud Networking, Traffic Director has been resolved for all affected projects as of Tuesday, 2022-03-08 12:42 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-03-08T21:21:27+00:00","modified":"2022-03-08T21:21:31+00:00","when":"2022-03-08T21:21:27+00:00","text":"Summary: global: Elevated HTTP 500s errors for a small number of customers with load balancers on Traffic Director-managed backends\nDescription: We believe the issue with Traffic Director is mitigated. We do not have an ETA for full resolution at this point. Customers should leave the workaround in place, as the issue could reoccur until the root cause has been determined.\nWe believe the issue is limited to Traffic Director users and does not affect all of Cloud Load Balancing or Cloud Networking.\nWe will provide an update by Tuesday, 2022-03-08 14:30 US/Pacific with current details.\nDiagnosis: Affected customers will see elevated HTTP 500s errors on load balancers with Traffic Director managed customer-run backends. Thus far, impact has only been observed in us-east1, but customers with multi-regional Traffic Director deployments could be impacted in more regions.\nWorkaround: Customers with Traffic Director managed backends should consider moving to backends that are not Traffic Director-managed.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-03-08T20:41:41+00:00","modified":"2022-03-08T20:41:44+00:00","when":"2022-03-08T20:41:41+00:00","text":"Summary: global: Elevated HTTP 500s errors for a small number of customers with load balancers on Traffic Director-managed Envoy backends\nDescription: We are experiencing an intermittent issue with Cloud Load Balancing, Cloud Networking, Traffic Director beginning at Tuesday, 2022-03-08 10:07:51 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-03-08 14:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers will see elevated HTTP 500s errors on load balancers with Traffic Director managed customer-run Envoy backends. Thus far, impact has only been observed in us-east1, but customers with multi-regional Traffic Director deployments could be impacted in more regions.\nWorkaround: Customers with Traffic Director managing their envoys should consider moving to backends that are not Traffic Director-managed.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-03-08T20:36:14+00:00","modified":"2022-03-08T20:36:18+00:00","when":"2022-03-08T20:36:14+00:00","text":"Summary: global: Elevated HTTP 500s errors for a small number of customers with load balancers on Traffic Director-managed Envoy backends\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point, but a customer self-mitigation is available.\nWe will provide more information by Tuesday, 2022-03-08 14:00 US/Pacific.\nDiagnosis: Affected customers will see elevated HTTP 500s errors on load balancers with Traffic Director managed customer-run Envoy backends. Thus far, impact has only been observed in us-east1, but customers with multi-regional Traffic Director deployments could be impacted in more regions.\nWorkaround: Customers with Traffic Director managing their envoys should consider moving to backends that are not Traffic Director-managed.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-03-15T23:23:16+00:00","modified":"2022-03-15T23:23:16+00:00","when":"2022-03-15T23:23:16+00:00","text":"**INCIDENT REPORT**\n**Summary**\nOn Tuesday, 8 March 2022, Traffic Director users, who had service mesh with Traffic Director-managed backends, experienced elevated service errors for 2 hours and 35 minutes. To our Traffic Director customers who were impacted during this service disruption, we sincerely apologize. We have conducted an internal investigation and are taking steps to improve our service.\n**Root Cause**\nTraffic Director has a configuration pipeline that distributes customer configurations to Traffic Director infrastructure globally. The configuration pipeline has been undergoing a multi-stage, multi-year architectural rewrite to address a number of limitations in the previous architecture. One step to move to the new architecture has been to migrate existing Traffic Director configurations to a different format. This data migration has been ongoing since November 2021. The Traffic Director code was updated to handle both old and new formats of the configuration.\nThe data format migration was not fully completed; it encountered sporadic failures that were believed to only affect test configurations of new unreleased features. These failures masked the true completion state for full migration of all customer data. As a result, some customer data was an inconsistent state of old and new formats.\nOn 8 March 2022 at 10:05 PT, a change to the Traffic Director code that processes the configuration was updated. The code change assumed that the configuration data format migration was fully completed. In fact, the data migration had not completed. The failures were believed to only affect test configurations of new unreleased features, but they also affected a certain set of customer configurations that met all of the following criteria:\n1. The project was part of a Shared VPC.\n2. The project had at least one configuration migrated to the new format, which would be true if: a. The customer modified the project configuration after 12 November 2021; or b. The internal migration tool successfully processed the configuration.\n3. The project had at least one configuration that was not migrated to the new format: a. The customer had not modified the project since 12 November 2021; or b.The internal migration tool failed to process the configuration.\n4. The hostname of interest was part of the migrated configuration or the data plane requested configuration from the Traffic Director based on the migrated configuration.\nIt would inadvertently delete the configurations which caused the downstream clients to lose their programming and deconfigure the data plane.\nThe change was rolled back, the configurations were recovered, and service was restored for all users.\n**Remediation and Prevention**\nGoogle engineers were alerted to the issue through a customer support case on 8 March 2022 at 10:24 US/Pacific and immediately started an investigation.\nOnce the nature and scope of the issue became clear, Google engineers rolled back the Traffic Director configuration rollout to prevent additional customer impact.\nAt 12:42 US/Pacific, engineers forced a reprogramming of configurations, which mitigated the issue.\nWe sincerely apologize for the length and severity of this incident. Our team at Google is taking immediate steps to prevent a recurrence and improve reliability in the future.\n**Detailed Description of Impact**\nOn Tuesday, 8 March 2022 from 10:07 to 12:42 US/Pacific, customers using Traffic Director-managed backends experienced elevated service errors. Affected customers would have seen their Traffic Director managed clients deprogrammed as the configuration was removed. The effect of the deprogramming for users behind Google Cloud Load Balancers (GCLB) would have been visible as 500 errors. Some affected customers were able to mitigate the issue for themselves through the workaround of moving to backends that were not Traffic Director managed.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"},{"title":"Traffic Director","id":"NroZwL2UMMionesUGP87"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"}],"uri":"incidents/LuGcJVjNTeC5Sb9pSJ9o","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"Hko5cWSXxGSsxfiSpg4n","number":"6491961050454270833","begin":"2022-02-22T05:45:00+00:00","created":"2022-02-25T17:57:41+00:00","end":"2022-02-25T20:22:00+00:00","modified":"2022-02-25T23:21:04+00:00","external_desc":"BigQuery S3 Data Transfer Service experiencing issues in us-multiregion, asia-noutheast1 and asia-southeast1","updates":[{"created":"2022-02-25T23:20:38+00:00","modified":"2022-02-25T23:20:38+00:00","when":"2022-02-25T23:20:38+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 21 February 2022 21:45\n**Incident End:** 25 February 2022 12:22\n**Duration:** 3 days, 14 hours, 37 minutes\n**Affected Services and Features:**\nGoogle BigQuery - Data Transfer Service\n**Regions/Zones:** Global\n**Description:**\nGoogle BigQuery S3 Data Transfer Service experienced table validation failures globally, with the majority of impact observed in us-multiregion, asia-northeast1, and asia-southeast1. From preliminary analysis, the root cause of the issue was a rollout that started on 21 February 2022 at 01:00 US/Pacific. A rollback was initiated on 25 February 2022 at 08:29 to mitigate the issue in the most heavily affected regions (us-multiregion, asia-northeast1, and asia-southeast1) which was completed on 25 February 2022 12:01.\n**Customer Impact:**\n* Transfer Runs for Google BigQuery S3 Data Transfer Service [1] will fail with an INVALID_ARGUMENT error if the following conditions are met: * The destination table in the Transfer Configuration contains a runtime parameter\n[2]. * The rollback has not completed in the given region. Regions where we identified impact have been prioritized and rolled back already.\n[1] - https://cloud.google.com/bigquery-transfer/docs/s3-transfer-intro\n[2] - https://cloud.google.com/bigquery-transfer/docs/s3-transfer-parameters\n**Additional Details:**\n* The rollback should complete across all regions by 01 March 2022.\n* Once the rollback completes, affected customers can retry the failed Transfer Runs to load any missed files.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-25T20:04:03+00:00","modified":"2022-02-25T20:04:05+00:00","when":"2022-02-25T20:04:03+00:00","text":"The issue with Google BigQuery has been resolved for all affected users as of Friday, 2022-02-25 12:03 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-25T17:57:41+00:00","modified":"2022-02-25T17:57:41+00:00","when":"2022-02-25T17:57:41+00:00","text":"Summary: BigQuery S3 Data Transfer Service experiencing issues in us-multiregion, asia-noutheast1 and asia-southeast1\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by the following dates/times:\nUS-Multiregion - Friday, 2022-02-25 12:00 US/Pacific.\nAsia-southeast1 and Asia-northeast1 - Friday, 2022-02-25 14:00 US/Pacific.\nOther regions potentially impacted - Tuesday, 2022-03-01 EOB US/Pacific.\nWe will provide more information by Friday, 2022-02-25 12:00 US/Pacific.\nDiagnosis: S3 transfers may fail table validation when table name contains template argument (e.g., \"{run_time}\")\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-25T23:20:38+00:00","modified":"2022-02-25T23:20:38+00:00","when":"2022-02-25T23:20:38+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 21 February 2022 21:45\n**Incident End:** 25 February 2022 12:22\n**Duration:** 3 days, 14 hours, 37 minutes\n**Affected Services and Features:**\nGoogle BigQuery - Data Transfer Service\n**Regions/Zones:** Global\n**Description:**\nGoogle BigQuery S3 Data Transfer Service experienced table validation failures globally, with the majority of impact observed in us-multiregion, asia-northeast1, and asia-southeast1. From preliminary analysis, the root cause of the issue was a rollout that started on 21 February 2022 at 01:00 US/Pacific. A rollback was initiated on 25 February 2022 at 08:29 to mitigate the issue in the most heavily affected regions (us-multiregion, asia-northeast1, and asia-southeast1) which was completed on 25 February 2022 12:01.\n**Customer Impact:**\n* Transfer Runs for Google BigQuery S3 Data Transfer Service [1] will fail with an INVALID_ARGUMENT error if the following conditions are met: * The destination table in the Transfer Configuration contains a runtime parameter\n[2]. * The rollback has not completed in the given region. Regions where we identified impact have been prioritized and rolled back already.\n[1] - https://cloud.google.com/bigquery-transfer/docs/s3-transfer-intro\n[2] - https://cloud.google.com/bigquery-transfer/docs/s3-transfer-parameters\n**Additional Details:**\n* The rollback should complete across all regions by 01 March 2022.\n* Once the rollback completes, affected customers can retry the failed Transfer Runs to load any missed files.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/Hko5cWSXxGSsxfiSpg4n","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"qfgJm8m4WPn2Ej2Z7vc2","number":"714206970471699851","begin":"2022-02-19T17:01:00+00:00","created":"2022-02-23T19:18:53+00:00","end":"2022-02-25T05:00:00+00:00","modified":"2022-02-25T18:42:09+00:00","external_desc":"Global: Requests to .NET Cloud Run applications fail if HTTP/2 or gRPC is used","updates":[{"created":"2022-02-25T18:41:39+00:00","modified":"2022-02-25T18:41:39+00:00","when":"2022-02-25T18:41:39+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 19 February 2022 09:01\n**Incident End:** 24 February 2022 20:48\n**Duration:** 5 days, 11 hours, 47 minutes\n**Affected Services and Features:**\nGoogle Cloud Run - .NET application failures using HTTP/2 or gRPC\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Run services using .NET HTTP/2 or gRPC experienced elevated errors. From preliminary analysis, the root cause of the issue is a change triggered by a recent rollout.\n**Customer Impact:**\nAffected .Net services observed \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request was made using HTTP/2 or gRPC.\n**Additional Details:**\nThe ability to disable \"preserve_downstream_scheme\" was removed upstream in Envoy proxy [1] which led to an unintentional breaking change in Cloud Run. The breaking change introduces an incompatibility with .NET default enforcement between incoming transport-layer security and “:scheme” header (see details [2] ). The Cloud Run and Envoy teams at Google were able to mitigate the issue once identified, and have taken steps to limit and better manage such changes in the future.\nThe issue was fully resolved on 24 February 2022 at 20:48 US/Pacific once a rollback of the change was completed.\n* [1] - https://www.envoyproxy.io/docs/envoy/latest/version_history/current#removed-config-or-runtime\n* [2] - https://github.com/dotnet/aspnetcore/issues/30532","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-25T05:00:47+00:00","modified":"2022-02-25T05:00:47+00:00","when":"2022-02-25T05:00:47+00:00","text":"The issue with Cloud Run has been resolved for all affected users as of Thursday, 2022-02-24 20:57 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-25T02:58:15+00:00","modified":"2022-02-25T02:58:16+00:00","when":"2022-02-25T02:58:15+00:00","text":"Summary: Global: Requests to .NET Cloud Run applications fail if HTTP/2 or gRPC is used\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-02-24 US/Pacific.\nWe will provide more information by Thursday, 2022-02-24 22:30 US/Pacific.\nDiagnosis: Affected customers will see \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request is made using HTTP/2 or gRPC.\nWorkaround: If using .NET 6, settings KestrelServerOptions.AllowAlternateSchemes to true will avoid the issue:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.server.kestrel.core.kestrelserveroptions.allowalternateschemes?view=aspnetcore-6.0","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-25T00:56:56+00:00","modified":"2022-02-25T00:56:57+00:00","when":"2022-02-25T00:56:56+00:00","text":"Summary: Global: Requests to .NET Cloud Run applications fail if HTTP/2 or gRPC is used\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-02-24 US/Pacific.\nWe will provide more information by Thursday, 2022-02-24 21:00 US/Pacific.\nDiagnosis: Affected customers will see \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request is made using HTTP/2 or gRPC.\nWorkaround: If using .NET 6, settings KestrelServerOptions.AllowAlternateSchemes to true will avoid the issue:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.server.kestrel.core.kestrelserveroptions.allowalternateschemes?view=aspnetcore-6.0","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-24T21:06:18+00:00","modified":"2022-02-24T21:06:19+00:00","when":"2022-02-24T21:06:18+00:00","text":"Summary: Global: Requests to .NET Cloud Run applications fail if HTTP/2 or gRPC is used\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-02-24 US/Pacific.\nWe will provide more information by Thursday, 2022-02-24 18:00 US/Pacific.\nDiagnosis: Affected customers will see \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request is made using HTTP/2 or gRPC.\nWorkaround: If using .NET 6, settings KestrelServerOptions.AllowAlternateSchemes to true will avoid the issue:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.server.kestrel.core.kestrelserveroptions.allowalternateschemes?view=aspnetcore-6.0","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-23T21:46:33+00:00","modified":"2022-02-23T21:46:35+00:00","when":"2022-02-23T21:46:33+00:00","text":"Summary: Global: Requests to .NET Cloud Run applications fail if HTTP/2 or gRPC is used\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2022-02-24 US/Pacific.\nWe will provide more information by Thursday, 2022-02-24 14:00 US/Pacific.\nDiagnosis: Affected customers will see \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request is made using HTTP/2 or gRPC.\nWorkaround: If using .NET 6, settings KestrelServerOptions.AllowAlternateSchemes to true will avoid the issue:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.server.kestrel.core.kestrelserveroptions.allowalternateschemes?view=aspnetcore-6.0","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-23T19:18:53+00:00","modified":"2022-02-23T19:18:55+00:00","when":"2022-02-23T19:18:53+00:00","text":"Summary: Global: Requests to .NET Cloud Run applications fail if HTTP/2 or gRPC is used\nDescription: We are experiencing an issue with Cloud Run beginning at Thursday, 2022-02-10 18:50 US/Pacific.\nMitigation work is currently underway by our engineering team. We do not have an ETA for mitigation at this point.\nWe apologize to all who are affected by the disruption.\nWe will provide an update by Wednesday, 2022-02-23 15:30 US/Pacific with current details.\nDiagnosis: Affected customers will see \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request is made using HTTP/2 or gRPC.\nWorkaround: If using .NET 6, settings KestrelServerOptions.AllowAlternateSchemes to true will avoid the issue:\nhttps://docs.microsoft.com/en-us/dotnet/api/microsoft.aspnetcore.server.kestrel.core.kestrelserveroptions.allowalternateschemes?view=aspnetcore-6.0","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-25T18:41:39+00:00","modified":"2022-02-25T18:41:39+00:00","when":"2022-02-25T18:41:39+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 19 February 2022 09:01\n**Incident End:** 24 February 2022 20:48\n**Duration:** 5 days, 11 hours, 47 minutes\n**Affected Services and Features:**\nGoogle Cloud Run - .NET application failures using HTTP/2 or gRPC\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Run services using .NET HTTP/2 or gRPC experienced elevated errors. From preliminary analysis, the root cause of the issue is a change triggered by a recent rollout.\n**Customer Impact:**\nAffected .Net services observed \"upstream connect error or disconnect/reset before headers. reset reason: remote reset\" when a request was made using HTTP/2 or gRPC.\n**Additional Details:**\nThe ability to disable \"preserve_downstream_scheme\" was removed upstream in Envoy proxy [1] which led to an unintentional breaking change in Cloud Run. The breaking change introduces an incompatibility with .NET default enforcement between incoming transport-layer security and “:scheme” header (see details [2] ). The Cloud Run and Envoy teams at Google were able to mitigate the issue once identified, and have taken steps to limit and better manage such changes in the future.\nThe issue was fully resolved on 24 February 2022 at 20:48 US/Pacific once a rollback of the change was completed.\n* [1] - https://www.envoyproxy.io/docs/envoy/latest/version_history/current#removed-config-or-runtime\n* [2] - https://github.com/dotnet/aspnetcore/issues/30532","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9D7d2iNBQWN24zc1VamE","service_name":"Cloud Run","affected_products":[{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"}],"uri":"incidents/qfgJm8m4WPn2Ej2Z7vc2","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"MqpDy4VdUKuQkw1GXPTo","number":"17820096163428229993","begin":"2022-02-15T17:14:00+00:00","created":"2022-02-15T17:49:25+00:00","end":"2022-02-19T18:46:00+00:00","modified":"2022-02-24T02:13:58+00:00","external_desc":"Global: Cloud SQL PostgreSQL password validation errors with high connection load.","updates":[{"created":"2022-02-24T02:10:59+00:00","modified":"2022-02-24T02:13:29+00:00","when":"2022-02-24T02:10:59+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 15 February 2022 09:14\n**Incident End:** 19 February 2022 10:46\n**Duration:** 4 days, 1 hour, 32 minutes\n**Affected Services and Features:**\nCloudSQL- PostgreSQL connection errors\n**Regions/Zones:** Global\n**Description:**\nApplications using close to maximum number of connections, intermittently failed to connect to database instances. The connection error happened because Cloud SQL for PostgreSQL was using additional connections for internal use.\n**Customer Impact:**\nCustomers impacted by this issue would have seen errors when establishing new connections to Cloud SQL for PostgreSQL instances. The error message would have been: FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already :: proc.c:347\n**Additional details:**\nThe issue was fully resolved on 19 February 2022 at 10:46 US/Pacific. In addition, the following prevention actions were completed:\n* For impacted instances, Cloud SQL has disabled the use of additional connections.\n* The maintenance rollout was canceled to prevent further instances from being affected.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-22T22:45:09+00:00","modified":"2022-02-22T22:45:11+00:00","when":"2022-02-22T22:45:09+00:00","text":"The issue with Cloud SQL has been resolved for all affected instances as of Tuesday, 2022-02-22 14:30 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-22T19:22:35+00:00","modified":"2022-02-22T19:22:41+00:00","when":"2022-02-22T19:22:35+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load.\nDescription: Engineering team is currently working on mitigating the remainder of the affected instances (ETA EOB Feb 22, 2022).\nAffected customer instances have been mitigated and will not observe the password validation errors. If you are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe will provide an update by Tuesday, 2022-02-22 19:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections to instances with high connection load - \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-18T19:42:10+00:00","modified":"2022-02-18T19:42:17+00:00","when":"2022-02-18T19:42:10+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load.\nDescription: Engineering team is currently working on mitigating the non-HA instances (ETA Feb 22, 2022). The mitigation efforts are still underway for HA instances which continue to be affected.\nAffected customer instances have been mitigated and will not observe the password validation errors. If you are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe will provide an update by Tuesday, 2022-02-22 12:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections to instances with high connection load - \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-17T22:31:16+00:00","modified":"2022-02-17T22:31:22+00:00","when":"2022-02-17T22:31:16+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load.\nDescription: Engineering team continues to work on deploying the fix.\nAffected customer instances have been mitigated and will not observe the password validation errors. If you are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe will provide an update by Friday, 2022-02-18 12:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections to instances with high connection load - \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-17T19:30:36+00:00","modified":"2022-02-17T19:30:43+00:00","when":"2022-02-17T19:30:36+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load.\nDescription: Engineering team continues to work on deploying the fix.\nAffected customer instances have been mitigated and will not observe the password validation errors.\nWe will provide an update by Thursday, 2022-02-17 17:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-16T21:08:47+00:00","modified":"2022-02-16T21:08:49+00:00","when":"2022-02-16T21:08:47+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load.\nDescription: Affected customer instances have been mitigated and will not observe the password validation errors.\nEngineering team is currently working on rolling out a fix. ETA for rollout is unknown at this time.\nWe will provide an update by Thursday, 2022-02-17 12:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-16T18:11:14+00:00","modified":"2022-02-16T18:11:16+00:00","when":"2022-02-16T18:11:14+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load\nDescription: Engineering team is currently working on mitigation of the issue.\nWe will provide an update by Wednesday, 2022-02-16 14:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-16T14:22:56+00:00","modified":"2022-02-16T14:23:05+00:00","when":"2022-02-16T14:22:56+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load\nDescription: We believe the issue with FEATURE is partially resolved. We are currently performing checks to see if any instances have been missed.\nFull resolution is expected to complete by Wednesday, 2022-02-16 10:00 US/Pacific.\nWe will provide an update by Wednesday, 2022-02-16 10:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-16T03:28:30+00:00","modified":"2022-02-16T03:28:31+00:00","when":"2022-02-16T03:28:30+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load\nDescription: We believe the issue with Cloud SQL is partially resolved.\nFull resolution is expected to complete by Wednesday, 2022-02-16 06:30 US/Pacific.\nWe will provide an update by Wednesday, 2022-02-16 06:30 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-15T23:18:57+00:00","modified":"2022-02-15T23:18:59+00:00","when":"2022-02-15T23:18:57+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load\nDescription: Our engineering team continues to work on the mitigation of the issue.\nWe will provide more information by Tuesday, 2022-02-15 19:30 US/Pacific.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection load.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-15T20:44:48+00:00","modified":"2022-02-15T20:44:50+00:00","when":"2022-02-15T20:44:48+00:00","text":"Summary: Global: Cloud SQL PostgreSQL password validation errors with high connection load\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-02-15 15:30 US/Pacific.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection rate.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-15T19:26:59+00:00","modified":"2022-02-15T19:27:02+00:00","when":"2022-02-15T19:26:59+00:00","text":"Summary: Global: Cloud SQL Postgres instances experiences password validation error on some connections.\nDescription: We are experiencing an issue with Cloud SQL beginning at Tuesday, 2022-01-26 11:24 US/Pacific.\nOur engineering team continues to investigate the issue, and after further investigation it was determined that the trigger conditions are not actually due to \u003e50 QPS.\nWe will provide an update by Tuesday, 2022-02-15 13:00 US/Pacific with current details.\nDiagnosis: Affected customers may see a password validation error message when establishing new connections: \"FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already...\".\nWorkaround: Users can enable connection pooling to reduce the connection rate.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-15T17:49:24+00:00","modified":"2022-02-15T17:49:26+00:00","when":"2022-02-15T17:49:24+00:00","text":"Summary: Global: Some Cloud SQL Postgres instances have the following flag set and are unable to disable: cloudsql.enable_password_validation=on\nDescription: All Postgres instances connections have been rate-limited to 50/sec to prevent triggering the issue.\nCustomers may consider using the workaround to pool connections to avoid hitting the temporarily rate limit.\nWe will provide more information by Tuesday, 2022-02-15 12:00 US/Pacific.\nDiagnosis: Affected postgres instances from a recent release have the following flag set and are unable to remove or disable this flag: cloudsql.enable_password_validation=on. This flag does not appear in Cloud Console, and attempting to disable flag via gcloud returns error where the flag is not recognized or supported. Password validation occurs on every new client connection but is limited to 50 QPS, and thus higher rates will return errors.\nWorkaround: Users can enable connection pooling to avoid connecting more than 50 times a second.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-24T02:10:59+00:00","modified":"2022-02-24T02:13:29+00:00","when":"2022-02-24T02:10:59+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 15 February 2022 09:14\n**Incident End:** 19 February 2022 10:46\n**Duration:** 4 days, 1 hour, 32 minutes\n**Affected Services and Features:**\nCloudSQL- PostgreSQL connection errors\n**Regions/Zones:** Global\n**Description:**\nApplications using close to maximum number of connections, intermittently failed to connect to database instances. The connection error happened because Cloud SQL for PostgreSQL was using additional connections for internal use.\n**Customer Impact:**\nCustomers impacted by this issue would have seen errors when establishing new connections to Cloud SQL for PostgreSQL instances. The error message would have been: FATAL: pwd_validation: Failed to connect to admin database, error: FATAL: sorry, too many clients already :: proc.c:347\n**Additional details:**\nThe issue was fully resolved on 19 February 2022 at 10:46 US/Pacific. In addition, the following prevention actions were completed:\n* For impacted instances, Cloud SQL has disabled the use of additional connections.\n* The maintenance rollout was canceled to prevent further instances from being affected.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"hV87iK5DcEXKgWU2kDri","service_name":"Google Cloud SQL","affected_products":[{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"}],"uri":"incidents/MqpDy4VdUKuQkw1GXPTo","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"oAxmtvN5RuKAYHMG4ToR","number":"17366090916923453747","begin":"2022-02-15T13:41:00+00:00","created":"2022-02-17T13:42:31+00:00","end":"2022-02-15T17:07:00+00:00","modified":"2022-02-17T13:45:08+00:00","external_desc":"We experienced an intermittent issue with Cloud Networking.","updates":[{"created":"2022-02-17T13:44:45+00:00","modified":"2022-02-17T13:44:45+00:00","when":"2022-02-17T13:44:45+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 15 February 2022 05:41\n**Incident End:** 15 February 2022 09:07\n**Duration:** 3 hours, 26 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking - Global External HTTPS load balancer SSL certificates\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Networking experienced issues with global external HTTPS load balancer SSL certificate configuration updates for a duration of 3 hours, 26 minutes. A configuration change caused a large amount of data to be unintentionally rewritten, and the component responsible for processing the updated data was overwhelmed.\n**Customer Impact:**\nCustomers experienced delays with initial provisioning of managed SSL certificates.\nCustomers experienced issues with programming of newly uploaded, updated, or deleted SSL certificates that were not applied to the dataplane.\n**Additional details:**\nThe issue was fully resolved on 15 February 2022 at 09:07 US/Pacific after:\nOperators modified the configuration push system to handle the increase in data volume.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-17T13:42:32+00:00","modified":"2022-02-17T13:42:32+00:00","when":"2022-02-17T13:42:32+00:00","text":"We experienced an intermittent issue with Cloud Networking beginning at Tuesday, 2022-02-15 05:41 US/Pacific.\nDiagnosis: Affected customers may have experienced delays when provisioning new or updated certificates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-17T13:44:45+00:00","modified":"2022-02-17T13:44:45+00:00","when":"2022-02-17T13:44:45+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 15 February 2022 05:41\n**Incident End:** 15 February 2022 09:07\n**Duration:** 3 hours, 26 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking - Global External HTTPS load balancer SSL certificates\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Networking experienced issues with global external HTTPS load balancer SSL certificate configuration updates for a duration of 3 hours, 26 minutes. A configuration change caused a large amount of data to be unintentionally rewritten, and the component responsible for processing the updated data was overwhelmed.\n**Customer Impact:**\nCustomers experienced delays with initial provisioning of managed SSL certificates.\nCustomers experienced issues with programming of newly uploaded, updated, or deleted SSL certificates that were not applied to the dataplane.\n**Additional details:**\nThe issue was fully resolved on 15 February 2022 at 09:07 US/Pacific after:\nOperators modified the configuration push system to handle the increase in data volume.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/oAxmtvN5RuKAYHMG4ToR","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"xtW5y29FeYgjfLW3AwKY","number":"12290226274247929876","begin":"2022-02-11T20:00:00+00:00","created":"2022-02-11T23:18:10+00:00","end":"2022-02-12T02:26:00+00:00","modified":"2022-02-15T15:27:38+00:00","external_desc":"Global: Delays provisioning SSL certificates for HTTPS load balancers","updates":[{"created":"2022-02-15T04:08:06+00:00","modified":"2022-02-15T15:27:38+00:00","when":"2022-02-15T04:08:06+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 11 February 2022 12:00\n**Incident End:** 11 February 2022 18:26\n**Duration:** 6 hours, 26 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking - Global External HTTPS load balancer SSL certificates\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Networking experienced issues with global external HTTPS load balancer SSL certificate configuration updates for a duration of 6 hours, 26 minutes. This was triggered by a component rollback, which tripped a safety check that prevented changes from taking effect. The backlog required further intervention from operators to clear; when it did the issue was mitigated.\n**Customer Impact:**\n- Customers experienced delays with initial provisioning of managed SSL certificates.\n- Customers experienced issues with programming of newly uploaded, updated, or deleted SSL certificates that were not applied to the dataplane.\n**Additional details:**\nThe issue was fully resolved on 11 February 2022 at 18:26 US/Pacific after:\nModifying the safety check.\nSupervising the pipeline until it successfully completed, removing the delta and resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-12T02:32:18+00:00","modified":"2022-02-12T02:32:21+00:00","when":"2022-02-12T02:32:18+00:00","text":"The issue with Cloud Networking has been resolved for all affected users as of Friday, 2022-02-11 18:26 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-12T01:54:11+00:00","modified":"2022-02-12T01:54:13+00:00","when":"2022-02-12T01:54:11+00:00","text":"Summary: Global: Delays provisioning SSL certificates for HTTPS load balancers\nDescription: Mitigation work is still underway by our engineering team. A configuration change is rolling out as a potential fix.\nWe will provide more information by Friday, 2022-02-11 20:30 US/Pacific.\nDiagnosis: Affected customers may experience delays when provisioning new or updated certificates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-11T23:18:04+00:00","modified":"2022-02-11T23:18:11+00:00","when":"2022-02-11T23:18:04+00:00","text":"Summary: Global: Delays provisioning SSL certificates for HTTPS load balancers\nDescription: We are experiencing an intermittent issue with Cloud Networking beginning at Friday, 2022-02-11 12:00 US/Pacific.\nMitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point. We apologize to all who are affected by the disruption\nWe will provide more information by Friday, 2022-02-11 18:00 US/Pacific.\nDiagnosis: Affected customers may experience delays when provisioning new or updated certificates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-15T04:08:06+00:00","modified":"2022-02-15T15:27:38+00:00","when":"2022-02-15T04:08:06+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 11 February 2022 12:00\n**Incident End:** 11 February 2022 18:26\n**Duration:** 6 hours, 26 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking - Global External HTTPS load balancer SSL certificates\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Networking experienced issues with global external HTTPS load balancer SSL certificate configuration updates for a duration of 6 hours, 26 minutes. This was triggered by a component rollback, which tripped a safety check that prevented changes from taking effect. The backlog required further intervention from operators to clear; when it did the issue was mitigated.\n**Customer Impact:**\n- Customers experienced delays with initial provisioning of managed SSL certificates.\n- Customers experienced issues with programming of newly uploaded, updated, or deleted SSL certificates that were not applied to the dataplane.\n**Additional details:**\nThe issue was fully resolved on 11 February 2022 at 18:26 US/Pacific after:\nModifying the safety check.\nSupervising the pipeline until it successfully completed, removing the delta and resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/xtW5y29FeYgjfLW3AwKY","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"46bP9UWxaLXHyCUQdjPR","number":"6197302924433084519","begin":"2022-02-10T08:32:00+00:00","created":"2022-02-10T10:01:11+00:00","end":"2022-02-10T14:34:00+00:00","modified":"2022-02-10T22:01:44+00:00","external_desc":"Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST","updates":[{"created":"2022-02-10T22:01:11+00:00","modified":"2022-02-10T22:01:11+00:00","when":"2022-02-10T22:01:11+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 10 February 2022 00:32\n**Incident End:** 10 February 2022 06:34\n**Duration:** 6 hours, 32 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking, Google App Engine, Google Cloud Storage, Google Compute Engine\n**Regions/Zones:** asia-south1\n**Description:**\nGoogle Cloud Networking experienced elevated latency in the asia-south1 region for 6 hours and 32 minutes. From preliminary analysis, the root cause of the issue was increased memory utilization related to a query workload.\n**Customer Impact:**\nCustomers may have experienced elevated latency in the asia-south1 region between Google Cloud Load Balancers (GCLB) and downstream services including Google App Engine, Google Cloud Storage, and Google Compute Engine.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-10T14:18:36+00:00","modified":"2022-02-10T14:18:37+00:00","when":"2022-02-10T14:18:36+00:00","text":"The issue with Google App Engine, Google Cloud Storage, Cloud Networking, Google Compute Engine has been resolved for all affected users as of Thursday, 2022-02-10 06:18 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-10T11:58:16+00:00","modified":"2022-02-10T11:58:17+00:00","when":"2022-02-10T11:58:16+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2022-02-10 06:30 US/Pacific.\nDiagnosis: Customer may see - increased latency.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T11:27:42+00:00","modified":"2022-02-10T11:27:42+00:00","when":"2022-02-10T11:27:42+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:32 US/Pacific\nOur engineering team is narrowing down the root cause and continues to investigate. Customer may see various error in the APAC region.\nWe will provide an update by Thursday, 2022-02-10 04:00 US/Pacific with current details.\nDiagnosis: Customer may see - Time outs, 502 error, and slow response times, increased latency.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T11:05:27+00:00","modified":"2022-02-10T11:05:28+00:00","when":"2022-02-10T11:05:27+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:32 US/Pacific\nOur engineering team is narrowing down the root cause and continues to investigate. Customer may see various error in the APAC region.\nWe will provide an update by Thursday, 2022-02-10 03:30 US/Pacific with current details.\nDiagnosis: Customer may see - Time outs, 502 error, and slow response times, increased latency.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T10:59:22+00:00","modified":"2022-02-10T10:59:23+00:00","when":"2022-02-10T10:59:22+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:32 US/Pacific\nOur engineering team continues to investigate the issue. Customer may see various error in the APAC region\nWe will provide an update by Thursday, 2022-02-10 03:30 US/Pacific with current details.\nDiagnosis: Customer may see - Time outs, 502 error, and slow response times\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T10:26:08+00:00","modified":"2022-02-10T10:26:08+00:00","when":"2022-02-10T10:26:08+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:32 US/Pacific\nOur engineering team continues to investigate the issue. Customer may see various error in the APAC region\nWe will provide an update by Thursday, 2022-02-10 03:00 US/Pacific with current details.\nDiagnosis: Customer may see - Time outs, 502 error, and slow response times\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T10:15:42+00:00","modified":"2022-02-10T10:15:43+00:00","when":"2022-02-10T10:15:42+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking, starting at 2022-02-10 00:32:56 PST\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:32 US/Pacific\nOur engineering team continues to investigate the issue. Customer maybe see various error in the APAC region\nWe will provide an update by Thursday, 2022-02-10 03:00 US/Pacific with current details.\nDiagnosis: Customer may see - Time outs, 502 error, and slow response times\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T10:06:18+00:00","modified":"2022-02-10T10:06:19+00:00","when":"2022-02-10T10:06:18+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking,\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:00 US/Pacific\nOur engineering team continues to investigate the issue. Customer maybe see various error in the APAC region\nWe will provide an update by Thursday, 2022-02-10 02:30 US/Pacific with current details.\nDiagnosis: Customer may see - Time outs, 502 error, and slow response times\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T10:03:28+00:00","modified":"2022-02-10T10:03:29+00:00","when":"2022-02-10T10:03:28+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking,\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:00 US/Pacific\nOur engineering team continues to investigate the issue. Customer maybe see various error in the APAC region\nWe will provide an update by Thursday, 2022-02-10 02:30 US/Pacific with current details.\nDiagnosis: Customer may see issue with APP engine, GCS, GCLP - Time outs, 502 error, and slow response times\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T10:01:05+00:00","modified":"2022-02-10T10:01:11+00:00","when":"2022-02-10T10:01:05+00:00","text":"Summary: Google engineering are investigating issues with Cloud Networking,\nDescription: We are experiencing an issue with Cloud Networking Thursday, 2022-02-10 00:00 US/Pacific\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-02-10 02:30 US/Pacific with current details.\nDiagnosis: Customer may see with APP engine, GCS, GCLP - Time out, 502 error, and slow response\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-10T22:01:11+00:00","modified":"2022-02-10T22:01:11+00:00","when":"2022-02-10T22:01:11+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 10 February 2022 00:32\n**Incident End:** 10 February 2022 06:34\n**Duration:** 6 hours, 32 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking, Google App Engine, Google Cloud Storage, Google Compute Engine\n**Regions/Zones:** asia-south1\n**Description:**\nGoogle Cloud Networking experienced elevated latency in the asia-south1 region for 6 hours and 32 minutes. From preliminary analysis, the root cause of the issue was increased memory utilization related to a query workload.\n**Customer Impact:**\nCustomers may have experienced elevated latency in the asia-south1 region between Google Cloud Load Balancers (GCLB) and downstream services including Google App Engine, Google Cloud Storage, and Google Compute Engine.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"}],"uri":"incidents/46bP9UWxaLXHyCUQdjPR","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"m6gcTj6EdTDt7oLjR5da","number":"12694478571410041419","begin":"2022-02-10T06:00:00+00:00","created":"2022-02-10T11:34:42+00:00","end":"2022-02-10T12:05:00+00:00","modified":"2022-02-11T17:00:09+00:00","external_desc":"Contact Support Widget is down","updates":[{"created":"2022-02-11T17:00:05+00:00","modified":"2022-02-11T17:00:05+00:00","when":"2022-02-11T17:00:05+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 09 February 2022 22:00\n**Incident End:** 10 February 2022 04:05\n**Duration:** 6 hours, 5 minutes\n**Affected Services and Features:**\nGoogle Cloud Console - Support Widget\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Contact Support widgets experienced elevated errors and were unavailable in the Cloud Console for 6 hours, 5 minutes. From preliminary analysis, the root cause of the issue is due to a rollout of the UI server.\n**Customer Impact:**\nGoogle Cloud Platform customers were unable to create billing support cases via the Support Widget in the Google Cloud Console.\n**Additional details:**\nThe issue was fully resolved on 10 February 2022 at 04:05 US/Pacific after a rollback of the change was completed.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-10T12:04:19+00:00","modified":"2022-02-10T12:04:20+00:00","when":"2022-02-10T12:04:19+00:00","text":"The issue with Cloud Console has been resolved for all affected users as of Thursday, 2022-02-10 04:03 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-10T11:36:56+00:00","modified":"2022-02-10T11:36:56+00:00","when":"2022-02-10T11:36:56+00:00","text":"Summary: Contact Support Widget is down\nDescription: We are experiencing an issue with the Contact Support Widget for GCP billing support not loading for users.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-02-10 04:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customer will be unable to access billing support widget\nWorkaround: Customer can navigate to https://apps.google.com/supportwidget/helphome?product_name=Cloud+Platform for direct billing support.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-10T11:34:42+00:00","modified":"2022-02-10T11:34:43+00:00","when":"2022-02-10T11:34:42+00:00","text":"Summary: Contact Support Widget is down\nDescription: We are experiencing an issue with the Contact Support Widget for GCP billing support not loading for users.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2022-02-10 04:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customer will be unable to access billing support widget\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-11T17:00:05+00:00","modified":"2022-02-11T17:00:05+00:00","when":"2022-02-11T17:00:05+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 09 February 2022 22:00\n**Incident End:** 10 February 2022 04:05\n**Duration:** 6 hours, 5 minutes\n**Affected Services and Features:**\nGoogle Cloud Console - Support Widget\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Contact Support widgets experienced elevated errors and were unavailable in the Cloud Console for 6 hours, 5 minutes. From preliminary analysis, the root cause of the issue is due to a rollout of the UI server.\n**Customer Impact:**\nGoogle Cloud Platform customers were unable to create billing support cases via the Support Widget in the Google Cloud Console.\n**Additional details:**\nThe issue was fully resolved on 10 February 2022 at 04:05 US/Pacific after a rollback of the change was completed.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Support","id":"bGThzF7oEGP5jcuDdMuk"}],"uri":"incidents/m6gcTj6EdTDt7oLjR5da","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"bRpbkj2tcGDGQ6j4d8Uv","number":"12173750429148254762","begin":"2022-02-07T11:42:00+00:00","created":"2022-02-07T12:00:13+00:00","end":"2022-02-07T12:20:00+00:00","modified":"2022-05-26T19:46:01+00:00","external_desc":"Cloud Interconnect can see packet loss for users accessing VMs and Google Services","updates":[{"created":"2022-02-07T12:20:06+00:00","modified":"2022-02-07T12:20:10+00:00","when":"2022-02-07T12:20:06+00:00","text":"This incident with Cloud Interconnect was initially triggered by our internal monitoring systems.\nUpon further investigation, our engineering teams believe that the scope is very limited and/or no customers were impacted.\nIf you have questions or feel that you may be impacted, please open a case with the Support Team and we will work with you until the issue is resolved. No further updates will be provided here.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-07T12:00:10+00:00","modified":"2022-02-07T12:00:15+00:00","when":"2022-02-07T12:00:10+00:00","text":"Summary: Cloud Interconnect can see packet loss for users accessing VMs and Google Services\nDescription: We are investigating a potential issue with Cloud Interconnect.\nWe will provide more information by Monday, 2022-02-07 05:00 US/Pacific.\nDiagnosis: Potential packet loss over interconnects\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-07T12:20:06+00:00","modified":"2022-02-07T12:20:10+00:00","when":"2022-02-07T12:20:06+00:00","text":"This incident with Cloud Interconnect was initially triggered by our internal monitoring systems.\nUpon further investigation, our engineering teams believe that the scope is very limited and/or no customers were impacted.\nIf you have questions or feel that you may be impacted, please open a case with the Support Team and we will work with you until the issue is resolved. No further updates will be provided here.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/bRpbkj2tcGDGQ6j4d8Uv","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"1cPiGbWDMYHFMcTaLADd","number":"16268067613416205109","begin":"2022-02-02T20:00:00+00:00","created":"2022-02-08T19:22:23+00:00","end":"2022-02-09T13:48:00+00:00","modified":"2022-02-10T23:13:01+00:00","external_desc":"Global: Cloud build unable to create github triggers","updates":[{"created":"2022-02-10T23:12:33+00:00","modified":"2022-02-10T23:12:33+00:00","when":"2022-02-10T23:12:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 02 February 2022 12:00\n**Incident End:** 09 February 2022 05:48\n**Duration:** 6 days, 17 hours, 48 minutes\n**Affected Services and Features:**\nCloud Build\n**Regions/Zones:** Global\n**Description:**\nCreating Cloud Build GitHub triggers in the Cloud Console failed when the URL of the page ended with a project ID as the last argument (e.g. https://console.cloud.google.com/cloud-build/triggers/add?project=project-id-1234) Creation of triggers via gcloud was not impacted. From preliminary analysis, the root cause of the issue was that the Cloud Console began sending resource names in a format Cloud Build could not parse.\n**Customer Impact:**\nCustomers attempting to create GitHub triggers when using a Cloud Console URL that ended in their project ID would have experienced pages that displayed “Failed to load” errors.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-09T14:07:35+00:00","modified":"2022-02-09T14:07:39+00:00","when":"2022-02-09T14:07:35+00:00","text":"The issue with Cloud Build has been resolved for all affected projects as of Wednesday, 2022-02-09 05:48 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-02-08T23:16:39+00:00","modified":"2022-02-08T23:16:46+00:00","when":"2022-02-08T23:16:39+00:00","text":"Summary: Global: Cloud build unable to create github triggers\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2022-02-09 07:00 US/Pacific.\nDiagnosis: Customers attempting to create github triggers using a Cloud Console URL that ends in their project id (example: https://console.cloud.google.com/cloud-build/triggers/add?project=$project-id) will see a \"Failed to load\" error with following message:\n\"There was an error while loading /cloud-build/triggers?project=\nPlease try again\"\nWorkaround: Modify the URL in the browser to end with the project number instead of project id\ne.g. https://console.cloud.google.com/cloud-build/triggers/add?project=$project-num","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-08T20:13:20+00:00","modified":"2022-02-08T20:13:21+00:00","when":"2022-02-08T20:13:20+00:00","text":"Summary: Global: Cloud build unable to create github triggers\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2022-02-08 15:00 US/Pacific.\nDiagnosis: Customers attempting to create github triggers using a Cloud Console URL that ends in their project id (example: https://console.cloud.google.com/cloud-build/triggers/add?project=$project-id) will see a \"Failed to load\" error with following message:\n\"There was an error while loading /cloud-build/triggers?project=\nPlease try again\"\nWorkaround: Modify the URL in the browser to end with the project number instead of project id\ne.g. https://console.cloud.google.com/cloud-build/triggers/add?project=$project-num","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-08T19:56:33+00:00","modified":"2022-02-08T19:56:34+00:00","when":"2022-02-08T19:56:33+00:00","text":"Summary: Global: Cloud build unable to create github triggers\nDescription: We are experiencing an issue with Cloud Build beginning at Wednesday, 2022-02-02 12:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-02-08 13:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers will see a \"Failed to load\" error with following message:\n\"There was an error while loading /cloud-build/triggers?project=\nPlease try again\"\nWorkaround: Modify the URL in the browser to end with the project number\ne.g. https://console.cloud.google.com/cloud-build/triggers/add?project=$project-id ---\u003e https://console.cloud.google.com/cloud-build/triggers/add?project=$project-num","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-02-08T19:22:23+00:00","modified":"2022-02-08T19:22:23+00:00","when":"2022-02-08T19:22:23+00:00","text":"Summary: Global: Cloud build unable to create github triggers\nDescription: We are experiencing an issue with Cloud Build beginning at Wednesday, 2022-02-02 12:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-02-08 12:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers will see a \"Failed to load\" error with following message:\n\"There was an error while loading /cloud-build/triggers?project=\nPlease try again\"\nWorkaround: Modify the URL in the browser to end with the project number\ne.g. https://console.cloud.google.com/cloud-build/triggers/add?project=$project-id ---\u003e https://console.cloud.google.com/cloud-build/triggers/add?project=$project-num","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-02-10T23:12:33+00:00","modified":"2022-02-10T23:12:33+00:00","when":"2022-02-10T23:12:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 02 February 2022 12:00\n**Incident End:** 09 February 2022 05:48\n**Duration:** 6 days, 17 hours, 48 minutes\n**Affected Services and Features:**\nCloud Build\n**Regions/Zones:** Global\n**Description:**\nCreating Cloud Build GitHub triggers in the Cloud Console failed when the URL of the page ended with a project ID as the last argument (e.g. https://console.cloud.google.com/cloud-build/triggers/add?project=project-id-1234) Creation of triggers via gcloud was not impacted. From preliminary analysis, the root cause of the issue was that the Cloud Console began sending resource names in a format Cloud Build could not parse.\n**Customer Impact:**\nCustomers attempting to create GitHub triggers when using a Cloud Console URL that ended in their project ID would have experienced pages that displayed “Failed to load” errors.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Build","id":"fw8GzBdZdqy4THau7e1y"}],"uri":"incidents/1cPiGbWDMYHFMcTaLADd","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"whAt3eeGhT2SbR2aQXwA","number":"15275509219608406501","begin":"2022-01-16T08:49:46+00:00","created":"2022-01-16T08:49:54+00:00","end":"2022-01-16T10:10:36+00:00","modified":"2022-01-16T10:10:36+00:00","external_desc":"Users will not receive alerts on Firebase console.","updates":[{"created":"2022-01-16T10:10:36+00:00","modified":"2022-01-16T10:10:36+00:00","when":"2022-01-16T10:10:36+00:00","text":"The issue with Firebase Console is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-16T10:07:27+00:00","modified":"2022-01-16T10:07:27+00:00","when":"2022-01-16T10:07:27+00:00","text":"Summary: Users will not receive alerts on Firebase console.\nDescription: We believe the issue with Firebase Console is mitigated. Alerts may be delayed by up to 30 minutes but are still being processed.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Sunday, 2022-01-16 04:00 US/Pacific with current details.\nDiagnosis: Users will not receive alerts on Firebase console.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-01-16T09:27:18+00:00","modified":"2022-01-16T09:27:18+00:00","when":"2022-01-16T09:27:18+00:00","text":"Summary: Users will not receive alerts on Firebase console.\nDescription: We are experiencing an issue with Firebase Console.Our engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-01-16 04:30 US/Pacific with current details.\nDiagnosis: Users will not receive alerts on Firebase console.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-01-16T09:26:04+00:00","modified":"2022-01-16T09:26:10+00:00","when":"2022-01-16T09:26:04+00:00","text":"Summary: Users will not receive alerts on Firebase console.\nDescription: We are experiencing an issue with Firebase Console.Our engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-01-16 04:30 US/Pacific with current details.\nDiagnosis: Users will not receive alerts on Firebase console.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2022-01-16T08:49:48+00:00","modified":"2022-01-16T08:49:54+00:00","when":"2022-01-16T08:49:48+00:00","text":"Summary: Users will not receive alerts on Firebase console.\nDescription: We've received a report of an issue with Firebase Console as of Sunday, 2022-01-16 00:29 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Sunday, 2022-01-16 01:25 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2022-01-16T10:10:36+00:00","modified":"2022-01-16T10:10:36+00:00","when":"2022-01-16T10:10:36+00:00","text":"The issue with Firebase Console is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"CETSkT92V21G6A1x28me","service_name":"Cloud Firestore","affected_products":[{"title":"Cloud Firestore","id":"CETSkT92V21G6A1x28me"}],"uri":"incidents/whAt3eeGhT2SbR2aQXwA","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"NYcRpDHwjnhD6Gh2yUyK","number":"15829028318356552113","begin":"2022-01-11T21:20:00+00:00","created":"2022-01-11T21:30:39+00:00","end":"2022-01-11T22:16:00+00:00","modified":"2022-05-26T20:02:06+00:00","external_desc":"us-east1-c: Load balancing creation/modifications not taking effect.","updates":[{"created":"2022-01-11T22:16:34+00:00","modified":"2022-01-11T22:16:35+00:00","when":"2022-01-11T22:16:34+00:00","text":"The issue with Cloud Networking has been resolved for all affected projects as of Tuesday, 2022-01-11 14:14 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-11T21:30:33+00:00","modified":"2022-01-11T21:30:39+00:00","when":"2022-01-11T21:30:33+00:00","text":"Summary: us-east1-c: Load balancing creation/modifications not taking effect.\nDescription: We are experiencing an issue with Cloud Networking. Customers are unable to make changes to load balancer in us-east1-c region.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-01-11 14:42 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Create or modify load balancers actions do not take effect.\nWorkaround: Utilize other zones in us-east1.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-01-11T22:16:34+00:00","modified":"2022-01-11T22:16:35+00:00","when":"2022-01-11T22:16:34+00:00","text":"The issue with Cloud Networking has been resolved for all affected projects as of Tuesday, 2022-01-11 14:14 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/NYcRpDHwjnhD6Gh2yUyK","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"izF3BTJMavU36BBKJNAz","number":"12681297379738414769","begin":"2022-01-11T20:50:00+00:00","created":"2022-01-11T22:53:17+00:00","end":"2022-01-11T23:04:00+00:00","modified":"2022-01-12T21:43:01+00:00","external_desc":"Apigee is experiencing issues loading the Apigee UI at apigee.google.com.","updates":[{"created":"2022-01-12T21:43:01+00:00","modified":"2022-01-12T21:43:01+00:00","when":"2022-01-12T21:43:01+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 11 January 2022 12:50\n**Incident End:** 11 January 2022 15:04\n**Duration:** 2 hours, 14 minutes\n**Affected Services and Features:**\n- Apigee X - management UI\n- Apigee Hybrid - management UI\n**Regions/Zones:** Global\n**Description:**\nApigee X and Apigee Hybrid experienced elevated page errors loading the console user interface globally for 2 hours, 14 minutes. From preliminary analysis, the root cause of the issue was due to an uncaught error in a new release.\n**Customer Impact:**\n- Users with custom Google Workspace domains were unable to load Apigee console pages at apigee.google.com.\n- Page banners were visible with an \"Internal Server Error\" message\n**Additional details:**\n- This only impacted users with email domains other than @gmail.com\n- Runtime traffic was unaffected.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-12T00:00:32+00:00","modified":"2022-01-12T00:00:32+00:00","when":"2022-01-12T00:00:32+00:00","text":"The issue with Apigee has been resolved for all affected users as of Tuesday, 2022-01-11 15:58 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-11T23:22:21+00:00","modified":"2022-01-11T23:22:22+00:00","when":"2022-01-11T23:22:21+00:00","text":"Summary: Apigee is experiencing issues loading the Apigee UI at apigee.google.com.\nDescription: We believe the issue with loading the Apigee UI is resolved. Engineers are working to confirm full resolution.\nWe will provide an update by Tuesday, 2022-01-11 16:30 US/Pacific with current details.\nDiagnosis: Users may receive a message \"Internal Server Error\". Runtime traffic is unaffected.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2022-01-11T22:53:16+00:00","modified":"2022-01-11T22:53:17+00:00","when":"2022-01-11T22:53:16+00:00","text":"Summary: Apigee is experiencing issues loading the Apigee UI at apigee.google.com.\nDescription: We are experiencing an issue with Apigee beginning at Tuesday, 2022-01-11 12:50 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2022-01-11 15:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Users may receive a message \"Internal Server Error\". Runtime traffic is unaffected.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-01-12T21:43:01+00:00","modified":"2022-01-12T21:43:01+00:00","when":"2022-01-12T21:43:01+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 11 January 2022 12:50\n**Incident End:** 11 January 2022 15:04\n**Duration:** 2 hours, 14 minutes\n**Affected Services and Features:**\n- Apigee X - management UI\n- Apigee Hybrid - management UI\n**Regions/Zones:** Global\n**Description:**\nApigee X and Apigee Hybrid experienced elevated page errors loading the console user interface globally for 2 hours, 14 minutes. From preliminary analysis, the root cause of the issue was due to an uncaught error in a new release.\n**Customer Impact:**\n- Users with custom Google Workspace domains were unable to load Apigee console pages at apigee.google.com.\n- Page banners were visible with an \"Internal Server Error\" message\n**Additional details:**\n- This only impacted users with email domains other than @gmail.com\n- Runtime traffic was unaffected.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9Y13BNFy4fJydvjdsN3X","service_name":"Apigee","affected_products":[{"title":"Apigee","id":"9Y13BNFy4fJydvjdsN3X"}],"uri":"incidents/izF3BTJMavU36BBKJNAz","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"NMcnk6aE8xMHHwRGmyry","number":"328163091910127973","begin":"2022-01-08T23:15:00+00:00","created":"2022-01-09T00:54:20+00:00","end":"2022-01-09T02:36:00+00:00","modified":"2022-01-21T15:56:50+00:00","external_desc":"US-WEST1: Multiple cloud products experiencing network issues","updates":[{"created":"2022-01-21T15:56:50+00:00","modified":"2022-01-21T15:56:50+00:00","when":"2022-01-21T15:56:50+00:00","text":"# UPDATED INCIDENT REPORT\n## Summary\nOn Saturday, 8 January 2022, multiple GCP products in us-west1-b experienced increased network latency for a duration of 3 hours and 22 minutes. To our affected customers in us-west1-b who were impacted during this outage, we sincerely apologize. This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform’s performance and availability.\n## Root Cause\nGoogle leverages \"Software-defined Networking” (SDN) to simplify and automate the growth and expansion of our data center networks. This allows us to dynamically scale up data center networks to meet customer demand. This outage began when a routine maintenance event was performed on one of the SDN components. This maintenance event triggered an application failover that prompted a newly active replica to perform reconciliation from a previous checkpoint. These events are expected to be seamless and cause no network disruption. However, the checkpoint data was incorrectly missing a particular piece of configuration information; this was propagated to ~15% of the network switches serving us-west1-b.\nSeconds after this event, the SDN automatically corrected the incomplete configuration. Reprogramming of the affected switches triggered a race condition within the switch firmware, eventually causing them to crash. Automatic repair and mitigation actions were invoked within 10 minutes of switches failing. However, the unexpectedly large number, and network proximity, of the failures prevented a fully automatic recovery. As such, the outage was not mitigated until on-call engineers manually recovered the affected switches. The corrupted checkpoint data was only present in a single location and therefore no other cloud zone was ever at risk.\n## Remediation and Prevention\nThe outage was detected by Google Engineers on Saturday, 8 January at 15:25 US/Pacific, who immediately started an investigation. At 16:03, engineers tried to migrate traffic away from the impacted switches in the switch fabric, but this did not resolve the issue. At 17:19, engineers ascertained that the impacted switches needed to have their services restarted, and at 17:37, they began to do so. The network issue was mitigated at 18:00, when enough switches had been restarted to successfully serve all network traffic. The issue was fully resolved by 18:36, when all services were marked as recovered.\nThe conditions that caused this disruption existed only in a small subset of Google's network. The function responsible for causing it has been disabled and will not be reenabled until the steps below have been fully implemented.\n1. Avoiding the trigger: - We have audited and disabled the SDN feature that triggered this incident. - We have identified test coverage gaps in our SDN release process, which will be addressed to prevent a repeat of this and other similar issues. - Our systems will be modified to validate SDN checkpoint data before it is persisted for later recovery. - Our SDN maintenance workflow safety checks will be enhanced to automatically stop the workflow during an outage.\n2. Ensure changes are applied to the SDN more progressively: - We will redistribute SDN services to limit the areas impacted by failed maintenance workflows. - We will be developing new SDN safety components that validate potentially disruptive operations by applying them to network devices that are similarly configured, but not currently active, before deploying them further.\n3. Reduce time to resolution: - We will improve network management tooling to simplify and expedite mass switch recovery. - We will deploy monitoring improvements to more quickly identify broad network switch failures.\n## Detailed Description of Impact\nOn Saturday, 8 January from 15:15 to 18:36 US/Pacific:\n#### Cloud Router\nAffected Cloud Router customers would have experienced elevated packet loss from 40% to 100%.\n#### Cloud VPN\nAffected Cloud VPN customers would have experienced elevated packet loss from 40% to 100%.\n#### Cloud DNS\nCloud DNS customers experienced increased latency and errors on DNS requests.\n#### Cloud AI\nCloud AI customers experienced elevated latency on prediction requests.\n#### Cloud Run\nCloud Run customers experienced network connectivity issues.\n#### Cloud Spanner\nCloud Spanner Customers experienced elevated latency.\n#### Google Compute Engine\nGoogle Compute Engine customers experienced network latency, connectivity issues and slow input/output (I/O) operations on disks due to network latency, which may have manifested as unresponsive Compute Engine instances, including Google Kubernetes instances.\n#### Other Services\nOther services that require Google Cloud Networking in us-west1-b were also affected. Customers would experience latency, errors, and delays.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-19T15:39:36+00:00","modified":"2022-01-19T15:39:36+00:00","when":"2022-01-19T15:39:36+00:00","text":"INCIDENT REPORT\n## Summary\nOn Saturday, 8 January 2022, multiple GCP products in us-west1-b experienced increased network latency for a duration of 3 hours and 22 minutes. To our affected customers in us-west1-b who were impacted during this outage, we sincerely apologize. This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform’s performance and availability.\n## Root Cause\nGoogle leverages \"Software-defined Networking” (SDN) to simplify and automate the growth and expansion of our data center networks. This allows us to dynamically scale up data center networks to meet customer demand. This outage began when a routine maintenance event was performed on one of the SDN components. This maintenance event triggered an application failover that prompted a newly active replica to perform reconciliation from a previous checkpoint. These events are expected to be seamless and cause no network disruption. However, the checkpoint data was incorrectly missing a particular piece of configuration information; this was propagated to ~15% of the network switches serving us-west1-b.\nSeconds after this event, the SDN automatically corrected the incomplete configuration. Reprogramming of the affected switches triggered a race condition within the switch firmware, eventually causing them to crash. Automatic repair and mitigation actions were invoked within 10 minutes of switches failing. However, the unexpectedly large number, and network proximity, of the failures prevented a fully automatic recovery. As such, the outage was not mitigated until on-call engineers manually recovered the affected switches. The corrupted checkpoint data was only present in a single location and therefore no other cloud zone was ever at risk.\n## Remediation and Prevention\nThe outage was detected by Google Engineers on Saturday, 8 January at 15:25 US/Pacific, who immediately started an investigation. At 16:03, engineers tried to migrate traffic away from the impacted switches in the switch fabric, but this did not resolve the issue. At 17:19 engineers ascertained that the impacted switches needed to have their services restarted, and at 17:37 they began to do so. The network issue was mitigated at 18:00 when enough switches had been restarted to successfully serve all network traffic. The issue was fully resolved by 18:36 when all services were marked as recovered.\nWe are still working on the details of steps to avert further issues of this type, which in line with Google’s standard practice, are being designed to ensure the following:\n- Prevention of a reoccurrence of an outage of this nature.\n- Detection and mitigation of similar, future outages more quickly.\nThe conditions that caused this disruption existed only in a small part of the network. The function responsible for causing it has been disabled and will not be re-enabled until the steps above have been fully implemented.\n## Detailed Description of Impact\nOn Saturday, 8 January from 15:15 to 18:36 US/Pacific:\n#### Cloud Router\nAffected Cloud Router customers would have experienced elevated packet loss from 40% to 100%.\n#### Cloud VPN\nAffected Cloud VPN customers would have experienced elevated packet loss from 40% to 100%.\n#### Cloud DNS\nCloud DNS customers experienced increased latency and errors on DNS requests.\n#### Cloud AI\nCloud AI customers experienced elevated latency on prediction requests.\n#### Cloud Run\nCloud Run customers experienced network connectivity issues.\n#### Cloud Spanner\nCloud Spanner Customers experienced elevated latency.\n#### Google Compute Engine\nGoogle Compute Engine customers experienced network latency, connectivity issues and slow input/output (I/O) operations on disks due to network latency, which may have manifested as unresponsive Compute Engine instances, including Google Kubernetes instances.\n#### Other Services\nOther services that require Google Cloud Networking in us-west1-b were also affected. Customers would experience latency, errors, and delays.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-11T02:34:49+00:00","modified":"2022-01-11T02:34:49+00:00","when":"2022-01-11T02:34:49+00:00","text":"Mini Incident Report (Full Incident Report To Follow)\nWe apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 08 January 2022 15:14\n**Incident End:** 08 January 2022 18:36\n**Duration:** 3 hours, 22 minutes\n**Affected Services and Features:**\n- Cloud Router\n- Cloud VPN\n- Cloud DNS\n- Cloud AI\n- Cloud Run\n- Cloud Spanner\n- Persistent DIsk\n**Regions/Zones:** us-west1-b\n**Description:**\nMultiple GCP products experienced network issues in the us-west1 region for 3 hours and 22 minutes. From preliminary analysis, the root cause of the issue is related to an unexpected port/switch configuration and we are continuing investigations.\n**Customer Impact:**\n- Cloud Router - Customers may experience elevated packet loss.\n- Cloud DNS - Increased latency and/or errors reaching out to Cloud DNS\n- Cloud AI - Elevated prediction latencies\n- Cloud Run - Network connectivity issues\n- Cloud Spanner - Elevated network latencies\n- Persistent Disk - Short period of slow I/O operations on disks, which may have manifested as unresponsive -Compute Engine instances.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-09T02:42:53+00:00","modified":"2022-01-09T02:42:54+00:00","when":"2022-01-09T02:42:53+00:00","text":"The issue with Cloud Networking has been resolved for all affected projects as of Saturday, 2022-01-08 18:36 US/Pacific.\nWe will publish an analysis of this incident once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2022-01-09T02:02:08+00:00","modified":"2022-01-09T02:02:09+00:00","when":"2022-01-09T02:02:08+00:00","text":"Summary: US-WEST1: Multiple cloud products experiencing network issues\nDescription: We are experiencing network issues in us-west1.\nBelow is the list of affected products:\n* Cloud Router - Customers may experience elevated packet loss.\n* Cloud DNS - Increased latency and/or errors reaching out to Cloud DNS\n* Cloud AI - Elevated prediction latencies\n* Cloud Run - Network connectivity issues\n* Cloud Spanner - Elevated network latencies\nMitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Saturday, 2022-01-08 18:53 US/Pacific.\nDiagnosis: Affected customer will see packet loss.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2022-01-09T01:49:22+00:00","modified":"2022-01-09T01:49:23+00:00","when":"2022-01-09T01:49:22+00:00","text":"Summary: US-WEST1: Multiple cloud products experiencing network issues\nDescription: We are experiencing network issues in us-west1.\nBelow is the list of affected products:\n* Cloud Router - Customers may experience elevated packet loss.\n* Cloud DNS - Increased latency and/or errors reaching out to Cloud DNS\n* Cloud AI - Elevated prediction latencies\n* Cloud Run - Network connectivity issues\nMitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Saturday, 2022-01-08 18:53 US/Pacific.\nDiagnosis: Affected customer will see packet loss.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2022-01-09T00:54:13+00:00","modified":"2022-01-09T00:54:21+00:00","when":"2022-01-09T00:54:13+00:00","text":"Summary: Multiple cloud connectivity products experiencing packet loss in US-WEST1\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Saturday, 2022-01-08 17:53 US/Pacific.\nDiagnosis: Affected customer will see packet loss.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2022-01-21T15:56:50+00:00","modified":"2022-01-21T15:56:50+00:00","when":"2022-01-21T15:56:50+00:00","text":"# UPDATED INCIDENT REPORT\n## Summary\nOn Saturday, 8 January 2022, multiple GCP products in us-west1-b experienced increased network latency for a duration of 3 hours and 22 minutes. To our affected customers in us-west1-b who were impacted during this outage, we sincerely apologize. This is not the level of quality and reliability we strive to offer you, and we are taking immediate steps to improve the platform’s performance and availability.\n## Root Cause\nGoogle leverages \"Software-defined Networking” (SDN) to simplify and automate the growth and expansion of our data center networks. This allows us to dynamically scale up data center networks to meet customer demand. This outage began when a routine maintenance event was performed on one of the SDN components. This maintenance event triggered an application failover that prompted a newly active replica to perform reconciliation from a previous checkpoint. These events are expected to be seamless and cause no network disruption. However, the checkpoint data was incorrectly missing a particular piece of configuration information; this was propagated to ~15% of the network switches serving us-west1-b.\nSeconds after this event, the SDN automatically corrected the incomplete configuration. Reprogramming of the affected switches triggered a race condition within the switch firmware, eventually causing them to crash. Automatic repair and mitigation actions were invoked within 10 minutes of switches failing. However, the unexpectedly large number, and network proximity, of the failures prevented a fully automatic recovery. As such, the outage was not mitigated until on-call engineers manually recovered the affected switches. The corrupted checkpoint data was only present in a single location and therefore no other cloud zone was ever at risk.\n## Remediation and Prevention\nThe outage was detected by Google Engineers on Saturday, 8 January at 15:25 US/Pacific, who immediately started an investigation. At 16:03, engineers tried to migrate traffic away from the impacted switches in the switch fabric, but this did not resolve the issue. At 17:19, engineers ascertained that the impacted switches needed to have their services restarted, and at 17:37, they began to do so. The network issue was mitigated at 18:00, when enough switches had been restarted to successfully serve all network traffic. The issue was fully resolved by 18:36, when all services were marked as recovered.\nThe conditions that caused this disruption existed only in a small subset of Google's network. The function responsible for causing it has been disabled and will not be reenabled until the steps below have been fully implemented.\n1. Avoiding the trigger: - We have audited and disabled the SDN feature that triggered this incident. - We have identified test coverage gaps in our SDN release process, which will be addressed to prevent a repeat of this and other similar issues. - Our systems will be modified to validate SDN checkpoint data before it is persisted for later recovery. - Our SDN maintenance workflow safety checks will be enhanced to automatically stop the workflow during an outage.\n2. Ensure changes are applied to the SDN more progressively: - We will redistribute SDN services to limit the areas impacted by failed maintenance workflows. - We will be developing new SDN safety components that validate potentially disruptive operations by applying them to network devices that are similarly configured, but not currently active, before deploying them further.\n3. Reduce time to resolution: - We will improve network management tooling to simplify and expedite mass switch recovery. - We will deploy monitoring improvements to more quickly identify broad network switch failures.\n## Detailed Description of Impact\nOn Saturday, 8 January from 15:15 to 18:36 US/Pacific:\n#### Cloud Router\nAffected Cloud Router customers would have experienced elevated packet loss from 40% to 100%.\n#### Cloud VPN\nAffected Cloud VPN customers would have experienced elevated packet loss from 40% to 100%.\n#### Cloud DNS\nCloud DNS customers experienced increased latency and errors on DNS requests.\n#### Cloud AI\nCloud AI customers experienced elevated latency on prediction requests.\n#### Cloud Run\nCloud Run customers experienced network connectivity issues.\n#### Cloud Spanner\nCloud Spanner Customers experienced elevated latency.\n#### Google Compute Engine\nGoogle Compute Engine customers experienced network latency, connectivity issues and slow input/output (I/O) operations on disks due to network latency, which may have manifested as unresponsive Compute Engine instances, including Google Kubernetes instances.\n#### Other Services\nOther services that require Google Cloud Networking in us-west1-b were also affected. Customers would experience latency, errors, and delays.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud DNS","id":"TUZUsWSJUVJGW97Jq2sH"},{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"AI Platform Prediction","id":"eFCqAnWci3AZiqsz1NjQ"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/NMcnk6aE8xMHHwRGmyry","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"cBRgkAaxS8Rq3vvbPHaw","number":"17207454738058883218","begin":"2021-12-21T12:31:14+00:00","created":"2021-12-21T13:07:19+00:00","end":"2021-12-21T14:03:27+00:00","modified":"2021-12-21T14:03:27+00:00","external_desc":"Internal errors for Dataset commands insert/update in us-central1","updates":[{"created":"2021-12-21T14:03:25+00:00","modified":"2021-12-21T14:03:28+00:00","when":"2021-12-21T14:03:25+00:00","text":"Upon further investigation we believe that the issue with Google BigQuery is affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-21T13:07:11+00:00","modified":"2021-12-21T13:07:20+00:00","when":"2021-12-21T13:07:11+00:00","text":"Summary: Internal errors for Dataset commands insert/update in us-central1\nDescription: We are experiencing an issue with Google BigQuery beginning at Tuesday, 2021-12-21 03:30 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-12-21 05:59 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customer may observe an internal error for Dataset commands insert/update in us-central1.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-12-21T14:03:25+00:00","modified":"2021-12-21T14:03:28+00:00","when":"2021-12-21T14:03:25+00:00","text":"Upon further investigation we believe that the issue with Google BigQuery is affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/cBRgkAaxS8Rq3vvbPHaw","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"fniZhzAhYdcJuosEwwtZ","number":"13628602836948027359","begin":"2021-12-17T17:10:00+00:00","created":"2021-12-17T18:43:10+00:00","end":"2021-12-18T06:11:00+00:00","modified":"2021-12-20T18:05:19+00:00","external_desc":"us multiregion: Elevated tail latency on read operations for GCS buckets","updates":[{"created":"2021-12-20T18:04:45+00:00","modified":"2021-12-20T18:04:45+00:00","when":"2021-12-20T18:04:45+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 17 December 2021 09:10\n**Incident End:** 17 December 2021 22:11\n**Incident Duration:** 13 hours, 1 minute\n**Affected Services and Features:**\nGoogle Cloud Storage (GCS) - Read and Write object operations\n**Regions/Zones:** US multi-region\n**Description:**\nGoogle Cloud Storage buckets experienced elevated tail latency on read and write object operations for a total duration of 13 hours, 1 minute. From preliminary analysis, an internal job incorrectly placed a significant load on the backend database.\nThis originally presented as increased tailed latency. The database was significantly overloaded leading to elevated 5xx errors for a 47 minute period between 10:50 and 11:37. The 5XX errors were resolved at 11:37 by adding additional resources and stopping the offending job. However, the job caused an extended backlog of work, which took until 22:11 to clear.\n**Customer Impact:**\n* Elevated 5xx errors related to read timeouts between 10:50 and 11:37\n* Elevated tail latency on read operations from GCS buckets for the duration of the incident.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-17T20:49:56+00:00","modified":"2021-12-17T20:49:57+00:00","when":"2021-12-17T20:49:56+00:00","text":"The issue with Google Cloud Storage has been resolved for all affected users as of Friday, 2021-12-17 11:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-17T19:47:03+00:00","modified":"2021-12-17T19:47:05+00:00","when":"2021-12-17T19:47:03+00:00","text":"Summary: us multiregion: Elevated tail latency on read operations for GCS buckets\nDescription: We believe the issue with Google Cloud Storage is partially resolved and the error rates decreased significantly.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Friday, 2021-12-17 13:30 US/Pacific with current details.\nDiagnosis: Affected customers may experience elevated tail latency on read operations for buckets in the us multiregion.\nWorkaround: Retrying failed or slow requests may succeed due to the relatively low error rate.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-17T19:22:47+00:00","modified":"2021-12-17T19:22:49+00:00","when":"2021-12-17T19:22:47+00:00","text":"Summary: us multiregion: Elevated tail latency on read operations for GCS buckets\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-12-17 13:00 US/Pacific.\nDiagnosis: Affected customers may experience elevated tail latency on read operations for buckets in the us multiregion.\nWorkaround: Retrying failed or slow requests may succeed due to the relatively low error rate.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-17T19:00:57+00:00","modified":"2021-12-17T19:01:04+00:00","when":"2021-12-17T19:00:57+00:00","text":"Summary: us: Elevated latency in us GCS buckets\nDescription: We are experiencing an issue with Google Cloud Storage beginning at Friday, 2021-12-17 09:00 US/Pacific.\nOur engineering team continues to investigate the issue. Current estimates indicates that 1% of GCS requests are impacted.\nWe will provide an update by Friday, 2021-12-17 13:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers may experience elevated tail latency on their buckets in the us multiregion.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-17T18:43:08+00:00","modified":"2021-12-17T18:43:10+00:00","when":"2021-12-17T18:43:08+00:00","text":"Summary: us: Elevated latency in us GCS buckets\nDescription: We are investigating a potential issue with Google Cloud Storage.\nWe will provide more information by Friday, 2021-12-17 11:56 US/Pacific.\nDiagnosis: Affected customers may experience elevated latency on their buckets in the us multiregion.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-12-20T18:04:45+00:00","modified":"2021-12-20T18:04:45+00:00","when":"2021-12-20T18:04:45+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 17 December 2021 09:10\n**Incident End:** 17 December 2021 22:11\n**Incident Duration:** 13 hours, 1 minute\n**Affected Services and Features:**\nGoogle Cloud Storage (GCS) - Read and Write object operations\n**Regions/Zones:** US multi-region\n**Description:**\nGoogle Cloud Storage buckets experienced elevated tail latency on read and write object operations for a total duration of 13 hours, 1 minute. From preliminary analysis, an internal job incorrectly placed a significant load on the backend database.\nThis originally presented as increased tailed latency. The database was significantly overloaded leading to elevated 5xx errors for a 47 minute period between 10:50 and 11:37. The 5XX errors were resolved at 11:37 by adding additional resources and stopping the offending job. However, the job caused an extended backlog of work, which took until 22:11 to clear.\n**Customer Impact:**\n* Elevated 5xx errors related to read timeouts between 10:50 and 11:37\n* Elevated tail latency on read operations from GCS buckets for the duration of the incident.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"UwaYoXQ5bHYHG6EdiPB8","service_name":"Google Cloud Storage","affected_products":[{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"}],"uri":"incidents/fniZhzAhYdcJuosEwwtZ","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"SjJ3FN51MAEJy7cZmoss","number":"4933937362309678094","begin":"2021-12-07T09:56:00+00:00","created":"2021-12-13T14:16:00+00:00","end":"2021-12-14T00:59:00+00:00","modified":"2021-12-14T19:59:08+00:00","external_desc":"Global: pubsub.googleapis.com autoscaling not worked as expected","updates":[{"created":"2021-12-14T19:59:05+00:00","modified":"2021-12-14T19:59:05+00:00","when":"2021-12-14T19:59:05+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 07 Dec 2021 01:56 PST\n**Incident End:** 13 Dec 2021 16:59 PST\n**Duration:** 6 Days, 15 hours, 3 minutes\n**Affected Services and Features:**\nGoogle Compute Engine (GCE) Autoscaler - Autoscaling using the “pubsub.googleapis.com/subscription/num_undelivered_messages” metric.\n**Regions/Zones:** Global\n**Description:**\nGCE autoscaling service configured to use the “pubsub.googleapis.com/subscription/num_undelivered_messages” metric, experienced an issue while processing the autoscaling function. From preliminary analysis, the root cause was identified as code changes affecting the autoscaler policy validation that was misconfigured for the affected metric.\n**Customer Impact:**\nCustomers using the metric pubsub.googleapis.com/subscription/num_undelivered_messages for autoscaling observed the autoscaling did not complete successfully.\nAutoscaling did not work for existing and new managed instance groups created by customers for the duration of the incident.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-14T01:12:31+00:00","modified":"2021-12-14T01:12:33+00:00","when":"2021-12-14T01:12:31+00:00","text":"The issue with Google Compute Engine has been resolved for all affected users as of Monday, 2021-12-13 16:59 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-13T18:52:25+00:00","modified":"2021-12-13T18:52:29+00:00","when":"2021-12-13T18:52:25+00:00","text":"Summary: Global: pubsub.googleapis.com autoscaling not worked as expected\nDescription: We believe the issue with Google Compute Engine is partially resolved for previously impacted projects. Newly created projects may still experience the issue until the rollout of the fix completes.\nFull resolution is expected to complete on Tuesday, 2021-12-14.\nDiagnosis: Customers that are use scaling on the pubsub.googleapis.com/subscription/num_undelivered_messages may see auto scaling not working.\nWorkaround: As known workarounds, if customers have a filter with resource.label.subscription_id = ... they could change it to resource.labels.subscription_id = ... to make auto-scaling work.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-13T16:21:17+00:00","modified":"2021-12-13T16:21:20+00:00","when":"2021-12-13T16:21:17+00:00","text":"Summary: GCP engineer are investigating an issue with auto-scaling pubsub.googleapis.com may not be working as intended.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2021-12-14.\nDiagnosis: Customers that are use scaling on the pubsub.googleapis.com/subscription/num_undelivered_messages may see auto scaling not working.\nWorkaround: As known workarounds, if customers have a filter with resource.label.subscription_id = ... they could change it to resource.labels.subscription_id = ... to make auto-scaling work.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-13T14:48:37+00:00","modified":"2021-12-13T14:48:40+00:00","when":"2021-12-13T14:48:37+00:00","text":"Summary: GCP engineer are investigating an issue with auto-scaling - pubsub.googleapis.com may not be working as intended.\nDescription: Beginning at Monday, 2021-12-13 00:00 US/Pacific. We are experiencing an issue with Google Compute Engine beginning at Monday, 2021-12-13 05:21:05 PST.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-12-13 09:15 US/Pacific with current details.\nDiagnosis: Customers that are use scaling on the pubsub.googleapis.com/subscription/num_undelivered_messages may see auto scaling not working.\nWorkaround: As known workarounds, if customers have a filter with resource.label.subscription_id = ... they could change it to resource.labels.subscription_id = ... to make auto-scaling work.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-13T14:26:15+00:00","modified":"2021-12-13T14:26:18+00:00","when":"2021-12-13T14:26:15+00:00","text":"Summary: GCP engineer are investigating an issue with auto-scaling - pubsub.googleapis.com may not be working as intended.\nDescription: Beginning at Monday, 2021-12-13 00:00 US/Pacific. We are experiencing an issue with Google Compute Engine beginning at Monday, 2021-12-13 05:21:05 PST.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-12-13 07:15 US/Pacific with current details.\nDiagnosis: Customers that are use scaling on the pubsub.googleapis.com/subscription/num_undelivered_messages may see auto scaling not working.\nWorkaround: As known workarounds, if customers have a filter with resource.label.subscription_id = ... they could change it to resource.labels.subscription_id = ... to make auto-scaling work.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-13T14:15:58+00:00","modified":"2021-12-13T14:16:01+00:00","when":"2021-12-13T14:15:58+00:00","text":"Summary: GCP engineer are investigating an issue with auto-scaling - pubsub.googleapis.com may not be working as intended.\nDescription: Beginning at Monday, 2021-12-13 00:00 US/Pacific. We are experiencing an issue with Google Compute Engine beginning at Monday, 2021-12-13 05:21:05 PST.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-12-13 07:15 US/Pacific with current details.\nDiagnosis: Customers that are use scaling on the pubsub.googleapis.com/subscription/num_undelivered_messages masy see auto scaling not working.\nWorkaround: As known workarounds, if customers have a filter with resource.label.subscription_id = ... they could change it to resource.labels.subscription_id = ... to make auto-scaling work.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-12-14T19:59:05+00:00","modified":"2021-12-14T19:59:05+00:00","when":"2021-12-14T19:59:05+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 07 Dec 2021 01:56 PST\n**Incident End:** 13 Dec 2021 16:59 PST\n**Duration:** 6 Days, 15 hours, 3 minutes\n**Affected Services and Features:**\nGoogle Compute Engine (GCE) Autoscaler - Autoscaling using the “pubsub.googleapis.com/subscription/num_undelivered_messages” metric.\n**Regions/Zones:** Global\n**Description:**\nGCE autoscaling service configured to use the “pubsub.googleapis.com/subscription/num_undelivered_messages” metric, experienced an issue while processing the autoscaling function. From preliminary analysis, the root cause was identified as code changes affecting the autoscaler policy validation that was misconfigured for the affected metric.\n**Customer Impact:**\nCustomers using the metric pubsub.googleapis.com/subscription/num_undelivered_messages for autoscaling observed the autoscaling did not complete successfully.\nAutoscaling did not work for existing and new managed instance groups created by customers for the duration of the incident.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"L3ggmi3Jy4xJmgodFA9K","service_name":"Google Compute Engine","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"}],"uri":"incidents/SjJ3FN51MAEJy7cZmoss","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"NinksF6tPrSYEzmMrQfs","number":"1107972568783514759","begin":"2021-11-16T19:20:12+00:00","created":"2021-11-16T19:20:14+00:00","end":"2021-11-16T19:41:01+00:00","modified":"2021-11-16T19:41:02+00:00","external_desc":"Global: Google App Engine Flex customers getting 404 errors on their websites.","updates":[{"created":"2021-11-16T19:41:01+00:00","modified":"2021-11-16T19:41:03+00:00","when":"2021-11-16T19:41:01+00:00","text":"The issue with Google App Engine has been resolved for all affected users as of Tuesday, 2021-11-16 11:40 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-16T19:30:45+00:00","modified":"2021-11-16T19:30:47+00:00","when":"2021-11-16T19:30:45+00:00","text":"Summary: Global: Google App Engine Flex customers getting 404 errors on their websites.\nDescription: We are experiencing an issue with Google App Engine.\nOur engineering team continues to investigate the issue.\nWe will provide further updates on the issue on the status page link: https://status.cloud.google.com/incidents/6PM5mNd43NbMqjCZ5REh\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customer applications using GCLB getting 404 responses and new deployments will fail.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-16T19:20:14+00:00","modified":"2021-11-16T19:20:15+00:00","when":"2021-11-16T19:20:14+00:00","text":"Summary: Global: Google App Engine Flex customers getting 404 errors on their websites.\nDescription: We are experiencing an issue with Google App Engine.\nOur engineering team continues to investigate the issue.\nWe will provide further updates on the issue on the status page link: https://status.cloud.google.com/incidents/6PM5mNd43NbMqjCZ5REh\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customer applications using GCLB getting 404 responses.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-16T19:41:01+00:00","modified":"2021-11-16T19:41:03+00:00","when":"2021-11-16T19:41:01+00:00","text":"The issue with Google App Engine has been resolved for all affected users as of Tuesday, 2021-11-16 11:40 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"kchyUtnkMHJWaAva8aYc","service_name":"Google App Engine","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/NinksF6tPrSYEzmMrQfs","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"M4J2kuEduFhpiF6BhDRE","number":"1692838651892905949","begin":"2021-11-16T18:50:09+00:00","created":"2021-11-16T18:50:11+00:00","end":"2021-11-16T18:59:34+00:00","modified":"2021-11-16T18:59:34+00:00","external_desc":"Multi-Region : Cloud Run experiencing issues with network traffic","updates":[{"created":"2021-11-16T18:59:34+00:00","modified":"2021-11-16T18:59:35+00:00","when":"2021-11-16T18:59:34+00:00","text":"The issue with Cloud Run has been resolved for all affected users as of Tuesday, 2021-11-16 10:59 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-16T18:50:11+00:00","modified":"2021-11-16T18:50:11+00:00","when":"2021-11-16T18:50:11+00:00","text":"Summary: Multi-Region : Cloud Run experiencing issues with network traffic\nDescription: We are experiencing an issue with Cloud Run beginning at Tuesday, 2021-11-16 09:35 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-11-16 11:24 US/Pacific with current details.\nPlease check https://status.cloud.google.com/incidents/6PM5mNd43NbMqjCZ5REh for further details on the issue.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may see network traffic drop drastically to Cloud Run.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-16T18:59:34+00:00","modified":"2021-11-16T18:59:35+00:00","when":"2021-11-16T18:59:34+00:00","text":"The issue with Cloud Run has been resolved for all affected users as of Tuesday, 2021-11-16 10:59 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9D7d2iNBQWN24zc1VamE","service_name":"Cloud Run","affected_products":[{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"}],"uri":"incidents/M4J2kuEduFhpiF6BhDRE","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"tXL2SNKYJeUEAbEnLJzD","number":"12615772904666642135","begin":"2021-11-16T18:34:55+00:00","created":"2021-11-16T18:34:58+00:00","end":"2021-11-16T18:58:27+00:00","modified":"2021-11-16T18:58:27+00:00","external_desc":"Multi-Region: App Engine experiencing network traffic issues","updates":[{"created":"2021-11-16T18:58:27+00:00","modified":"2021-11-16T18:58:28+00:00","when":"2021-11-16T18:58:27+00:00","text":"The issue with Google App Engine has been resolved for all affected users as of Tuesday, 2021-11-16 10:58 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-16T18:34:57+00:00","modified":"2021-11-16T18:34:58+00:00","when":"2021-11-16T18:34:57+00:00","text":"Summary: Multi-Region: App Engine experiencing network traffic issues\nDescription: We are experiencing an issue with Google App Engine beginning at Tuesday, 2021-11-16 09:35 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe apologize to all who are affected by the disruption.\nPlease check https://status.cloud.google.com/incidents/6PM5mNd43NbMqjCZ5REh for further details on the issue.\nDiagnosis: Customers may see reduced traffic in their apps\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-16T18:58:27+00:00","modified":"2021-11-16T18:58:28+00:00","when":"2021-11-16T18:58:27+00:00","text":"The issue with Google App Engine has been resolved for all affected users as of Tuesday, 2021-11-16 10:58 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"kchyUtnkMHJWaAva8aYc","service_name":"Google App Engine","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/tXL2SNKYJeUEAbEnLJzD","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"6PM5mNd43NbMqjCZ5REh","number":"9092799694103361605","begin":"2021-11-16T17:34:00+00:00","created":"2021-11-16T18:10:07+00:00","end":"2021-11-16T19:28:00+00:00","modified":"2021-11-23T02:30:42+00:00","external_desc":"Global: Experiencing Issue with Cloud networking","updates":[{"created":"2021-11-23T02:29:31+00:00","modified":"2021-11-23T02:29:31+00:00","when":"2021-11-23T02:29:31+00:00","text":"## INCIDENT REPORT\n## Introduction\nWe apologize for any impact the service disruption on Tuesday, 16 November 2021, may have had on your organization. Thank you for your patience and understanding as we worked to resolve the issue.\nWe want to share some information about what happened and the steps we are taking to ensure this issue doesn’t occur again. We also want to assure you that this service disruption does not have any bearing on our preparedness or platform reliability going into Black Friday/Cyber Monday (BFCM).\n## Incident Summary\nOn Tuesday, 16 November 2021 at 09:35 PT, Google Cloud Networking experienced issues with the Google External Proxy Load Balancing (GCLB) service. Affected customers received Google 404 errors in response to HTTP/S requests.\nGoogle engineers were alerted to the issue via automated alerting at 09:50 PT, which aligned with incoming customer support requests, and we immediately started to mitigate the issue by rolling back to the last known good configuration.\nBetween 09:35 and 10:08 PT, customers affected by the outage may have encountered 404 errors when accessing any web page (URL) served by Google External Proxy Load Balancing.\nA rollback to the last known good configuration completed at 10:08 PT, which resolved the 404 errors. To avoid the risk of a recurrence, our engineers suspended customer-initiated configuration changes in GCLB. As a result, GCLB service customers were unable to make changes to their load balancing configuration between 10:04 and 11:28 PT.\nDuring the change suspension period, we validated the fix to safeguard against recurrence and deployed additional proctoring and monitoring to ensure safe resumption of service.\nBy 11:28 PT, customer configuration pushes resumed, and normal service was restored. The total duration of impact was 1 hour and 53 minutes.\n## Root Cause\nThis incident was caused by a bug in the configuration pipeline that propagates customer configuration rules to GCLB. The bug was introduced 6 months ago and allowed a race condition (when behavior depends on the timing of data accesses) that would, in very rare cases, push a corrupted configuration file to GCLB. The GCLB update pipeline contains extensive validation checks to prevent corrupt configurations, but the race condition was one that could corrupt the file near the end of the pipeline.\nA Google engineer discovered this bug on 12 November, which caused us to declare an internal high-priority incident because of the latent risk to production systems. After analyzing the bug, we froze a part of our configuration system to make the likelihood of the race condition even lower. Since the race condition had existed in the fleet for several months already, the team believed that this extra step made the risk even lower. Thus the team believed the lowest-risk path, especially given the proximity to BFCM, was to roll out fixes in a controlled manner as opposed to a same-day emergency patch.\nWe developed two mitigations: patch A closed the race condition itself; and patch B added additional input validation to the binary receiving the configuration to prevent it from accepting the new configuration, even if the race condition occurred.\nBoth patches were ready and verified to fix the problem by 13 November. Gradual rollouts of both patches started on Monday, 15 November, and patch B completed rollout by that evening. On Tuesday, 16 November, as the patch A rollout was within 30 minutes of completing, the race condition did manifest in an unpatched cluster, and the outage started.\nAdditionally, even though patch B did protect against the kind of input errors observed during testing, the actual race condition produced a different form of error in the configuration, which the completed rollout of patch B did not prevent from being accepted.\nOnce the root cause was identified, our engineers mitigated the issue by restoring a known-good configuration, and completed and verified the fix, which eliminates the risk of recurrence.\n## Service(s) Affected:\n- Google Cloud Networking: Customer HTTP/S endpoints served 404 error pages. During partial recovery, traffic was served, but customers were unable to make changes to their load balancer configurations.\n- GCLB can be used to load balance traffic to a number of other Google Cloud services, which lost traffic because of the outage. Customers who use serverless network endpoint groups on GCLB as a frontend to Google Cloud Run, Google App Engine, Google App Engine Flex, or Google Cloud Functions received 404 errors when attempting to access their service. Customers using Apigee, Firebase, or Google App Engine Flex received 404 errors when attempting to access their service.\n## Zone(s) Affected:\nGlobal\n## How Customers Experienced the Issue:\nBetween 09:35 and 10:08 PT, most endpoints served by global GCLB load balancers returned a 404 error. For an additional 1 hour and 20 minutes, customers were unable to make changes to their load balancing configuration.\n## Workaround(s):\nNone.\nService was restored on 16 November 2021 at 11:28 PT, and the Google Cloud Status Dashboard was updated by 12:08 PT to reflect this.\n## Remediation and Prevention\nWe have fixed the underlying bug and are taking the following actions to prevent recurrence:\nWe immediately added additional alerting, which will notify us to similar issues significantly faster going forward.\nWe are adding safeguards to prevent similar issues from occurring in the future. These safeguards provide strengthened automated correctness-checking to configurations before they are applied.\nWe are accelerating planned architectural changes that will improve how we isolate and resolve such issues in the future.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-17T04:56:27+00:00","modified":"2021-11-23T02:30:42+00:00","when":"2021-11-17T04:56:27+00:00","text":"## PRELIMINARY INCIDENT REPORT ##\nWe apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 16 November 2021 09:34\n**Incident End:** 16 November 2021 11:28\n**Duration:** 1 hour, 54 minutes\n**Affected Services and Features:**\n- Google Cloud Networking\n- Google Cloud Functions\n- Google Cloud Run\n- Google App Engine\n- Google App Engine Flex\n- Apigee\n- Firebase\n**Regions/Zones:**\nus-central,\neurope-west1,\nglobal\n**Description:**\nGoogle Cloud Networking experienced issues with Google Cloud Load Balancing (GCLB) service resulting in impact to several downstream Google Cloud services. Impacted customers observed Google 404 errors on their websites. From preliminary analysis, the root cause of the issue was a latent bug in a network configuration service which was triggered during routine system operation.\nThe outage has been root caused and the mitigation fully deployed, with two forms of safeguards protecting against the issue happening in the future.\n**Customer Impact:**\n- Google Cloud Networking – Customers were unable to make changes to their website load balancing and their websites served 404 error pages.\n- Google Cloud Functions – Customers who use GCLB service received 404 errors when attempting to access their service.\n- Google Cloud Run – Observed a 25% decrease in traffic in us-central1. Customers who use GCLB service received 404 errors when attempting to access their service.\n- Google App Engine – Observed 80% decrease in traffic in us-central and europe-west1. Customers who use GCLB service received 404 errors when attempting to access their service.\n- Google App Engine Flex – Customers who use GCLB received 404 errors when attempting to access their service and customer deployments experienced failures.\n- Apigee – Customers who use GCLB received 404 errors for runtime requests.\n- Google Firebase – Customers who use GCLB service received 404 errors when attempting to access their service.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-16T20:08:52+00:00","modified":"2021-11-16T20:08:53+00:00","when":"2021-11-16T20:08:52+00:00","text":"The issue with Cloud Networking has been resolved for all affected projects as of Tuesday, 2021-11-16 11:28 US/Pacific.\nCustomers impacted by the issue may have encountered 404 errors when accessing web pages served by the Google External Proxy Load Balancer between 09:35 and 10:10 US/Pacific.\nCustomer impact from 10:10 to 11:28 US/Pacific was configuration changes to External Proxy Load Balancers not taking effect. As of 11:28 US/Pacific configuration pushes resumed.\nGoogle Cloud Run, Google App Engine, Google Cloud Functions, and Apigee were also impacted.\nWe will publish an analysis of this incident, once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-16T19:26:03+00:00","modified":"2021-11-16T19:26:04+00:00","when":"2021-11-16T19:26:03+00:00","text":"Summary: Global: Experiencing Issue with Cloud networking\nDescription: We believe the issue with Cloud Networking is partially resolved.\nCustomers will be unable to apply changes to their load balancers until the issue is fully resolved.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-11-16 12:28 US/Pacific with current details.\nDiagnosis: Customers impacted by the issue may have encountered 404 errors when accessing web pages served by the Google External Proxy Load Balancer between 09:35 and 10:10 US/Pacific.\nCustomer impact from 10:10 US/Pacific onward is configuration changes to External Proxy Load Balancers not taking effect.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-11-16T18:17:50+00:00","modified":"2021-11-16T18:17:51+00:00","when":"2021-11-16T18:17:50+00:00","text":"Summary: Global: Experiencing Issue with Cloud networking\nDescription: We believe the issue with Cloud Networking is partially resolved.\nCustomers will be unable to apply changes to their load balancers until the issue is fully resolved.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-11-16 11:28 US/Pacific with current details.\nDiagnosis: Customers may encounter 404 errors when accessing web pages.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-11-16T18:10:07+00:00","modified":"2021-11-16T18:10:08+00:00","when":"2021-11-16T18:10:07+00:00","text":"Summary: Global: Experiencing Issue with Cloud networking\nDescription: We are experiencing an issue with Cloud Networking beginning at Tuesday, 2021-11-16 09:53 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-11-16 10:40 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may encounter 404 errors when accessing web pages.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2021-11-23T02:29:31+00:00","modified":"2021-11-23T02:29:31+00:00","when":"2021-11-23T02:29:31+00:00","text":"## INCIDENT REPORT\n## Introduction\nWe apologize for any impact the service disruption on Tuesday, 16 November 2021, may have had on your organization. Thank you for your patience and understanding as we worked to resolve the issue.\nWe want to share some information about what happened and the steps we are taking to ensure this issue doesn’t occur again. We also want to assure you that this service disruption does not have any bearing on our preparedness or platform reliability going into Black Friday/Cyber Monday (BFCM).\n## Incident Summary\nOn Tuesday, 16 November 2021 at 09:35 PT, Google Cloud Networking experienced issues with the Google External Proxy Load Balancing (GCLB) service. Affected customers received Google 404 errors in response to HTTP/S requests.\nGoogle engineers were alerted to the issue via automated alerting at 09:50 PT, which aligned with incoming customer support requests, and we immediately started to mitigate the issue by rolling back to the last known good configuration.\nBetween 09:35 and 10:08 PT, customers affected by the outage may have encountered 404 errors when accessing any web page (URL) served by Google External Proxy Load Balancing.\nA rollback to the last known good configuration completed at 10:08 PT, which resolved the 404 errors. To avoid the risk of a recurrence, our engineers suspended customer-initiated configuration changes in GCLB. As a result, GCLB service customers were unable to make changes to their load balancing configuration between 10:04 and 11:28 PT.\nDuring the change suspension period, we validated the fix to safeguard against recurrence and deployed additional proctoring and monitoring to ensure safe resumption of service.\nBy 11:28 PT, customer configuration pushes resumed, and normal service was restored. The total duration of impact was 1 hour and 53 minutes.\n## Root Cause\nThis incident was caused by a bug in the configuration pipeline that propagates customer configuration rules to GCLB. The bug was introduced 6 months ago and allowed a race condition (when behavior depends on the timing of data accesses) that would, in very rare cases, push a corrupted configuration file to GCLB. The GCLB update pipeline contains extensive validation checks to prevent corrupt configurations, but the race condition was one that could corrupt the file near the end of the pipeline.\nA Google engineer discovered this bug on 12 November, which caused us to declare an internal high-priority incident because of the latent risk to production systems. After analyzing the bug, we froze a part of our configuration system to make the likelihood of the race condition even lower. Since the race condition had existed in the fleet for several months already, the team believed that this extra step made the risk even lower. Thus the team believed the lowest-risk path, especially given the proximity to BFCM, was to roll out fixes in a controlled manner as opposed to a same-day emergency patch.\nWe developed two mitigations: patch A closed the race condition itself; and patch B added additional input validation to the binary receiving the configuration to prevent it from accepting the new configuration, even if the race condition occurred.\nBoth patches were ready and verified to fix the problem by 13 November. Gradual rollouts of both patches started on Monday, 15 November, and patch B completed rollout by that evening. On Tuesday, 16 November, as the patch A rollout was within 30 minutes of completing, the race condition did manifest in an unpatched cluster, and the outage started.\nAdditionally, even though patch B did protect against the kind of input errors observed during testing, the actual race condition produced a different form of error in the configuration, which the completed rollout of patch B did not prevent from being accepted.\nOnce the root cause was identified, our engineers mitigated the issue by restoring a known-good configuration, and completed and verified the fix, which eliminates the risk of recurrence.\n## Service(s) Affected:\n- Google Cloud Networking: Customer HTTP/S endpoints served 404 error pages. During partial recovery, traffic was served, but customers were unable to make changes to their load balancer configurations.\n- GCLB can be used to load balance traffic to a number of other Google Cloud services, which lost traffic because of the outage. Customers who use serverless network endpoint groups on GCLB as a frontend to Google Cloud Run, Google App Engine, Google App Engine Flex, or Google Cloud Functions received 404 errors when attempting to access their service. Customers using Apigee, Firebase, or Google App Engine Flex received 404 errors when attempting to access their service.\n## Zone(s) Affected:\nGlobal\n## How Customers Experienced the Issue:\nBetween 09:35 and 10:08 PT, most endpoints served by global GCLB load balancers returned a 404 error. For an additional 1 hour and 20 minutes, customers were unable to make changes to their load balancing configuration.\n## Workaround(s):\nNone.\nService was restored on 16 November 2021 at 11:28 PT, and the Google Cloud Status Dashboard was updated by 12:08 PT to reflect this.\n## Remediation and Prevention\nWe have fixed the underlying bug and are taking the following actions to prevent recurrence:\nWe immediately added additional alerting, which will notify us to similar issues significantly faster going forward.\nWe are adding safeguards to prevent similar issues from occurring in the future. These safeguards provide strengthened automated correctness-checking to configurations before they are applied.\nWe are accelerating planned architectural changes that will improve how we isolate and resolve such issues in the future.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"},{"title":"Apigee","id":"9Y13BNFy4fJydvjdsN3X"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/6PM5mNd43NbMqjCZ5REh","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"ZoUf49v2qbJ9xRK63kaM","number":"9650429565293316538","begin":"2021-11-13T07:14:48+00:00","created":"2021-11-13T07:14:50+00:00","end":"2021-11-13T08:29:30+00:00","modified":"2021-11-13T08:29:30+00:00","external_desc":"Some Users might have received credit cards deemed invalid email erroneously.","updates":[{"created":"2021-11-13T08:29:30+00:00","modified":"2021-11-13T08:29:30+00:00","when":"2021-11-13T08:29:30+00:00","text":"The issue with Google Compute Engine has been resolved for all affected users as of Saturday, 2021-11-12 23:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-13T07:19:03+00:00","modified":"2021-11-13T07:19:04+00:00","when":"2021-11-13T07:19:03+00:00","text":"Summary: Some Users might have received credit cards deemed invalid email erroneously.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Saturday, 2021-11-13 00:35 US/Pacific.\nWe will provide more information by Saturday, 2021-11-13 00:35 US/Pacific.\nDiagnosis: Users might have received an email stating their payment instrument was bad and in some cases would limit their ability to use Cloud products until corrected. This is impacting majorly users in Canada.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-13T07:14:50+00:00","modified":"2021-11-13T07:14:50+00:00","when":"2021-11-13T07:14:50+00:00","text":"Summary: Some Users might have received credit cards deemed invalid email erroneously.\nDescription: We are experiencing an issue with Cloud Billing / Payments beginning at Friday, 2021-11-12 15:00 US/Pacific.\nIt is mainly impacting credit card customers with billing addresses in Canada.\nOur engineering team continues to investigate the issue. We will provide an update by Friday, 2021-11-12 23:35 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Users might have received an email stating their payment instrument was bad and in some cases would limit their ability to use Cloud products until corrected. This is impacting majorly users in Canada.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T08:29:30+00:00","modified":"2021-11-13T08:29:30+00:00","when":"2021-11-13T08:29:30+00:00","text":"The issue with Google Compute Engine has been resolved for all affected users as of Saturday, 2021-11-12 23:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"L3ggmi3Jy4xJmgodFA9K","service_name":"Google Compute Engine","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"}],"uri":"incidents/ZoUf49v2qbJ9xRK63kaM","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"o44ZUgX95KF11F7cqd5v","number":"16591718137333047668","begin":"2021-11-13T02:45:00+00:00","created":"2021-11-14T00:47:59+00:00","end":"2021-11-14T04:39:00+00:00","modified":"2021-11-15T21:27:23+00:00","external_desc":"us-central1, us-west1, us-west1, us-east1: Cloud Spanner missing stackdriver CPU, memory and disk metrics.","updates":[{"created":"2021-11-15T21:26:47+00:00","modified":"2021-11-15T21:26:47+00:00","when":"2021-11-15T21:26:47+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 18:45\n**Incident End:** 13 November 2021 20:39\n**Duration:** 1 day, 1 hours, 54 minutes\n**Affected Services and Features:**\n- Cloud Spanner - Missing/Incorrect Cloud Monitoring metrics for CPU, memory and disk\n**Regions/Zones:**\n- us-central1\n- us-east1\n- Global\n**Description:**\nCloud Spanner experienced unavailable and incorrect monitoring metrics for instance resources including cpu, memory and disk for a duration of 1 day, 1 hours and 54 minutes. From preliminary analysis, the root cause of the issue is a rollout flag which incorrectly modified how metrics are exported.\n**Customer Impact:**\n- us-central1: Experienced missing metrics during the impact duration until mitigation at 13 November 2021 18:30\n- us-east1: Experienced missing metrics during the impact duration until mitigation at 13 November 2021 17:45\n- All other regions: Many customers globally experienced metrics that incorrectly under-reported actual values until full resolution at 13 November 2021 20:39","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-14T05:17:05+00:00","modified":"2021-11-14T05:17:07+00:00","when":"2021-11-14T05:17:05+00:00","text":"The issue with Cloud Spanner has been resolved for all affected users as of Saturday, 2021-11-13 21:16 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-14T00:47:58+00:00","modified":"2021-11-14T00:48:00+00:00","when":"2021-11-14T00:47:58+00:00","text":"Summary: us-central1, us-west1, us-west1, us-east1: Cloud Spanner missing stackdriver CPU, memory and disk metrics.\nDescription: Mitigation work is currently underway by our engineering team.\nCurrent data indicates that customers in us-central1, us-west1, us-west1 and us-east1 are affected by this issue.\nWe will provide more information by Saturday, 2021-11-13 23:00 US/Pacific.\nDiagnosis: Missing Stackdriver metrics on the dashboard.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-15T21:26:47+00:00","modified":"2021-11-15T21:26:47+00:00","when":"2021-11-15T21:26:47+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 18:45\n**Incident End:** 13 November 2021 20:39\n**Duration:** 1 day, 1 hours, 54 minutes\n**Affected Services and Features:**\n- Cloud Spanner - Missing/Incorrect Cloud Monitoring metrics for CPU, memory and disk\n**Regions/Zones:**\n- us-central1\n- us-east1\n- Global\n**Description:**\nCloud Spanner experienced unavailable and incorrect monitoring metrics for instance resources including cpu, memory and disk for a duration of 1 day, 1 hours and 54 minutes. From preliminary analysis, the root cause of the issue is a rollout flag which incorrectly modified how metrics are exported.\n**Customer Impact:**\n- us-central1: Experienced missing metrics during the impact duration until mitigation at 13 November 2021 18:30\n- us-east1: Experienced missing metrics during the impact duration until mitigation at 13 November 2021 17:45\n- All other regions: Many customers globally experienced metrics that incorrectly under-reported actual values until full resolution at 13 November 2021 20:39","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"EcNGGUgBtBLrtm4mWvqC","service_name":"Cloud Spanner","affected_products":[{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"}],"uri":"incidents/o44ZUgX95KF11F7cqd5v","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"A1HUEnN25xk8XiSmjmsj","number":"14307305938107923640","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T09:54:31+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:12:22+00:00","external_desc":"We are experiencing an issue with multiple Google Cloud components.","updates":[{"created":"2021-11-13T00:50:51+00:00","modified":"2021-11-13T01:12:22+00:00","when":"2021-11-13T00:50:51+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:52:25+00:00","modified":"2021-11-12T11:52:25+00:00","when":"2021-11-12T11:52:25+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T10:11:51+00:00","modified":"2021-11-12T14:05:29+00:00","when":"2021-11-12T10:11:51+00:00","text":"Summary: We are experiencing an issue with .\nmultiple Google Cloud components\nDescription: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: n/a\nWorkaround: n/a","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T09:54:29+00:00","modified":"2021-11-12T14:05:15+00:00","when":"2021-11-12T09:54:29+00:00","text":"Summary: We are experiencing an issue with .\nmultiple Google Cloud components\nDescription: We are experiencing an issue with .\nmultiple Google Cloud components\nOur engineering team continues to investigate the issue.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:50:51+00:00","modified":"2021-11-13T01:12:22+00:00","when":"2021-11-13T00:50:51+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/A1HUEnN25xk8XiSmjmsj","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"1xkAB1KmLrh5g3v9ZEZ7","number":"1942372389113677880","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T09:42:26+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2022-03-28T19:45:30+00:00","external_desc":"We are experiencing Networking issues","updates":[{"created":"2021-11-13T00:43:20+00:00","modified":"2022-03-28T19:45:30+00:00","when":"2021-11-13T00:43:20+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\n* App Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:44:23+00:00","modified":"2021-11-12T11:44:26+00:00","when":"2021-11-12T11:44:23+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:38:17+00:00","modified":"2021-11-12T11:38:20+00:00","when":"2021-11-12T11:38:17+00:00","text":"Summary: We are experiencing Networking issues\nDescription: We believe the issue with Google Cloud infrastructure components is partially resolved with the exception of the Google App Engine that is also recovering.\nWe will provide an update by Friday, 2021-11-12 04:10 US/Pacific with current details.\nDiagnosis: Customers might face connectivity issues.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T10:57:04+00:00","modified":"2021-11-12T10:57:06+00:00","when":"2021-11-12T10:57:04+00:00","text":"Summary: We are experiencing Networking issues\nDescription: Mitigation work is currently underway by our engineering team. We do not have an ETA for mitigation at this point but the services should be improving at this point.\nThe issue is affecting:\n- Cloud App Engine : Customers may see traffic drop for us-central1 and europe-west1.\n- Cloud Bigtable: mitigation still in progress, ETA for resolution still unknown\n- Cloud Monitoring UI: There is a mitigation in place at the GFE infrastructure level that is rolling out and is expected to resolve this issue.\n- Cloud Console: All Cloud Console paths may be unavailable.\n- Cloud Spanner: Customers coming through GFE (not CFE or cloud interconnect) will experience UNAVAILABLE error and latency for both DATA and ADMIN operations\n- Cloud Functions: Customers may see traffic drop for us-central1 and europe-west1.\n- Cloud Run: Cloud Run users are seeing increased HTTP 500s and authentication failures when trying to access apps.\n- Google Cloud Endpoints : Cloud Endpoints may be unavailable in europe-west1 and europe-west4 (most affected regions)\n- Cloud SQL:Regions europe-west1,europe-west4 and europe-west5 (could be more). Workaround: Users should retry failed operations.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 03:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers might face connectivity issues.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T10:36:22+00:00","modified":"2021-11-12T10:36:25+00:00","when":"2021-11-12T10:36:22+00:00","text":"Summary: We are experiencing Networking issues\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nThe issue is affecting,\n- Cloud App Engine : Customers may see traffic drop for us-central1 and europe-west1.\n- Cloud Bigtable\n- Cloud Console : All Cloud Console paths may be unavailable.\n- Cloud Spanner\n- Cloud Functions : Customers may see traffic drop for us-central1 and europe-west1.\n- Cloud Run\n- Google Cloud Endpoints : Cloud Endpoints may be unavailable in some regions.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 03:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers might face connectivity issues.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T10:26:41+00:00","modified":"2021-11-12T10:26:41+00:00","when":"2021-11-12T10:26:41+00:00","text":"Summary: We are experiencing Networking issues\nDescription: We are experiencing an issue with Google Cloud infrastructure components beginning at Friday, 2021-11-12 00:36:24 PST US/Pacific.\nThe issue is affecting,\n- Cloud App Engine : Customers may see traffic drop for us-central1 and europe-west1.\n- Cloud Bigtable\n- Cloud Console : All Cloud Console paths may be unavailable.\n- Cloud Spanner\n- Cloud Functions : Customers may see traffic drop for us-central1 and europe-west1.\n- Cloud Run\n- Google Cloud Endpoints : Cloud Endpoints may be unavailable in some regions.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 02:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers might face connectivity issues.\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T10:13:01+00:00","modified":"2021-11-12T10:13:01+00:00","when":"2021-11-12T10:13:01+00:00","text":"Summary: We are experiencing Networking issues\nDescription: We are experiencing an issue with Google Cloud infrastructure components beginning at Friday, 2021-11-12 00:36:24 PST US/Pacific.\nThe issue is affecting,\n- Cloud App Engine\n- Cloud Bigtable\n- Cloud Console\n- Cloud Spanner\n- Cloud Functions\n- Cloud Run\n- Google Cloud APIs\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 02:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T09:42:25+00:00","modified":"2021-11-12T09:42:26+00:00","when":"2021-11-12T09:42:25+00:00","text":"Summary: We are experiencing Networking issues\nDescription: We are experiencing an issue with Google Cloud infrastructure components beginning at Friday, 2021-11-12 00:36:24 PST US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 02:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:43:20+00:00","modified":"2022-03-28T19:45:30+00:00","when":"2021-11-13T00:43:20+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\n* App Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/1xkAB1KmLrh5g3v9ZEZ7","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"MHS8ukmQamTNoZ9uqHvF","number":"18047497303663607236","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T10:14:42+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:12:49+00:00","external_desc":"Issue with Google Cloud components","updates":[{"created":"2021-11-13T00:41:50+00:00","modified":"2021-11-13T01:12:49+00:00","when":"2021-11-13T00:41:50+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:49:52+00:00","modified":"2021-11-12T11:49:53+00:00","when":"2021-11-12T11:49:52+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T10:14:41+00:00","modified":"2021-11-12T10:14:43+00:00","when":"2021-11-12T10:14:41+00:00","text":"Summary: Issue with Google Cloud components\nDescription: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue. For regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:41:50+00:00","modified":"2021-11-13T01:12:49+00:00","when":"2021-11-13T00:41:50+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/MHS8ukmQamTNoZ9uqHvF","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"8hZjcddmMVe9W1jh8SCN","number":"14767488434731058018","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T10:09:20+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:11:13+00:00","external_desc":"Issue with Google App Engine","updates":[{"created":"2021-11-13T00:37:47+00:00","modified":"2021-11-13T01:11:13+00:00","when":"2021-11-13T00:37:47+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:48:38+00:00","modified":"2021-11-12T11:48:38+00:00","when":"2021-11-12T11:48:38+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T10:09:18+00:00","modified":"2021-11-12T10:09:21+00:00","when":"2021-11-12T10:09:18+00:00","text":"Summary: Issue with Google App Engine\nDescription: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue. For regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:37:47+00:00","modified":"2021-11-13T01:11:13+00:00","when":"2021-11-13T00:37:47+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/8hZjcddmMVe9W1jh8SCN","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"u4oEtmqmJ1fmY8bjzxxa","number":"1802042647669406530","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T10:19:16+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:14:11+00:00","external_desc":"Issue with Google Cloud SQL","updates":[{"created":"2021-11-13T00:32:46+00:00","modified":"2021-11-13T01:14:11+00:00","when":"2021-11-13T00:32:46+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:46:17+00:00","modified":"2021-11-12T11:46:18+00:00","when":"2021-11-12T11:46:17+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T10:19:14+00:00","modified":"2021-11-13T00:34:27+00:00","when":"2021-11-12T10:19:14+00:00","text":"Summary: Issue with Google Cloud SQL\nDescription: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:30 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue. For regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:32:46+00:00","modified":"2021-11-13T01:14:11+00:00","when":"2021-11-13T00:32:46+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/u4oEtmqmJ1fmY8bjzxxa","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"cJwQRK4bgybhu7nNQ5W4","number":"13388333705131857687","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T09:57:58+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:13:29+00:00","external_desc":"Issue with Eventarc","updates":[{"created":"2021-11-13T00:36:10+00:00","modified":"2021-11-13T01:13:29+00:00","when":"2021-11-13T00:36:10+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:47:23+00:00","modified":"2021-11-12T11:47:25+00:00","when":"2021-11-12T11:47:23+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T09:57:56+00:00","modified":"2021-11-12T09:57:58+00:00","when":"2021-11-12T09:57:56+00:00","text":"Summary: Issue with Eventarc\nDescription: beginning at Friday, 2021-11-12 00:36 US/Pacific US/Pacific\nWe are experiencing an issue with Google Cloud components.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7\nDiagnosis: None at this time\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:36:10+00:00","modified":"2021-11-13T01:13:29+00:00","when":"2021-11-13T00:36:10+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"YaFawoMaXnqgY4keUBnW","service_name":"Eventarc","affected_products":[{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"}],"uri":"incidents/cJwQRK4bgybhu7nNQ5W4","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"PgiLpR4P4S1x7QgwXamt","number":"5068446511008051635","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T10:05:08+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:13:11+00:00","external_desc":"We are experiencing an issue with multiple Google Cloud components {beginning on Friday}, 2021-11-12 00:36 US/Pacific US/Pacific.","updates":[{"created":"2021-11-13T00:33:28+00:00","modified":"2021-11-13T01:13:11+00:00","when":"2021-11-13T00:33:28+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:47:44+00:00","modified":"2021-11-12T11:47:45+00:00","when":"2021-11-12T11:47:44+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are still impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T10:05:07+00:00","modified":"2021-11-12T14:03:10+00:00","when":"2021-11-12T10:05:07+00:00","text":"We are experiencing an issue with multiple Google Cloud components {beginning on Friday}, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption\nDiagnosis: None\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:33:28+00:00","modified":"2021-11-13T01:13:11+00:00","when":"2021-11-13T00:33:28+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/PgiLpR4P4S1x7QgwXamt","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"qgC4WYnmToo6fkqEnqYf","number":"7446355211445985074","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T09:43:57+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:13:52+00:00","external_desc":"Issue with Container Analysis API","updates":[{"created":"2021-11-13T00:39:35+00:00","modified":"2021-11-13T01:13:52+00:00","when":"2021-11-13T00:39:35+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:47:57+00:00","modified":"2021-11-12T11:47:59+00:00","when":"2021-11-12T11:47:57+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T09:50:33+00:00","modified":"2021-11-12T14:04:53+00:00","when":"2021-11-12T09:50:33+00:00","text":"Summary: Issue with Container Analysis API\nDescription: We are experiencing an issue with multiple Google Cloud components {beginning on Friday}, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T09:43:56+00:00","modified":"2021-11-12T09:43:58+00:00","when":"2021-11-12T09:43:56+00:00","text":"Summary: Issue with Container Analysis API\nDescription: We are experiencing an issue with Container Analysis API beginning at Friday, 2021-11-12 00:36 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 02:40 US/Pacific with current details.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:39:35+00:00","modified":"2021-11-13T01:13:52+00:00","when":"2021-11-13T00:39:35+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/qgC4WYnmToo6fkqEnqYf","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"4qh2oE7kwsMoMNXScfoT","number":"1318251267977763247","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T09:53:30+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:10:53+00:00","external_desc":"We are experiencing an issue with multiple Google Cloud components {beginning on Friday}, 2021-11-12 00:36 US/Pacific US/Pacific.","updates":[{"created":"2021-11-13T00:37:03+00:00","modified":"2021-11-13T01:10:53+00:00","when":"2021-11-13T00:37:03+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:47:22+00:00","modified":"2021-11-12T11:47:23+00:00","when":"2021-11-12T11:47:22+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T09:54:04+00:00","modified":"2021-11-12T11:06:52+00:00","when":"2021-11-12T09:54:04+00:00","text":"Summary: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nDescription: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T09:53:30+00:00","modified":"2021-11-12T14:03:37+00:00","when":"2021-11-12T09:53:30+00:00","text":"Summary: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None\nWorkaround: None","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:37:03+00:00","modified":"2021-11-13T01:10:53+00:00","when":"2021-11-13T00:37:03+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/4qh2oE7kwsMoMNXScfoT","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"9A7WPjqgreJGbpmA6FqJ","number":"17971436015441651880","begin":"2021-11-12T08:30:00+00:00","created":"2021-11-12T09:40:40+00:00","end":"2021-11-12T10:14:00+00:00","modified":"2021-11-13T01:11:58+00:00","external_desc":"Issue with Cloud Bigtable","updates":[{"created":"2021-11-13T00:32:08+00:00","modified":"2021-11-13T01:11:58+00:00","when":"2021-11-13T00:32:08+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T11:49:05+00:00","modified":"2021-11-12T11:49:05+00:00","when":"2021-11-12T11:49:05+00:00","text":"The issue with Google Cloud infrastructure components has been resolved for all affected products as of Friday, 2021-11-12 03:38 US/Pacific.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we worked on resolving the issue.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-12T09:51:57+00:00","modified":"2021-11-12T14:05:45+00:00","when":"2021-11-12T09:51:57+00:00","text":"Summary: Issue with Cloud Bigtable\nDescription: We are experiencing an issue with multiple Google Cloud components beginning on Friday, 2021-11-12 00:36 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/1xkAB1KmLrh5g3v9ZEZ7, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-11-12T09:40:32+00:00","modified":"2021-11-12T09:40:41+00:00","when":"2021-11-12T09:40:32+00:00","text":"Summary: Issue with Cloud Bigtable\nDescription: We are experiencing an issue with Cloud Bigtable beginning at Friday, 2021-11-12 00:36 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-11-12 02:30 US/Pacific with current details.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-13T00:32:08+00:00","modified":"2021-11-13T01:11:58+00:00","when":"2021-11-13T00:32:08+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 12 November 2021 00:30\n**Incident End:** 12 November 2021 02:14\n**Duration:** 1 hour 44 minutes\n**Affected Services and Features:**\n* Cloud Developer Tools\n* Cloud Endpoints\n* Cloud Eventarc\n* Google App Engine\n* Google Cloud Bigtable\n* Google Cloud Console\n* Google Cloud Infrastructure Components\n* Google Cloud Networking\n* Google Cloud SQL\n* Operations\n**Regions/Zones:** Europe\n**Description:**\nGoogle’s Front End load balancing service experienced failures resulting in impact to several downstream Google Cloud services in Europe. From preliminary analysis, the root cause of the issue was caused by a new infrastructure feature triggering a latent issue within internal network load balancer code.\n**Customer Impact:**\n* Google Cloud Console - Affected customers in Europe were unable to load the console, or experienced timeouts.\n* Container Registry - Affected customers were unable to connect to the service using the API in europe-west1.\n* Cloud Bigtable - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Spanner - Affected customers in Europe experienced unavailable errors and latency for both data and admin operations.\n* Cloud Endpoints - Service was unavailable in europe-west1 and europe-west4.\n* Firebase Messaging - Affected customers experienced issues receiving notifications in asia-southeast1.\n* Cloud SQL - Affected customers experienced probe creation request failures in europe-west1, europe-west4, and europe-west5.\n* Cloud Eventarc - Affected customers were unable to create triggers in europe-west1.\nApp Engine - Affected customers experienced elevated errors in europe-west1.\n**Additional details:**\nThe error was caught within 4 minutes by automated safety systems, and further spread was slowed at this point. The issue was fully mitigated approximately 1hr 44m later, when our engineering team completed a rollout to disable the vulnerable code path. The issue will be fully prevented going forward via a root cause fix, which will complete rollout by 12 November 2021 21:00 US/Pacific.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Endpoints","id":"xfbaRzmPSf836WvAy35z"},{"title":"Eventarc","id":"YaFawoMaXnqgY4keUBnW"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/9A7WPjqgreJGbpmA6FqJ","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"sXRB8Ty5Xzb13t8DCrD8","number":"9131558500250370522","begin":"2021-11-02T16:26:00+00:00","created":"2021-11-02T16:26:09+00:00","end":"2021-11-02T16:47:00+00:00","modified":"2022-03-29T18:32:48+00:00","external_desc":"Global: Cloud Monitoring returning partial results on some queries","updates":[{"created":"2021-11-02T16:47:08+00:00","modified":"2021-11-02T16:47:15+00:00","when":"2021-11-02T16:47:08+00:00","text":"The issue with Cloud Monitoring is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-11-02T16:26:03+00:00","modified":"2021-11-02T16:26:09+00:00","when":"2021-11-02T16:26:03+00:00","text":"Summary: Global: Cloud Monitoring returning partial results on some queries\nDescription: We are experiencing an issue with Cloud Monitoring beginning at Monday, 2021-11-01 07:45 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-11-02 10:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Affected customers may see partial or incomplete results when running some queries.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-02T16:47:08+00:00","modified":"2021-11-02T16:47:15+00:00","when":"2021-11-02T16:47:08+00:00","text":"The issue with Cloud Monitoring is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"}],"uri":"incidents/sXRB8Ty5Xzb13t8DCrD8","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"aCXLk4u6GJoLn9PqSMm1","number":"13045675828271798087","begin":"2021-11-02T12:13:00+00:00","created":"2021-12-07T21:14:09+00:00","end":"2021-12-16T20:25:00+00:00","modified":"2021-12-20T20:29:09+00:00","external_desc":"Global: Issues with Cloud SQL for MySQL instance migration to 5.7 when source databases have gtid_mode set to ON.","updates":[{"created":"2021-12-20T20:28:34+00:00","modified":"2021-12-20T20:28:34+00:00","when":"2021-12-20T20:28:34+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 02 November 2021 04:13\n**Incident End:** 16 December 2021 12:25\n**Duration:** 44 days, 8 hours, 12 minutes\n**Affected Services and Features:**\nCloudSQL- MySQL instance migration.\n**Regions/Zones:** Global\n**Description:**\nCloudSQL experienced an issue with MySQL instance migration to 5.7.36 with source database gtid_mode set to ON for a duration of 44 days, 8 hours, 12 minutes. The root cause of the issue is a MySQL bug https://bugs.mysql.com/bug.php?id=105761 that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021, from a source database that had gtid_mode set to ON.\n**Customer Impact:**\nCloud SQL External Server or Database Migration Service (DMS) users who migrated to Cloud SQL for MySQL 5.7.36, with replication using managed dumps may have experienced replication failures if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those jobs during the replication phase, the migration job may have failed.\nThe issue was observed when ALL the following conditions are met:\n* Cloud SQL MySQL version migrated to is 5.7.36.\n* Source database gtid_mode is ON.\n* Customer is running continuous migration (see DMS doc https://cloud.google.com/database-migration/docs/mysql/create-migration-job).\n* Customer is not migrating with a file, i.e. migration is using a managed dump.\nNew instances should not experience this issue, but please follow the below recommended workaround for existing instances that are impacted. We will be sending out targeted communications to the affected customers.\n***Workaround:*** Customers should start a new migration by dumping the data from a source database instance running MySQL 5.7.35 or lower versions to Google Cloud Storage (GCS) manually (Cloud SQL, DMS), using a mysqldump file, and then migrate using the dump file to Cloud SQL for MySQL 5.7.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-16T21:13:01+00:00","modified":"2021-12-16T21:13:04+00:00","when":"2021-12-16T21:13:01+00:00","text":"The issue with Cloud SQL has been resolved as of Thursday, 2021-12-16 12:25 US/Pacific.\nNew instances will no longer experience this issue, but please follow the recommended workaround for existing instances that are impacted. We will be sending out targeted communications to the affected customers.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-12-13T22:00:09+00:00","modified":"2021-12-13T22:00:12+00:00","when":"2021-12-13T22:00:09+00:00","text":"Summary: Global: Issues with Cloud SQL for MySQL instance migration to 5.7 when source databases have gtid_mode set to ON.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete over the next few days.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL for MySQL 5.7 with GTID replication using managed dumps may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those jobs during the replication phase, the migration job may fail.\nWorkaround: Customers should start a new migration by dumping the data from a source database instance running MySQL 5.7.35 or lower versions to Google Cloud Storage (GCS) manually (Cloud SQL, DMS), using a mysqldump file, and then migrate using the dump file to Cloud SQL for MySQL 5.7.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-09T06:04:17+00:00","modified":"2021-12-09T06:04:17+00:00","when":"2021-12-09T06:04:17+00:00","text":"Summary: Global: Issues with Cloud SQL for MySQL instance migration to 5.7 when source databases have gtid_mode set to ON.\nDescription: Our engineering team continues to investigate the issue caused by a MySQL bug https://bugs.mysql.com/bug.php?id=105761 that may affect customers who migrated to MySQL 5.7.36 on or after November 2nd 2021, from a source database that had gtid_mode set to ON.\nThe issue is being observed when ALL of the following conditions are met:\n1. Cloud SQL for MySQL version migrated to is 5.7.36\n2. source database gtid_mode is ON\n3. The migration was a continuous migration (see DMS doc https://cloud.google.com/database-migration/docs/mysql/configure-source-database)\n4. The migration was performed using an auto-generated initial dump, also called a managed dump.\nWe will provide further updates by Monday, 2021-12-13 17:00 US/Pacific with details.\nIn the meantime, we recommend halting any migrations to Cloud SQL for MySQL 5.7 where the source database has gtid_mode set to ON and is using managed dumps, to prevent this scenario from occurring. The workaround outlined below may be used for these migrations instead.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL for MySQL 5.7 with GTID replication using managed dumps may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those jobs during the replication phase, the migration job may fail.\nWorkaround: Customers should start a new migration by dumping the data from a source database instance running MySQL 5.7.35 or lower versions to Google Cloud Storage (GCS) manually (Cloud SQL, DMS), using a mysqldump file, and then migrate using the dump file to Cloud SQL for MySQL 5.7.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-09T01:21:58+00:00","modified":"2021-12-09T01:21:59+00:00","when":"2021-12-09T01:21:58+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36 from source databases that have gtid_mode set to ON.\nDescription: Our engineering team continues to investigate the issue caused by a MySQL bug https://bugs.mysql.com/bug.php?id=105761 that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021, from a source database that had gtid_mode set to ON.\nThe issue is being observed when ALL of the following conditions are met:\n1. Cloud SQL MySQL version migrated to is 5.7.36\n2. source database gtid_mode is ON\n3. Customer is running continuous migration (see DMS doc https://cloud.google.com/database-migration/docs/postgres/create-migration-job)\n4. Customer is not migrating with a file, i.e. migration is using a managed dump.\nWe will provide further updates by Monday, 2021-12-13 17:00 US/Pacific with details.\nIn the meantime, we recommend halting any migrations to Cloud SQL MySQL version 5.7 where the source database has gtid_mode set to ON to prevent this scenario from occurring. The workaround outlined below may be used for these migrations.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL MySQL 5.7 with GTID replication may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those jobs during the replication phase, the migration job may fail.\nWorkaround: Customers should start a new migration by dumping the data from source MySQL 5.7.35 or lower versions to Google Cloud Storage (GCS) manually (Cloud SQL, DMS), using mysqldump, and then migrate using the file to Cloud SQL MySQL 5.7 version.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-08T21:59:06+00:00","modified":"2021-12-08T21:59:12+00:00","when":"2021-12-08T21:59:06+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36.\nDescription: Our engineering team continues to investigate the issue that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021\nWe will provide an update by Wednesday, 2021-12-08 17:00 US/Pacific with current details.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL MySQL 5.7 with GTID replication may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those data during replication phase, the migration job may fail.\nWorkaround: Customers may need to start a new migration by dumping the data from source mysql 5.7.35 or lower versions to Google Cloud Storage (GCS) manually, using mysqldump, and then migrate the file to destination mysql version.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-08T06:28:36+00:00","modified":"2021-12-08T06:28:37+00:00","when":"2021-12-08T06:28:36+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36.\nDescription: Our engineering team continues to investigate the issue that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021\nWe will provide an update by Wednesday, 2021-12-08 14:00 US/Pacific with current details.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL MySQL 5.7 with GTID replication may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those data during replication phase, the migration job may fail.\nWorkaround: Customers may need to start a new migration by dumping the data from source mysql 5.7.35 or lower versions to Google Cloud Storage (GCS) manually, using mysqldump, and then migrate the file to destination mysql version.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-08T03:31:31+00:00","modified":"2021-12-08T03:31:32+00:00","when":"2021-12-08T03:31:31+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36.\nDescription: Our engineering team continues to investigate the issue that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021\nWe will provide an update by Tuesday, 2021-12-07 23:30 US/Pacific with current details.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL MySQL 5.7 with GTID replication may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those data during replication phase, the migration job may fail.\nWorkaround: Customers may need to start a new migration by dumping the data from source mysql 5.7.35 or lower versions to Google Cloud Storage (GCS) manually, using mysqldump, and then migrate the file to destination mysql version.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-08T00:35:07+00:00","modified":"2021-12-08T00:35:08+00:00","when":"2021-12-08T00:35:07+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36.\nDescription: Our engineering team continues to investigate the issue that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021\nWe will provide an update by Tuesday, 2021-12-07 19:30 US/Pacific with current details.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL MySQL 5.7 with GTID replication may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those data during replication phase, the migration job may fail.\nWorkaround: Customers may need to start a new migration by dumping the data from source mysql 5.7.35 or lower versions to Google Cloud Storage (GCS) manually, using mysqldump, and then migrate the file to destination mysql version.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-07T22:36:00+00:00","modified":"2021-12-07T22:36:01+00:00","when":"2021-12-07T22:36:00+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36\nDescription: Our engineering team continues to investigate the issue that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021.\nWe will provide an update by Tuesday, 2021-12-07 16:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who migrate to Cloud SQL MySQL 5.7 with GTID replication may experience errors if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those data during replication phase, the migration job may fail.\nWorkaround: Customers need to start a new migration by dumping the data from source mysql 5.7.35 or lower versions to Google Cloud Storage (GCS) manually, using mysqldump, and then migrate the file to destination mysql version.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-12-07T21:14:09+00:00","modified":"2021-12-07T21:14:10+00:00","when":"2021-12-07T21:14:09+00:00","text":"Summary: Global: Issues with Cloud SQL migrating to MySQL version 5.7.36\nDescription: We are experiencing an issue with Cloud SQL when migrating to MySQL version 5.7.36.\nThe issue may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-12-07 14:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Cloud SQL External Server or Database Migration Service (DMS) users who want to migrate to Cloud SQL MySQL 5.7 with GTID replication may experience data inconsistency (i.e. data in Cloud SQL may not be consistent with data on the source) issue if they have insert/update/delete transactions during the dump phase. Further more, if customer has updates to those inconsistent data during replication phase, the migration job may fail.\nWorkaround: Customers needs to start a new migration by dumping the data from source by themselves to Google Cloud Storage (GCS) and then migrate with the file.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-12-20T20:28:34+00:00","modified":"2021-12-20T20:28:34+00:00","when":"2021-12-20T20:28:34+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 02 November 2021 04:13\n**Incident End:** 16 December 2021 12:25\n**Duration:** 44 days, 8 hours, 12 minutes\n**Affected Services and Features:**\nCloudSQL- MySQL instance migration.\n**Regions/Zones:** Global\n**Description:**\nCloudSQL experienced an issue with MySQL instance migration to 5.7.36 with source database gtid_mode set to ON for a duration of 44 days, 8 hours, 12 minutes. The root cause of the issue is a MySQL bug https://bugs.mysql.com/bug.php?id=105761 that may affect customers who migrated to MySQL version 5.7.36 on or after November 2nd 2021, from a source database that had gtid_mode set to ON.\n**Customer Impact:**\nCloud SQL External Server or Database Migration Service (DMS) users who migrated to Cloud SQL for MySQL 5.7.36, with replication using managed dumps may have experienced replication failures if they have insert/update/delete transactions during the dump phase. Furthermore, if updates were made to those jobs during the replication phase, the migration job may have failed.\nThe issue was observed when ALL the following conditions are met:\n* Cloud SQL MySQL version migrated to is 5.7.36.\n* Source database gtid_mode is ON.\n* Customer is running continuous migration (see DMS doc https://cloud.google.com/database-migration/docs/mysql/create-migration-job).\n* Customer is not migrating with a file, i.e. migration is using a managed dump.\nNew instances should not experience this issue, but please follow the below recommended workaround for existing instances that are impacted. We will be sending out targeted communications to the affected customers.\n***Workaround:*** Customers should start a new migration by dumping the data from a source database instance running MySQL 5.7.35 or lower versions to Google Cloud Storage (GCS) manually (Cloud SQL, DMS), using a mysqldump file, and then migrate using the dump file to Cloud SQL for MySQL 5.7.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"hV87iK5DcEXKgWU2kDri","service_name":"Google Cloud SQL","affected_products":[{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"}],"uri":"incidents/aCXLk4u6GJoLn9PqSMm1","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"aFfDakd653GfsyCBJnZq","number":"3605126490861261353","begin":"2021-10-27T09:00:00+00:00","created":"2021-10-28T18:33:03+00:00","end":"2021-10-30T19:00:00+00:00","modified":"2021-11-01T19:27:39+00:00","external_desc":"Multi-region/US: BigQuery queries may show latencies during execution.","updates":[{"created":"2021-11-01T19:27:36+00:00","modified":"2021-11-01T19:27:36+00:00","when":"2021-11-01T19:27:36+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 27 Oct 2021 02:00\n**Incident End:** 30 Oct 2021 12:00\n**Duration:** 82 hours\n**Affected Services and Features:**\nGoogle BigQuery\n**Regions/Zones:** US Multi-region\n**Description:**\nGoogle BigQuery service performance experienced an increase in latency for queries in multiple regions in the US for 82 hours. From preliminary analysis, the root cause of the issue was capacity increase that caused rebalancing of workload and increased the scheduler processing time. This caused delays in query execution.\n**Customer Impact:** - Queries exhibited higher than normal latency. - Customers were unable to fully utilize their reserved slots.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-29T00:58:59+00:00","modified":"2021-10-29T00:59:00+00:00","when":"2021-10-29T00:58:59+00:00","text":"The issue with BigQuery queries showing latencies, has been resolved for all affected projects as of Thursday, 2021-10-28 17:45 US/Pacific.\nIf you have any further questions or are still impacted, please open a case with the Support Team and we will work with you until the issue is resolved.\nWe thank you for your patience while we worked towards resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-28T23:01:30+00:00","modified":"2021-10-28T23:01:31+00:00","when":"2021-10-28T23:01:30+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work is progressing and our Engineering teams continue to monitor the system performance.\nWe do not have an ETA for mitigation completion at this moment.\nWe will provide more information by Thursday, 2021-10-28 18:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-28T21:49:53+00:00","modified":"2021-10-28T21:49:53+00:00","when":"2021-10-28T21:49:53+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work is progressing and our Engineering teams continue to monitor the system performance.\nWe do not have an ETA for mitigation completion at this moment.\nWe will provide more information by Thursday, 2021-10-28 16:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-28T20:18:06+00:00","modified":"2021-10-28T20:18:07+00:00","when":"2021-10-28T20:18:06+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Engineering has confirmed that the mitigation work is progressing and we are continuing to monitor the system performance.\nWe do not have an ETA for the mitigation of the impact.\nWe will provide more information by Thursday, 2021-10-28 15:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-28T18:33:02+00:00","modified":"2021-10-28T18:33:03+00:00","when":"2021-10-28T18:33:02+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work is still underway by our engineering team. The issue is affecting the US region and is currently under investigation.\nWe do not have an ETA for the mitigation of the impact.\nWe will provide more information by Thursday, 2021-10-28 13:30 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-01T19:27:36+00:00","modified":"2021-11-01T19:27:36+00:00","when":"2021-11-01T19:27:36+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 27 Oct 2021 02:00\n**Incident End:** 30 Oct 2021 12:00\n**Duration:** 82 hours\n**Affected Services and Features:**\nGoogle BigQuery\n**Regions/Zones:** US Multi-region\n**Description:**\nGoogle BigQuery service performance experienced an increase in latency for queries in multiple regions in the US for 82 hours. From preliminary analysis, the root cause of the issue was capacity increase that caused rebalancing of workload and increased the scheduler processing time. This caused delays in query execution.\n**Customer Impact:** - Queries exhibited higher than normal latency. - Customers were unable to fully utilize their reserved slots.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/aFfDakd653GfsyCBJnZq","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"b29RZjsCa4Xuq874upMd","number":"16629709727814171895","begin":"2021-10-27T09:00:00+00:00","created":"2021-10-29T19:56:52+00:00","end":"2021-10-30T19:00:00+00:00","modified":"2021-11-01T19:26:57+00:00","external_desc":"Multi-region/US: BigQuery queries may show latencies during execution.","updates":[{"created":"2021-11-01T19:26:55+00:00","modified":"2021-11-01T19:26:55+00:00","when":"2021-11-01T19:26:55+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 27 Oct 2021 02:00\n**Incident End:** 30 Oct 2021 12:00\n**Duration:** 82 hours\n**Affected Services and Features:**\nGoogle BigQuery\n**Regions/Zones:** US Multi-region\n**Description:**\nGoogle BigQuery service performance experienced an increase in latency for queries in multiple regions in the US for 82 hours. From preliminary analysis, the root cause of the issue was capacity increase that caused rebalancing of workload and increased the scheduler processing time. This caused delays in query execution.\n**Customer Impact:** - Queries exhibited higher than normal latency. - Customers were unable to fully utilize their reserved slots.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-31T02:42:09+00:00","modified":"2021-10-31T02:42:10+00:00","when":"2021-10-31T02:42:09+00:00","text":"The issue with Google BigQuery has been resolved for all affected projects as of Saturday, 2021-10-30 16:30 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-30T22:43:07+00:00","modified":"2021-10-30T22:43:13+00:00","when":"2021-10-30T22:43:07+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work has completed and our engineering team is continuing to monitor to confirm that the issue has been resolved.\nWe will provide more information by Sunday, 2021-10-31 10:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-30T16:37:46+00:00","modified":"2021-10-30T16:37:47+00:00","when":"2021-10-30T16:37:46+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work is underway and our engineering team continues to monitor progress.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Saturday, 2021-10-30 16:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-30T00:49:48+00:00","modified":"2021-10-30T00:49:50+00:00","when":"2021-10-30T00:49:48+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work continues to be underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Saturday, 2021-10-30 10:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-29T22:55:42+00:00","modified":"2021-10-29T22:55:43+00:00","when":"2021-10-29T22:55:42+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-10-29 18:00 US/Pacific.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-29T20:52:45+00:00","modified":"2021-10-29T20:52:46+00:00","when":"2021-10-29T20:52:45+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: Engineering team continues to investigate the root cause and are currently working on the mitigation steps.\nWe will provide an update by Friday, 2021-10-29 16:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-29T19:56:46+00:00","modified":"2021-10-29T19:56:52+00:00","when":"2021-10-29T19:56:46+00:00","text":"Summary: Multi-region/US: BigQuery queries may show latencies during execution.\nDescription: We are experiencing an issue with Google BigQuery.\nOur engineering team continues to investigate the issue and is working to mitigate the impact for our customers.\nWe will provide an update by Friday, 2021-10-29 14:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some queries may may exhibit higher than normal latencies during execution.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-11-01T19:26:55+00:00","modified":"2021-11-01T19:26:55+00:00","when":"2021-11-01T19:26:55+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 27 Oct 2021 02:00\n**Incident End:** 30 Oct 2021 12:00\n**Duration:** 82 hours\n**Affected Services and Features:**\nGoogle BigQuery\n**Regions/Zones:** US Multi-region\n**Description:**\nGoogle BigQuery service performance experienced an increase in latency for queries in multiple regions in the US for 82 hours. From preliminary analysis, the root cause of the issue was capacity increase that caused rebalancing of workload and increased the scheduler processing time. This caused delays in query execution.\n**Customer Impact:** - Queries exhibited higher than normal latency. - Customers were unable to fully utilize their reserved slots.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/b29RZjsCa4Xuq874upMd","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"gFhaHLtkkPrs5N5dNMRG","number":"8757460366731879771","begin":"2021-10-27T02:58:00+00:00","created":"2021-10-27T04:14:23+00:00","end":"2021-10-27T06:07:00+00:00","modified":"2021-10-27T18:26:05+00:00","external_desc":"Cloud logs might be unavailable for 25% of customers in global location.","updates":[{"created":"2021-10-27T18:26:05+00:00","modified":"2021-10-27T18:26:05+00:00","when":"2021-10-27T18:26:05+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 26 October 2021 19:58\n**Incident End:** 26 October 2021 23:07\n**Duration:** 3 hours, 9 minutes\n**Affected Services and Features:**\n- Google Cloud Logging - Retrieval\n**Regions/Zones:** Global\n**Description:**\nSome Google Cloud Logging customers experienced unavailable logs for a duration of 3 hours and 9 minutes. From preliminary analysis, the root cause of the issue was an underlying file system upgrade which required an expensive re-initialization operation.\n**Customer Impact:**\n- 25% of projects were unable to query their logging data and queries would either time out or return a continuation token.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-27T04:50:03+00:00","modified":"2021-10-27T04:50:04+00:00","when":"2021-10-27T04:50:03+00:00","text":"The issue with Cloud Logging is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-27T04:49:10+00:00","modified":"2021-10-27T04:49:10+00:00","when":"2021-10-27T04:49:10+00:00","text":"Summary: Cloud logs might be unavailable for 25% of customers in global location\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2021-10-26 22:40 US/Pacific.\nWe will provide an update by Tuesday, 2021-10-26 22:52 US/Pacific with current details.\nDiagnosis: Logs won't be available, high latency when reading data.\nWorkaround: Unknown","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-27T04:14:22+00:00","modified":"2021-10-27T04:14:23+00:00","when":"2021-10-27T04:14:22+00:00","text":"Summary: Cloud logs might be unavailable for 25% of customers in global location\nDescription: We are experiencing an issue with Cloud Logging beginning at Tuesday, 2021-10-26 19:57 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-10-26 21:52 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Logs won't be available, high latency when reading data.\nWorkaround: Unknown","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-27T18:26:05+00:00","modified":"2021-10-27T18:26:05+00:00","when":"2021-10-27T18:26:05+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 26 October 2021 19:58\n**Incident End:** 26 October 2021 23:07\n**Duration:** 3 hours, 9 minutes\n**Affected Services and Features:**\n- Google Cloud Logging - Retrieval\n**Regions/Zones:** Global\n**Description:**\nSome Google Cloud Logging customers experienced unavailable logs for a duration of 3 hours and 9 minutes. From preliminary analysis, the root cause of the issue was an underlying file system upgrade which required an expensive re-initialization operation.\n**Customer Impact:**\n- 25% of projects were unable to query their logging data and queries would either time out or return a continuation token.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/gFhaHLtkkPrs5N5dNMRG","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"sCsR34eVfCzGQMGssusu","number":"16628408258355527882","begin":"2021-10-20T15:06:00+00:00","created":"2021-10-20T15:27:14+00:00","end":"2021-10-21T16:27:00+00:00","modified":"2022-03-29T18:43:47+00:00","external_desc":"Global: Issues with populating a UI element in Cloud Console for Cloud Run, Google Container Registry and Artifacts Registry","updates":[{"created":"2021-10-21T19:15:19+00:00","modified":"2021-10-21T19:15:19+00:00","when":"2021-10-21T19:15:19+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 04 October 2021 16:20\n**Incident End:** 20 October 2021 11:24\n**Duration:** 15 days, 19 hours, 4 minutes\n**Affected Services and Features:**\nGoogle Cloud Build, Google Cloud Run, Google Container Registry, Artifacts Registry\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Run, Google Container Registry, and Artifacts Registry experienced issues displaying build and source provenance information in Cloud Console for 15 days, 19 hours, and 4 minutes. From preliminary analysis, the root cause of the issue was related to a recent deployment that introduced a new type of provenance and deprecated an old type. Engineers rolled back the deployment to resolve the issue.\n**Customer Impact:**\nBuild and source provenance for Cloud Build, Cloud Run, Container Registry, and Artifacts Registry were not being populated in Google Cloud Console.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-21T16:27:08+00:00","modified":"2021-10-21T16:27:09+00:00","when":"2021-10-21T16:27:08+00:00","text":"The issue with Container Analysis API has been resolved for all affected users as of Thursday, 2021-10-21 09:27 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-20T19:02:13+00:00","modified":"2021-10-20T19:02:14+00:00","when":"2021-10-20T19:02:13+00:00","text":"Summary: Global: Issues with populating a UI element in Cloud Console for Cloud Run, Google Container Registry and Artifacts Registry\nDescription: Mitigation work is currently underway by our engineering team\nThe mitigation is expected to complete by Thursday, 2021-10-21 and\nWe will provide more information by Thursday, 2021-10-21 12:00 US/Pacific.\nDiagnosis: Customers were able to look in the Cloud Run, Google Container Registry and Artifacts Registry UIs and see the build and source provenance info previously. This information is no longer being populated.\nWorkaround: For Cloud Run, there is no known workaround.\nFor Google Container Registry and Artifacts Registry, if you have a Cloud Build ID, you can use `gcloud build describe","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T17:38:43+00:00","modified":"2021-10-20T17:38:43+00:00","when":"2021-10-20T17:38:43+00:00","text":"Summary: Issues with Cloud Run - Mitigation start\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2021-10-20 12:30 US/Pacific.\nDiagnosis: Cloud customers using Cloud Run were able to look back in the UI and see the software source location of their Cloud Run runs, but they can no longer do so.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T16:55:54+00:00","modified":"2021-10-20T16:55:55+00:00","when":"2021-10-20T16:55:54+00:00","text":"Summary: Issues with Cloud Run - Mitigation start\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2021-10-20 10:48 US/Pacific.\nDiagnosis: Cloud customers using Cloud Run were able to look back in the UI and see the software source location of their Cloud Run runs, but they can no longer do so.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T15:57:47+00:00","modified":"2021-10-20T15:57:53+00:00","when":"2021-10-20T15:57:47+00:00","text":"Summary: Issues with Cloud Run - Our Engineering team is investigating the issue.\nDescription: We are experiencing an issue with Container Analysis API beginning at Wednesday, 2021-10-20 08:45:56 PDT\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-20 10:00 US/Pacific with current details.\nDiagnosis: Cloud customers using Cloud Run were able to look back in the UI and see the software source location of their Cloud Run runs, but they can no longer do so.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T15:30:54+00:00","modified":"2021-10-20T15:30:57+00:00","when":"2021-10-20T15:30:54+00:00","text":"Summary: Issues with Cloud Run - Our Engineering team is investigating the issue.\nDescription: We are experiencing an issue with Container Analysis API beginning at Wednesday, 2021-10-20 08:45:56 PDT\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-20 08:59 US/Pacific with current details.\nDiagnosis: Cloud customers using Cloud Run were able to look back in the UI and see the software source location of their Cloud Run runs, but they can no longer do so.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T15:27:11+00:00","modified":"2021-10-20T15:27:14+00:00","when":"2021-10-20T15:27:11+00:00","text":"Summary: Issues with Cloud Run - Our Engineering team is investigating the issue.\nDescription: We are experiencing an issue with Container Analysis API beginning at Wednesday, 2021-10-20 08:06:56 PDT\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-20 08:59 US/Pacific with current details.\nDiagnosis: Cloud customers using Cloud Run were able to look back in the UI and see the software source location of their Cloud Run runs, but they can no longer do so.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-21T19:15:19+00:00","modified":"2021-10-21T19:15:19+00:00","when":"2021-10-21T19:15:19+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 04 October 2021 16:20\n**Incident End:** 20 October 2021 11:24\n**Duration:** 15 days, 19 hours, 4 minutes\n**Affected Services and Features:**\nGoogle Cloud Build, Google Cloud Run, Google Container Registry, Artifacts Registry\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Run, Google Container Registry, and Artifacts Registry experienced issues displaying build and source provenance information in Cloud Console for 15 days, 19 hours, and 4 minutes. From preliminary analysis, the root cause of the issue was related to a recent deployment that introduced a new type of provenance and deprecated an old type. Engineers rolled back the deployment to resolve the issue.\n**Customer Impact:**\nBuild and source provenance for Cloud Build, Cloud Run, Container Registry, and Artifacts Registry were not being populated in Google Cloud Console.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"},{"title":"Container Registry","id":"rECkDEKEadgZQkPefibM"},{"title":"Artifact Registry","id":"QbBuuiRdsLpMr9WmGwm5"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"}],"uri":"incidents/sCsR34eVfCzGQMGssusu","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"CcVrekxu6kxWYTuitVf4","number":"17775637427950719118","begin":"2021-10-20T09:00:00+00:00","created":"2021-10-20T15:52:09+00:00","end":"2021-10-20T16:30:00+00:00","modified":"2021-10-21T21:56:05+00:00","external_desc":"us-central1-c: GKE experiencing issues with some cluster and nodepool operations. Mitigation Underway.","updates":[{"created":"2021-10-21T21:55:02+00:00","modified":"2021-10-21T21:55:02+00:00","when":"2021-10-21T21:55:02+00:00","text":"We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\nFirst Impact\n**Incident Start:** 19 October 2021 11:40\n**Incident End:** 19 October 2021 16:21\n**Duration:** 4 hours, 41 minutes\nSecond Impact\n**Incident Start:** 20 October 2021 02:00\n**Incident End:** 20 October 2021 09:30\n**Duration:** 7 hours, 30 minutes\n**Affected Services and Features:**\nGoogle Kubernetes Engine\n**Regions/Zones:** us-central1-c\n**Description:**\nGoogle Kubernetes Engine experienced two impacts to operations, the first on 19 October 2021 and the second on 20 October 2021.\nFirst Impact: Customers may have experienced up to 100% failure rate for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 4 hours and 41 minutes. From preliminary analysis, the root cause was resource contention related to an unexpected increase in API operations. Engineers scaled up instances to mitigate the issue.\nSecond Impact: Customers may have experienced up to 80% failure for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 7 hours and 30 minutes. From preliminary analysis, the root case was created by an unexpected increase of nodepool operations in a single customer cluster. Engineers mitigated the issue through additional quota enforcement.\n**Customer Impact:**\nCustomers affected would have experienced 500/503 errors for create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-20T22:38:41+00:00","modified":"2021-10-20T22:38:41+00:00","when":"2021-10-20T22:38:41+00:00","text":"The issue with GKE cluster and nodepool operations has been resolved for all affected projects as of Wednesday, 2021-10-20 15:21 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-20T20:35:57+00:00","modified":"2021-10-20T20:35:58+00:00","when":"2021-10-20T20:35:57+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations.\nDescription: Our engineering team continues to work on the resolution of the issue.\nWe do not have an ETA for full resolution at this point.\nCustomers should see improvement in the cluster and nodepool operations completion.\nWe will provide an update by Wednesday, 2021-10-20 15:30 US/Pacific with current details.\nDiagnosis: The following operations may fail with 500/503 errors: create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T18:34:01+00:00","modified":"2021-10-20T18:34:02+00:00","when":"2021-10-20T18:34:01+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations.\nDescription: We believe the issue with Google Kubernetes Engine is partially resolved.\nCustomers should see improvement in the cluster and nodepool operations completion.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Wednesday, 2021-10-20 13:45 US/Pacific with current details.\nDiagnosis: The following operations may fail with 500/503 errors: create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T17:13:08+00:00","modified":"2021-10-20T17:13:09+00:00","when":"2021-10-20T17:13:08+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations.\nDescription: We believe the issue with Google Kubernetes Engine is partially resolved.\nCustomers should see improvement in the cluster and nodepool operations completion.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Wednesday, 2021-10-20 11:45 US/Pacific with current details.\nDiagnosis: The following operations may fail with 500/503 errors: create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T16:41:11+00:00","modified":"2021-10-20T16:41:12+00:00","when":"2021-10-20T16:41:11+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations.\nDescription: We are experiencing an issue with Google Kubernetes Engine beginning at Wednesday, 2021-10-20 08:30 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-20 10:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: The following operations may fail with 500/503 errors: create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T15:52:04+00:00","modified":"2021-10-20T15:52:09+00:00","when":"2021-10-20T15:52:04+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations.\nDescription: We are experiencing an issue with Google Kubernetes Engine beginning at Wednesday, 2021-10-20 08:30 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-20 09:42 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: The following operations may fail with 500/503 errors: create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-21T21:55:02+00:00","modified":"2021-10-21T21:55:02+00:00","when":"2021-10-21T21:55:02+00:00","text":"We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\nFirst Impact\n**Incident Start:** 19 October 2021 11:40\n**Incident End:** 19 October 2021 16:21\n**Duration:** 4 hours, 41 minutes\nSecond Impact\n**Incident Start:** 20 October 2021 02:00\n**Incident End:** 20 October 2021 09:30\n**Duration:** 7 hours, 30 minutes\n**Affected Services and Features:**\nGoogle Kubernetes Engine\n**Regions/Zones:** us-central1-c\n**Description:**\nGoogle Kubernetes Engine experienced two impacts to operations, the first on 19 October 2021 and the second on 20 October 2021.\nFirst Impact: Customers may have experienced up to 100% failure rate for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 4 hours and 41 minutes. From preliminary analysis, the root cause was resource contention related to an unexpected increase in API operations. Engineers scaled up instances to mitigate the issue.\nSecond Impact: Customers may have experienced up to 80% failure for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 7 hours and 30 minutes. From preliminary analysis, the root case was created by an unexpected increase of nodepool operations in a single customer cluster. Engineers mitigated the issue through additional quota enforcement.\n**Customer Impact:**\nCustomers affected would have experienced 500/503 errors for create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LCSbT57h59oR4W98NHuz","service_name":"Google Kubernetes Engine","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/CcVrekxu6kxWYTuitVf4","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"AB9DV685UZYD2Y7vDQ36","number":"17544221228110196672","begin":"2021-10-19T23:19:00+00:00","created":"2021-10-20T00:59:49+00:00","end":"2021-10-20T09:00:00+00:00","modified":"2022-03-29T14:17:12+00:00","external_desc":"US Multi-Regions: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway.","updates":[{"created":"2021-10-20T23:09:38+00:00","modified":"2021-10-20T23:09:38+00:00","when":"2021-10-20T23:09:38+00:00","text":"Please refer to the following for the mini incident report:\nhttps://status.cloud.google.com/incidents/m617oqASdckYoegRtPJr","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-20T09:00:18+00:00","modified":"2021-10-20T09:00:18+00:00","when":"2021-10-20T09:00:18+00:00","text":"The issue with Cloud Logging has been resolved for all affected users as of Tuesday, 2021-10-19 18:17:58 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-20T07:09:56+00:00","modified":"2021-10-20T07:09:57+00:00","when":"2021-10-20T07:09:56+00:00","text":"Summary: US Multi-Regions: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway.\nDescription: Mitigation work is still underway by our engineering team.\nCurrent status of Mitigation:\nus-west1: Mitigated.\nus-central1: In progress\nus-east1: In progress\nWe will provide more information by Wednesday, 2021-10-20 02:00 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T04:59:07+00:00","modified":"2021-10-20T04:59:08+00:00","when":"2021-10-20T04:59:07+00:00","text":"Summary: US Multi-Regions: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway.\nDescription: Mitigation work is still underway by our engineering team.\nCurrent status of Mitigation:\nus-west1: Mitigated.\nus-central1: In progress\nus-east1: In progress\nWe will provide more information by Wednesday, 2021-10-20 00:10 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T01:58:26+00:00","modified":"2021-10-20T01:58:26+00:00","when":"2021-10-20T01:58:26+00:00","text":"Summary: US Multi-Regions: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway.\nDescription: Mitigation work is still underway by our engineering team. The mitigation is expected to complete by Tuesday, 2021-10-20 00:00 US/Pacific.\nCurrent status of Mitigation:\nus-west1: Mitigated.\nus-central1: In progress\nus-east1: In progress\nWe will provide more information by Tuesday, 2021-10-19 22:00 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-20T00:59:49+00:00","modified":"2021-10-20T00:59:50+00:00","when":"2021-10-20T00:59:49+00:00","text":"Summary: US Multi-Regions: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway.\nDescription: Mitigation work is underway by our engineering team. We do not have an ETA for mitigation at the moment.\nus-west1: Mitigated.\nus-central1: In progress\nus-east1: In progress\nWe will provide more information by Tuesday, 2021-10-19 19:00 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-20T23:09:38+00:00","modified":"2021-10-20T23:09:38+00:00","when":"2021-10-20T23:09:38+00:00","text":"Please refer to the following for the mini incident report:\nhttps://status.cloud.google.com/incidents/m617oqASdckYoegRtPJr","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/AB9DV685UZYD2Y7vDQ36","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"bUY8Ac8dJK2n3LLyJ2n7","number":"53969941824177223","begin":"2021-10-19T19:45:40+00:00","created":"2021-10-19T19:45:41+00:00","end":"2021-10-19T20:36:36+00:00","modified":"2021-10-19T20:36:37+00:00","external_desc":"us-central1: Cloud Pub/Sub experiencing issues with seeing monitoring metric data","updates":[{"created":"2021-10-19T20:36:36+00:00","modified":"2021-10-19T20:36:37+00:00","when":"2021-10-19T20:36:36+00:00","text":"The issue with Cloud Pub/Sub has been resolved for all affected users as of Tuesday, 2021-10-19 13:36 US/Pacific.\nPlease refer to https://status.cloud.google.com/incidents/9X2yAZMaYvM7egWTBioq for more details .\nIf you have questions , please open a case with the Support Team\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T19:45:41+00:00","modified":"2021-10-19T19:45:42+00:00","when":"2021-10-19T19:45:41+00:00","text":"Summary: us-central1: Cloud Pub/Sub experiencing issues with seeing monitoring metric data\nDescription: We are experiencing an issue with Cloud Pub/Sub beginning at Tuesday, 2021-10-19 11:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nPlease refer to https://status.cloud.google.com/incidents/9X2yAZMaYvM7egWTBioq for further updates on the issue.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Issues with seeing monitoring metric data especially delivery_latency_health_score. Other metrics data may also be affected.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-19T20:36:36+00:00","modified":"2021-10-19T20:36:37+00:00","when":"2021-10-19T20:36:36+00:00","text":"The issue with Cloud Pub/Sub has been resolved for all affected users as of Tuesday, 2021-10-19 13:36 US/Pacific.\nPlease refer to https://status.cloud.google.com/incidents/9X2yAZMaYvM7egWTBioq for more details .\nIf you have questions , please open a case with the Support Team\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"dFjdLh2v6zuES6t9ADCB","service_name":"Google Cloud Pub/Sub","affected_products":[{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"}],"uri":"incidents/bUY8Ac8dJK2n3LLyJ2n7","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"YR89K3PwAZZCij3X1GPS","number":"10595384874103504176","begin":"2021-10-19T19:35:33+00:00","created":"2021-10-19T19:35:41+00:00","end":"2021-10-19T20:35:18+00:00","modified":"2021-10-19T20:35:18+00:00","external_desc":"us-central1: Cloud Bigtable experiencing issues with metrics for bigtable.googleapis.com","updates":[{"created":"2021-10-19T20:35:18+00:00","modified":"2021-10-19T20:35:18+00:00","when":"2021-10-19T20:35:18+00:00","text":"The issue with Cloud Bigtable has been resolved for all affected users as of Tuesday, 2021-10-19 12:45 US/Pacific.\nPlease refer to https://status.cloud.google.com/incidents/9X2yAZMaYvM7egWTBioq for more details .\nIf you have questions , please open a case with the Support Team\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T19:35:35+00:00","modified":"2021-10-19T19:35:41+00:00","when":"2021-10-19T19:35:35+00:00","text":"Summary: us-central1: Cloud Bigtable experiencing issues with metrics for bigtable.googleapis.com\nDescription: We are experiencing an issue with Cloud Bigtable beginning at Tuesday, 2021-10-19 11:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nPlease refer to https://status.cloud.google.com/incidents/9X2yAZMaYvM7egWTBioq for further updates on the issue.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers in us-central1 may experience issues seeing metric data for various Cloud Bigtable metrics\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-19T20:35:18+00:00","modified":"2021-10-19T20:35:18+00:00","when":"2021-10-19T20:35:18+00:00","text":"The issue with Cloud Bigtable has been resolved for all affected users as of Tuesday, 2021-10-19 12:45 US/Pacific.\nPlease refer to https://status.cloud.google.com/incidents/9X2yAZMaYvM7egWTBioq for more details .\nIf you have questions , please open a case with the Support Team\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LfZSuE3xdQU46YMFV5fy","service_name":"Google Cloud Bigtable","affected_products":[{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"}],"uri":"incidents/YR89K3PwAZZCij3X1GPS","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"tu3zwM1PgkKadEodDs8e","number":"4570248042958394911","begin":"2021-10-19T18:40:00+00:00","created":"2021-10-19T21:06:53+00:00","end":"2021-10-19T23:21:00+00:00","modified":"2021-10-21T21:56:38+00:00","external_desc":"us-central1-c: GKE experiencing issues with some cluster and nodepool operations. Issue Mitigated.","updates":[{"created":"2021-10-21T21:56:27+00:00","modified":"2021-10-21T21:56:27+00:00","when":"2021-10-21T21:56:27+00:00","text":"We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\nFirst Impact\n**Incident Start:** 19 October 2021 11:40\n**Incident End:** 19 October 2021 16:21\n**Duration:** 4 hours, 41 minutes\nSecond Impact\n**Incident Start:** 20 October 2021 02:00\n**Incident End:** 20 October 2021 09:30\n**Duration:** 7 hours, 30 minutes\n**Affected Services and Features:**\nGoogle Kubernetes Engine\n**Regions/Zones:** us-central1-c\n**Description:**\nGoogle Kubernetes Engine experienced two impacts to operations, the first on 19 October 2021 and the second on 20 October 2021.\nFirst Impact: Customers may have experienced up to 100% failure rate for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 4 hours and 41 minutes. From preliminary analysis, the root cause was resource contention related to an unexpected increase in API operations. Engineers scaled up instances to mitigate the issue.\nSecond Impact: Customers may have experienced up to 80% failure for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 7 hours and 30 minutes. From preliminary analysis, the root case was created by an unexpected increase of nodepool operations in a single customer cluster. Engineers mitigated the issue through additional quota enforcement.\n**Customer Impact:**\nCustomers affected would have experienced 500/503 errors for create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T23:40:39+00:00","modified":"2021-10-19T23:40:39+00:00","when":"2021-10-19T23:40:39+00:00","text":"The issue with GKE cluster and node pool operations, has been resolved for all affected projects as of Tuesday, 2021-10-19 16:21 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T23:25:15+00:00","modified":"2021-10-19T23:25:15+00:00","when":"2021-10-19T23:25:15+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations. Mitigation underway.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2021-10-19 17:00 US/Pacific.\nDiagnosis: The following operations may fail with 500/503 errors:\ncreate-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T22:32:04+00:00","modified":"2021-10-19T22:32:05+00:00","when":"2021-10-19T22:32:04+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations. Investigations ongoing.\nDescription: Our engineering team continues to investigate the issue with GKE at us-central1-c, starting Tuesday, 2021-10-19 11:40 US/Pacific\nWe will provide an update by Tuesday, 2021-10-19 16:30 US/Pacific with latest details.\nDiagnosis: The following operations may fail with 500/503 errors:\ncreate-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T21:50:45+00:00","modified":"2021-10-19T21:50:45+00:00","when":"2021-10-19T21:50:45+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations. Investigations ongoing.\nDescription: Our engineering team continues to investigate the issue with GKE at us-centra1-c, starting Tuesday, 2021-10-19 11:40 US/Pacific\nWe will provide an update by Tuesday, 2021-10-19 15:30 US/Pacific with latest details.\nDiagnosis: The following operations may fail with 500/503 errors:\ncreate-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T21:28:27+00:00","modified":"2021-10-19T21:28:27+00:00","when":"2021-10-19T21:28:27+00:00","text":"Summary: us-central1-c: GKE experiencing issues with some cluster and nodepool operations\nDescription: We are experiencing an issue with Google Kubernetes Engine beginning at Tuesday, 2021-10-19 11:40 US/Pacific.\nOur engineering team continues to investigate the issue.\nExisting workloads should continue to work while attempts to resize nodepools, create or delete nodepools and clusters fail\nWe will provide an update by Tuesday, 2021-10-19 15:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: create-cluster, delete-cluster, create-nodepool, and delete-nodepool, node-pool resizes operations fail with 500 and 503 errors.\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T21:06:53+00:00","modified":"2021-10-19T21:06:53+00:00","when":"2021-10-19T21:06:53+00:00","text":"Summary: us-central1-c: Create / Delete Cluster and NodePool operations failing\nDescription: We are experiencing an issue with Google Kubernetes Engine beginning at Tuesday, 2021-10-19 11:40 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-10-19 14:35 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: create-cluster, delete-cluster, create-nodepool, and delete-nodepool, node-pool resizes operations are failing with 500 and 503 errors.\nWorkaround: The failed operations may succeed on retrying them","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-21T21:56:27+00:00","modified":"2021-10-21T21:56:27+00:00","when":"2021-10-21T21:56:27+00:00","text":"We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\nFirst Impact\n**Incident Start:** 19 October 2021 11:40\n**Incident End:** 19 October 2021 16:21\n**Duration:** 4 hours, 41 minutes\nSecond Impact\n**Incident Start:** 20 October 2021 02:00\n**Incident End:** 20 October 2021 09:30\n**Duration:** 7 hours, 30 minutes\n**Affected Services and Features:**\nGoogle Kubernetes Engine\n**Regions/Zones:** us-central1-c\n**Description:**\nGoogle Kubernetes Engine experienced two impacts to operations, the first on 19 October 2021 and the second on 20 October 2021.\nFirst Impact: Customers may have experienced up to 100% failure rate for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 4 hours and 41 minutes. From preliminary analysis, the root cause was resource contention related to an unexpected increase in API operations. Engineers scaled up instances to mitigate the issue.\nSecond Impact: Customers may have experienced up to 80% failure for create-cluster, delete-cluster, delete-nodepool operations and node-pool resizes in us-central1-c for 7 hours and 30 minutes. From preliminary analysis, the root case was created by an unexpected increase of nodepool operations in a single customer cluster. Engineers mitigated the issue through additional quota enforcement.\n**Customer Impact:**\nCustomers affected would have experienced 500/503 errors for create-cluster, delete-cluster, create-nodepool, delete-nodepool, and node-pool resizes.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LCSbT57h59oR4W98NHuz","service_name":"Google Kubernetes Engine","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/tu3zwM1PgkKadEodDs8e","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"9X2yAZMaYvM7egWTBioq","number":"3843619539238907325","begin":"2021-10-19T18:00:00+00:00","created":"2021-10-19T19:05:47+00:00","end":"2021-10-19T19:45:00+00:00","modified":"2021-10-27T21:29:08+00:00","external_desc":"Global: Cloud Monitoring elevated errors requesting underlying monitoring data","updates":[{"created":"2021-10-27T21:26:28+00:00","modified":"2021-10-27T21:26:28+00:00","when":"2021-10-27T21:26:28+00:00","text":"# INCIDENT REPORT\n## Summary\nOn 19 October 2021 11:00 US/Pacific, Cloud Monitoring experienced errors querying all monitoring data for approximately 1 hour and 45 minutes in the us-central1 region. We apologize for the inconvenience and are taking steps toward preventing recurrence in the future.\n## Root Cause\nCloud Monitoring is a global service but is subdivided into internal locales, each of which collect monitoring data which is generated locally. When users query Cloud Monitoring, each query fans out through a series of nodes (called mixers) within the corresponding locales. The mixers reach out to source nodes to gather the appropriate data, temporarily retaining it within a limited set of memory.\nDuring a recent infrastructure change in the U.S. locale, the amount of memory allocated to mixers in the us-central1 region was inadvertently reduced. This caused mixer tasks to run low on memory. The number of tasks in a low memory state grew over a period of several days as the change was gradually rolled out to production, following Google's standard progressive rollout policies.\nThe mixer task has safeguards which are designed to detect and reduce the impact of low memory conditions by pausing queries that use significant memory. However, in this case, an existing misconfiguration of this safeguard prevented it from activating correctly. Eventually, tasks which were low on memory failed; enough tasks failed in total to cause widespread failures and service impact.\n## Remediation and Prevention\nGoogle engineers were alerted to the problem on 19 October 2021 at 11:11 and immediately started an investigation. Root cause - the reduction in memory allocation for mixer nodes - was identified at 11:32. Google engineers quickly identified a mitigation, which we began to roll out at 11:50. Restoring the proper memory capacity for mixer nodes fully mitigated the issue at 12:54.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions.\nWe are taking the following immediate steps to prevent this or similar issues from happening again:\n- Fixing the misconfiguration so that mixers which are low on memory will correctly detect that condition.\n- Introduce load-shedding, such that mixers which run out of memory will simply reject new queries until memory usage subsides, rather than failing.\n- Optimize the mixers to reduce the likelihood of out-of-memory scenarios.\n- Modifying Cloud Monitoring's rollout automation so that it automatically spots problems of this type, allowing engineers to be alerted sooner.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T22:48:17+00:00","modified":"2021-10-19T22:48:17+00:00","when":"2021-10-19T22:48:17+00:00","text":"Mini Incident Report while full Incident Report is prepared\nWe apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 19 October 2021 11:00\n**Incident End:** 19 October 2021 12:45\n**Duration:** 1 hours, 45 minutes\n**Affected Services and Features:**\nGoogle Cloud Monitoring, Google Cloud Dataflow, Google Cloud Pub/Sub, Google Cloud NAT, Google Cloud Router, Google Cloud Interconnect, Google Bigtable, Google Cloud Databases\n**Regions/Zones:** us-central1 and us-central2\n**Description:**\nGoogle Cloud Monitoring experienced errors querying monitoring data for approximately 1 hour and 45 minutes. From preliminary analysis, the root cause of the issue was due to resource contention that occurred following a recent roll out which included a misconfiguration. Engineers corrected the configuration, and restarted the affected instances to resolve the issue.\n**Customer Impact:**\n-Customers may have experienced errors or incomplete monitoring data.\n-Missing precomputed data from between 11:00 PT and 12:45 PT is expected, but can still be viewed via raw query.\n-Customers may have also experienced false alerts during the impact window based on the underlying monitoring data.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T20:27:28+00:00","modified":"2021-10-19T20:27:29+00:00","when":"2021-10-19T20:27:28+00:00","text":"The issue with Cloud Monitoring has been resolved for all affected users as of Tuesday, 2021-10-19 12:45 US/Pacific.\nWe will publish an analysis of this incident once we have completed our internal investigation.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T20:07:21+00:00","modified":"2021-10-19T20:07:22+00:00","when":"2021-10-19T20:07:21+00:00","text":"Summary: Global: Cloud Monitoring elevated errors requesting underlying monitoring data\nDescription: All customer impact is mitigated as of Tuesday, 2021-10-19 12:45 US/Pacific. Missing precomputed data from between 11:00 to 12:45 is expected, but can still be viewed via a raw query. Users might see lingering impact when running queries with large windows. Users may have received false alerts during that window based on the underlying monitoring data.\nWe will continue to monitor the situation. We do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-10-19 14:00 US/Pacific with current details.\nDiagnosis: Affected customers may see errors when trying to query their monitoring data.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-10-19T19:51:08+00:00","modified":"2021-10-19T19:51:08+00:00","when":"2021-10-19T19:51:08+00:00","text":"Summary: Global: Cloud Monitoring elevated errors requesting underlying monitoring data\nDescription: We believe the issue with Cloud Monitoring is partially resolved and the mitigation is continuing to reduce the error rate. We will continue to monitor the situation.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-10-19 14:00 US/Pacific with current details.\nDiagnosis: Affected customers may see errors when trying to query their monitoring data.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-10-19T19:11:27+00:00","modified":"2021-10-19T19:11:27+00:00","when":"2021-10-19T19:11:27+00:00","text":"Summary: Global: Cloud Monitoring elevated errors requesting underlying monitoring data\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2021-10-19 13:05 US/Pacific.\nDiagnosis: Affected customers may see errors when trying to query their monitoring data.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2021-10-27T21:26:28+00:00","modified":"2021-10-27T21:26:28+00:00","when":"2021-10-27T21:26:28+00:00","text":"# INCIDENT REPORT\n## Summary\nOn 19 October 2021 11:00 US/Pacific, Cloud Monitoring experienced errors querying all monitoring data for approximately 1 hour and 45 minutes in the us-central1 region. We apologize for the inconvenience and are taking steps toward preventing recurrence in the future.\n## Root Cause\nCloud Monitoring is a global service but is subdivided into internal locales, each of which collect monitoring data which is generated locally. When users query Cloud Monitoring, each query fans out through a series of nodes (called mixers) within the corresponding locales. The mixers reach out to source nodes to gather the appropriate data, temporarily retaining it within a limited set of memory.\nDuring a recent infrastructure change in the U.S. locale, the amount of memory allocated to mixers in the us-central1 region was inadvertently reduced. This caused mixer tasks to run low on memory. The number of tasks in a low memory state grew over a period of several days as the change was gradually rolled out to production, following Google's standard progressive rollout policies.\nThe mixer task has safeguards which are designed to detect and reduce the impact of low memory conditions by pausing queries that use significant memory. However, in this case, an existing misconfiguration of this safeguard prevented it from activating correctly. Eventually, tasks which were low on memory failed; enough tasks failed in total to cause widespread failures and service impact.\n## Remediation and Prevention\nGoogle engineers were alerted to the problem on 19 October 2021 at 11:11 and immediately started an investigation. Root cause - the reduction in memory allocation for mixer nodes - was identified at 11:32. Google engineers quickly identified a mitigation, which we began to roll out at 11:50. Restoring the proper memory capacity for mixer nodes fully mitigated the issue at 12:54.\nGoogle is committed to quickly and continually improving our technology and operations to prevent service disruptions.\nWe are taking the following immediate steps to prevent this or similar issues from happening again:\n- Fixing the misconfiguration so that mixers which are low on memory will correctly detect that condition.\n- Introduce load-shedding, such that mixers which run out of memory will simply reject new queries until memory usage subsides, rather than failing.\n- Optimize the mixers to reduce the likelihood of out-of-memory scenarios.\n- Modifying Cloud Monitoring's rollout automation so that it automatically spots problems of this type, allowing engineers to be alerted sooner.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"},{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Cloud NAT","id":"hCNpnTQHkUCCGxJy35Yq"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"}],"uri":"incidents/9X2yAZMaYvM7egWTBioq","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"e7k7vJdLNPgefin6BZnx","number":"14716856531473798729","begin":"2021-10-19T01:14:00+00:00","created":"2021-10-19T01:21:11+00:00","end":"2021-10-19T09:59:00+00:00","modified":"2022-03-28T23:42:57+00:00","external_desc":"us-west1: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway","updates":[{"created":"2021-10-20T23:08:27+00:00","modified":"2021-10-20T23:08:27+00:00","when":"2021-10-20T23:08:27+00:00","text":"Please refer to the following page for the mini incident report:\nhttps://status.cloud.google.com/incidents/m617oqASdckYoegRtPJr","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T09:59:09+00:00","modified":"2021-10-19T09:59:16+00:00","when":"2021-10-19T09:59:09+00:00","text":"The issue with Cloud Logging is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-19T04:16:47+00:00","modified":"2021-10-19T04:16:47+00:00","when":"2021-10-19T04:16:47+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2021-10-19 03:15 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T03:13:25+00:00","modified":"2021-10-19T03:13:25+00:00","when":"2021-10-19T03:13:25+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2021-10-18 21:15 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T02:31:00+00:00","modified":"2021-10-19T02:31:00+00:00","when":"2021-10-19T02:31:00+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs. Mitigation underway\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Monday, 2021-10-18 20:30 US/Pacific.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-19T01:21:11+00:00","modified":"2021-10-19T01:21:11+00:00","when":"2021-10-19T01:21:11+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs.\nDescription: We are experiencing an issue with Cloud Logging beginning at Monday, 2021-10-18 07:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-10-18 19:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience increased latency in Cloud Logging and issues with loading logs.\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-20T23:08:27+00:00","modified":"2021-10-20T23:08:27+00:00","when":"2021-10-20T23:08:27+00:00","text":"Please refer to the following page for the mini incident report:\nhttps://status.cloud.google.com/incidents/m617oqASdckYoegRtPJr","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/e7k7vJdLNPgefin6BZnx","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"m617oqASdckYoegRtPJr","number":"17153038161207436266","begin":"2021-10-18T18:04:00+00:00","created":"2021-10-18T18:18:52+00:00","end":"2021-10-18T20:18:00+00:00","modified":"2021-10-20T23:05:20+00:00","external_desc":"us-west1: Cloud Logging experiencing elevated error rate while reading logs","updates":[{"created":"2021-10-20T23:05:20+00:00","modified":"2021-10-20T23:05:20+00:00","when":"2021-10-20T23:05:20+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 18 October 2021 07:30\n**Incident End:** 20 October 2021 04:35\n**Duration:** 1 day, 21 hours, 5 minutes\n**Affected Services and Features:**\n- Google Cloud Logging\n**Regions/Zones:** us-west1\n**Description:**\nGoogle Cloud Logging experienced periods of increased latency and issues loading logs for 45 hours and 5 minutes in us-west1 due to resource contention related to unexpected increases in traffic. Our engineering team scaled up resources to mitigate the issue. Engineering team subsequently identified the source of the increase in traffic, and confirmed that it had normalized.\nOur engineering team has identified the following timeframes where customers may have experienced significant impact:\nFirst Impact Timeframe\n- Start: 18 October 2021 09:45\n- End: 18 October 2021 23:50\nSecond Impact Timeframe\n- Start: 19 October 2021 07:44\n- End: 19 October 2021 18:50\n**Customer Impact:**\nCustomers may have experienced increased latency in Cloud Console and issues loading logs.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-18T20:18:40+00:00","modified":"2021-10-18T20:18:41+00:00","when":"2021-10-18T20:18:40+00:00","text":"The issue with Cloud Logging has been resolved for all affected users as of Monday, 2021-10-18 13:18 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-18T19:52:37+00:00","modified":"2021-10-18T21:28:47+00:00","when":"2021-10-18T19:52:37+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs\nDescription: We are experiencing an issue with Cloud Logging beginning at Monday, 2021-10-18 07:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-10-18 13:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Increased latency in Cloud Console and issues with loading logs\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-18T18:49:08+00:00","modified":"2021-10-18T21:28:46+00:00","when":"2021-10-18T18:49:08+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs\nDescription: We are experiencing an issue with Cloud Logging beginning at Monday, 2021-10-18 07:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-10-18 12:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Increased latency in Cloud Console and issues with loading logs\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-18T18:18:51+00:00","modified":"2021-10-18T21:28:36+00:00","when":"2021-10-18T18:18:51+00:00","text":"Summary: us-west1: Cloud Logging experiencing elevated error rate while reading logs\nDescription: We are experiencing an issue with Cloud Logging beginning at Monday, 2021-10-18 07:27 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-10-18 11:50 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Increased latency in Cloud Console and issues with loading logs\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-20T23:05:20+00:00","modified":"2021-10-20T23:05:20+00:00","when":"2021-10-20T23:05:20+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 18 October 2021 07:30\n**Incident End:** 20 October 2021 04:35\n**Duration:** 1 day, 21 hours, 5 minutes\n**Affected Services and Features:**\n- Google Cloud Logging\n**Regions/Zones:** us-west1\n**Description:**\nGoogle Cloud Logging experienced periods of increased latency and issues loading logs for 45 hours and 5 minutes in us-west1 due to resource contention related to unexpected increases in traffic. Our engineering team scaled up resources to mitigate the issue. Engineering team subsequently identified the source of the increase in traffic, and confirmed that it had normalized.\nOur engineering team has identified the following timeframes where customers may have experienced significant impact:\nFirst Impact Timeframe\n- Start: 18 October 2021 09:45\n- End: 18 October 2021 23:50\nSecond Impact Timeframe\n- Start: 19 October 2021 07:44\n- End: 19 October 2021 18:50\n**Customer Impact:**\nCustomers may have experienced increased latency in Cloud Console and issues loading logs.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"}],"uri":"incidents/m617oqASdckYoegRtPJr","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"rXqQALuw55aCKd2QHfM3","number":"5462201987187757443","begin":"2021-10-13T12:25:00+00:00","created":"2021-10-13T16:45:23+00:00","end":"2021-10-13T17:50:00+00:00","modified":"2021-10-14T14:42:33+00:00","external_desc":"Global: Some Cloud Monitoring metric calculations are delayed or missing for multiple Cloud Services","updates":[{"created":"2021-10-14T14:42:33+00:00","modified":"2021-10-14T14:42:33+00:00","when":"2021-10-14T14:42:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 13 October 2021 05:25\n**Incident End:** 13 October 2021 10:50\n**Duration:** 5 hours, 25 minutes\n**Affected Services and Features:**\n- Cloud Monitoring\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Monitoring experienced degraded monitoring data availability globally for a duration of 5 hours and 25 minutes. During the disruption, impacted customers would have experienced missing monitoring metric data and missed monitoring alerts. The root cause was identified as a failed configuration on a standby task. A leadership change caused an unhealthy task to take over. The issue was mitigated by restarting the leader which allowed a healthy task to take back control which restored the service.\n**Customer Impact:**\n- Missing data for some monitoring metrics\n- Missed monitoring alerts","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-13T18:33:05+00:00","modified":"2021-10-13T18:33:06+00:00","when":"2021-10-13T18:33:05+00:00","text":"The issue with Cloud Monitoring has been resolved for all affected projects as of Wednesday, 2021-10-13 11:21 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-13T18:04:25+00:00","modified":"2021-10-13T18:04:25+00:00","when":"2021-10-13T18:04:25+00:00","text":"Summary: Global: Some Cloud Monitoring metric calculations are delayed or missing for multiple Cloud Services\nDescription: Mitigation work is currently underway by our engineering team, as we believe we have identified the root cause.\nWe will provide more information by Wednesday, 2021-10-13 13:00 US/Pacific.\nDiagnosis: Affected customers may notice services such as Google Bigtable, Cloud Pub/Sub, and App Engine Flexible not exporting metrics to Cloud Monitoring. Additional services with monitoring data that is aggregated by location may also be impacted. Configured alerts based on affected metrics may get incorrect alert signals based on the alert definition.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-13T16:50:29+00:00","modified":"2021-10-13T16:50:30+00:00","when":"2021-10-13T16:50:29+00:00","text":"Summary: Global: Some Cloud Monitoring metric calculations are delayed or missing for multiple Cloud Services\nDescription: We are experiencing an issue with Cloud Monitoring beginning at Wednesday, 2021-10-13 05:24 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-13 11:30 US/Pacific with current details.\nDiagnosis: Affected customers may notice services such as Google Bigtable, Cloud Pub/Sub, and App Engine Flexible not exporting metrics to Cloud Monitoring.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-13T16:45:16+00:00","modified":"2021-10-13T16:45:24+00:00","when":"2021-10-13T16:45:16+00:00","text":"Summary: Global: Some Cloud Monitoring metric calculations are delayed\nDescription: We are experiencing an issue with Cloud Monitoring beginning at Wednesday, 2021-10-13 00:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-13 10:19 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-14T14:42:33+00:00","modified":"2021-10-14T14:42:33+00:00","when":"2021-10-14T14:42:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 13 October 2021 05:25\n**Incident End:** 13 October 2021 10:50\n**Duration:** 5 hours, 25 minutes\n**Affected Services and Features:**\n- Cloud Monitoring\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Monitoring experienced degraded monitoring data availability globally for a duration of 5 hours and 25 minutes. During the disruption, impacted customers would have experienced missing monitoring metric data and missed monitoring alerts. The root cause was identified as a failed configuration on a standby task. A leadership change caused an unhealthy task to take over. The issue was mitigated by restarting the leader which allowed a healthy task to take back control which restored the service.\n**Customer Impact:**\n- Missing data for some monitoring metrics\n- Missed monitoring alerts","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"}],"uri":"incidents/rXqQALuw55aCKd2QHfM3","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"TQ7kUbAPqaEXrxqGKTP9","number":"8484543035619786647","begin":"2021-10-06T20:50:00+00:00","created":"2021-10-07T00:51:30+00:00","end":"2021-10-07T00:42:00+00:00","modified":"2021-10-07T17:33:56+00:00","external_desc":"us-central1-a,c: Cloud spanner jobs reporting high read latencies. Incident mitigated.","updates":[{"created":"2021-10-07T17:33:43+00:00","modified":"2021-10-07T17:33:43+00:00","when":"2021-10-07T17:33:43+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 06 October 2021 13:50\n**Incident End:** 06 October 2021 17:42\n**Duration:** 3 hours, 52 minutes\n**Affected Services and Features:**\n- Cloud Spanner: Latency\n**Regions/Zones:** us-central1\n**Description:**\nCloud Spanner experienced elevated p99.9 read latencies in us-central1 in some limited cases for a duration of 3 hours, 52 minutes. From preliminary analysis, the root cause of the issue was a resource contention issue on a small portion of the Spanner frontend servers in us-central1 that affected p99.9 latency for some customers.\n**Customer Impact:**\nA small portion of the regional Cloud Spanner instances in us-central1 experienced increased p99.9 latency for read requests. Multi-regional Cloud Spanner instances (nam6, nam7, nam9, nam10, nam11, nam12, and nam-eur-asia1) where the requests were routed to the impacted frontend servers in us-central1 were affected similarly.\n**Additional details:**\nThe issue has been fully mitigated by isolating the impacted frontend servers, and we do not believe there is a risk of recurrence at this time.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-07T01:28:30+00:00","modified":"2021-10-07T01:28:30+00:00","when":"2021-10-07T01:28:30+00:00","text":"The issue with Cloud Spanner has been resolved for all affected projects as of Wednesday, 2021-10-06 17:42 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-07T00:51:30+00:00","modified":"2021-10-07T00:51:31+00:00","when":"2021-10-07T00:51:30+00:00","text":"Summary: us-central1-a,c: Cloud spanner jobs reporting high read latencies. Investigations ongoing.\nDescription: We are experiencing an issue with high read latencies on Cloud Spanner jobs at us-central1-a,c beginning at Wednesday, 2021-10-06 14:00 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-10-06 18:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-07T17:33:43+00:00","modified":"2021-10-07T17:33:43+00:00","when":"2021-10-07T17:33:43+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 06 October 2021 13:50\n**Incident End:** 06 October 2021 17:42\n**Duration:** 3 hours, 52 minutes\n**Affected Services and Features:**\n- Cloud Spanner: Latency\n**Regions/Zones:** us-central1\n**Description:**\nCloud Spanner experienced elevated p99.9 read latencies in us-central1 in some limited cases for a duration of 3 hours, 52 minutes. From preliminary analysis, the root cause of the issue was a resource contention issue on a small portion of the Spanner frontend servers in us-central1 that affected p99.9 latency for some customers.\n**Customer Impact:**\nA small portion of the regional Cloud Spanner instances in us-central1 experienced increased p99.9 latency for read requests. Multi-regional Cloud Spanner instances (nam6, nam7, nam9, nam10, nam11, nam12, and nam-eur-asia1) where the requests were routed to the impacted frontend servers in us-central1 were affected similarly.\n**Additional details:**\nThe issue has been fully mitigated by isolating the impacted frontend servers, and we do not believe there is a risk of recurrence at this time.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"EcNGGUgBtBLrtm4mWvqC","service_name":"Cloud Spanner","affected_products":[{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"}],"uri":"incidents/TQ7kUbAPqaEXrxqGKTP9","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"ty5QqQdPZaypH55Mifkz","number":"4922318955216512507","begin":"2021-10-05T22:53:00+00:00","created":"2021-10-05T23:25:10+00:00","end":"2021-10-06T01:23:00+00:00","modified":"2021-10-06T17:07:11+00:00","external_desc":"Global: Jobs failing with internal error for GKE version 1.18","updates":[{"created":"2021-10-06T17:06:48+00:00","modified":"2021-10-06T17:06:48+00:00","when":"2021-10-06T17:06:48+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 05 October 2021 15:53\n**Incident End:** 05 October 2021 18:23\n**Duration:** 2 hours, 30 minutes\n**Affected Services and Features:**\n- Google Cloud AI: Training Jobs\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud AI experienced an issue with Distributed training jobs, Bring your own service account (BYOSA) jobs and VPC peering jobs that run on GKE v1.18 which failed with an internal error for a duration of 2 hours, 30 minutes. From preliminary analysis, the root cause of this issue was the removal of certain GKE v1.18 releases as an available GKE cluster configuration. The issue has been fixed by rolling Cloud AI forward to use GKE v1.19 for training jobs.\n**Customer Impact:**\n- Google Cloud AI Training jobs (Distributed training jobs, BYOSA jobs and VPC peering jobs) that run on GKE v1.18 would fail with an internal error.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-06T01:24:12+00:00","modified":"2021-10-06T01:24:13+00:00","when":"2021-10-06T01:24:12+00:00","text":"The issue with Cloud AI has been resolved for all affected projects as of Tuesday, 2021-10-05 18:23 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-10-06T01:11:46+00:00","modified":"2021-10-06T01:11:47+00:00","when":"2021-10-06T01:11:46+00:00","text":"Summary: Global: Jobs failing with internal error for GKE version 1.18\nDescription: Our Engineering team continues to work on mitigating the issue and have observed reduction in customer impact.\nThe mitigation is expected to complete by Tuesday, 2021-10-05 18:45 US/Pacific.\nWe will provide more information by Tuesday, 2021-10-05 18:45 US/Pacific.\nDiagnosis: All training jobs (Distributed training jobs, BYOSA jobs and VPC peering jobs) that run on GKE v1.18 to fail with internal error.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-05T23:50:02+00:00","modified":"2021-10-05T23:50:07+00:00","when":"2021-10-05T23:50:02+00:00","text":"Summary: Global: Jobs failing with internal error for GKE version 1.18\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Tuesday, 2021-10-05 18:30 US/Pacific.\nWe will provide more information by Tuesday, 2021-10-05 18:30 US/Pacific.\nDiagnosis: All training jobs (Distributed training jobs, BYOSA jobs and VPC peering jobs) that run on GKE v1.18 to fail with internal error.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-10-05T23:25:08+00:00","modified":"2021-10-05T23:25:10+00:00","when":"2021-10-05T23:25:08+00:00","text":"Summary: Global: Jobs failing with internal error for GKE version 1.18\nDescription: We are experiencing an issue with Cloud AI where distributed training jobs, BYOSA jobs and VPC peering jobs that run on GKE v1.18 will fail with internal error.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-10-05 17:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: All training jobs (Distributed training jobs, BYOSA jobs and VPC peering jobs) that run on GKE v1.18 to fail with internal error.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-10-06T17:06:48+00:00","modified":"2021-10-06T17:06:48+00:00","when":"2021-10-06T17:06:48+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 05 October 2021 15:53\n**Incident End:** 05 October 2021 18:23\n**Duration:** 2 hours, 30 minutes\n**Affected Services and Features:**\n- Google Cloud AI: Training Jobs\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud AI experienced an issue with Distributed training jobs, Bring your own service account (BYOSA) jobs and VPC peering jobs that run on GKE v1.18 which failed with an internal error for a duration of 2 hours, 30 minutes. From preliminary analysis, the root cause of this issue was the removal of certain GKE v1.18 releases as an available GKE cluster configuration. The issue has been fixed by rolling Cloud AI forward to use GKE v1.19 for training jobs.\n**Customer Impact:**\n- Google Cloud AI Training jobs (Distributed training jobs, BYOSA jobs and VPC peering jobs) that run on GKE v1.18 would fail with an internal error.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Machine Learning","id":"z9PfKanGZYvYNUbnKzRJ"},{"title":"AI Platform Training","id":"3e2H6fMaAhGSPSSPb8CV"},{"title":"Vertex AI Training","id":"baQeYW2fsPA2vvLCqN93"}],"uri":"incidents/ty5QqQdPZaypH55Mifkz","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"QSirAFiyN5yMeeE6GNxq","number":"4634903035067246523","begin":"2021-09-27T17:17:00+00:00","created":"2021-09-28T13:05:15+00:00","end":"2021-09-29T13:52:00+00:00","modified":"2022-03-29T18:53:13+00:00","external_desc":"Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE. Mitigation in progress. ETA - 00:00 PST, 29th Sept.","updates":[{"created":"2021-09-29T18:32:34+00:00","modified":"2021-09-29T18:32:34+00:00","when":"2021-09-29T18:32:34+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 22 September 2021 00:00\n**Incident End:** 29 September 2021 05:40\n**Duration:** 7 days, 5 hours, 40 minutes\n**Affected Services and Features:**\n- Google Kubernetes Engine (GKE)\n- Google Compute Engine (GCE)\n- Google Cloud Build (GCB)\n**Regions/Zones:** Global\n**Description:**\nGoogle Kubernetes Engine, Google Compute Engine, and Google Cloud build experienced connection failures in Docker workloads to Google Cloud Load Balancers (GCLB) and destinations hosted behind content distribution networks (CDN’s) with a specific network configuration, such as debian.org and github.com.\nFrom preliminary analysis, the root cause of the issue was a rollout to the network virtualization stack, which inadvertently broke mechanisms to tolerate Maximum Transmission Unit (MTU) mismatches. This change, in conjunction with with a mismatch for the MTU between nested Docker containers and Virtual Machines (VM’s), in combination with how the GCP networking subsystem handles large packets, resulted in packet drops leading to timeouts and connection errors for Docker workloads.\n**Customer Impact:**\nGoogle Kubernetes Engine: Increased timeout and “Connection Failed” errors when connecting to GCLB or destinations hosted behind content distribution networks (CDN’s) with a specific network configuration.\nGoogle Compute Engine: Increased timeout and “Connection Failed” errors when connecting to GCLB or destinations hosted behind content distribution networks (CDN’s) with a specific network configuration for Docker workloads running on GCE.\nGoogle Cloud Build: Increased build failures for builds which fetch sources from repositories hosted behind content distribution networks (CDN’s) with a specific network configuration.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-29T13:52:49+00:00","modified":"2021-09-29T13:52:51+00:00","when":"2021-09-29T13:52:49+00:00","text":"The issue with Google Kubernetes Engine has been resolved for all affected users as of Wednesday, 2021-09-29 06:52 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-29T10:33:05+00:00","modified":"2021-09-29T10:33:12+00:00","when":"2021-09-29T10:33:05+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE. Mitigation in progress. ETA - 00:00 PST, 29th Sept.\nDescription: Mitigation work is still underway by our engineering team. The rollback in in progress and on final stages of completion.\nThe mitigation is expected to complete by Wednesday, 2021-09-29 07:00 US/Pacific\nWe will provide more information by Wednesday, 2021-09-29 08:00 US/Pacific.\nDiagnosis: Some customers may be experiencing connection failures in Docker workflow to GCLB, GCB, or Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-29T07:31:11+00:00","modified":"2021-09-29T07:31:11+00:00","when":"2021-09-29T07:31:11+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE. Mitigation in progress. ETA - 00:00 PST, 29th Sept.\nDescription: Mitigation work is still underway by our engineering team. The rollback in in progress and on final stages of completion.\nThe mitigation is expected to complete by Wednesday, 2021-09-29 03:00 US/Pacific\nWe will provide more information by Wednesday, 2021-09-29 03:30 US/Pacific.\nDiagnosis: Some customers may be experiencing connection failures in Docker workflow to GCLB, GCB, or Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-29T05:00:01+00:00","modified":"2021-09-29T05:00:01+00:00","when":"2021-09-29T05:00:01+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE. Mitigation in progress. ETA - 00:00 PST, 29th Sept.\nDescription: Mitigation work is still underway by our engineering team. The rollback in in progress and on track towards completion.\nThe mitigation is expected to complete by Wednesday, 2021-09-29 00:00 US/Pacific\nWe will provide more information by Wednesday, 2021-09-29 00:30 US/Pacific.\nDiagnosis: Some customers may be experiencing connection failures in Docker workflow to GCLB, GCB, or Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-29T00:46:30+00:00","modified":"2021-09-29T00:46:30+00:00","when":"2021-09-29T00:46:30+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE. Mitigation in progress. ETA - 00:00 PST, 29th Sept.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Wednesday, 2021-09-29 00:00 US/Pacific\nWe will provide more information by Tuesday, 2021-09-28 22:00 US/Pacific.\nDiagnosis: Some customers may be experiencing connection failures in Docker workflow to GCLB, GCB, or Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-28T20:20:04+00:00","modified":"2021-09-28T20:20:04+00:00","when":"2021-09-28T20:20:04+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2021-09-30 18:00 US/Pacific\nWe will provide more information by Tuesday, 2021-09-28 18:00 US/Pacific.\nDiagnosis: Some customers may be experiencing connection failures in Docker workflow to GCLB, GCB, or Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-28T19:20:34+00:00","modified":"2021-09-28T19:20:34+00:00","when":"2021-09-28T19:20:34+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2021-09-30 18:00 US/Pacific\nWe will provide more information by Tuesday, 2021-09-28 18:00 US/Pacific.\nDiagnosis: Some customers might experience a connection failure in Docker workflow to GCLB or Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-28T19:02:22+00:00","modified":"2021-09-28T19:02:22+00:00","when":"2021-09-28T19:02:22+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2021-09-30 18:00 US/Pacific\nWe will provide more information by Tuesday, 2021-09-28 18:00 US/Pacific.\nDiagnosis: Some customers might experience a connection failure in Docker workflow to Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-28T13:05:07+00:00","modified":"2021-09-28T13:05:16+00:00","when":"2021-09-28T13:05:07+00:00","text":"Summary: Global: We have identified a Networking connectivity issue that impacts Docker workloads inside GKE and potentially GCE.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2021-09-30 18:00 US/Pacific\nWe will provide more information by Tuesday, 2021-09-28 12:00 US/Pacific.\nDiagnosis: Some customers might experience a connection failure in Docker workflow to Fastly destinations such as debian.org, github.com with a timeout error.\nWorkaround: If you are impacted, please try adding an init container manifest into docker in docker deployment.This will ensures packets are sent with a proper MTU that will work with Fastly destinations.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-29T18:32:34+00:00","modified":"2021-09-29T18:32:34+00:00","when":"2021-09-29T18:32:34+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 22 September 2021 00:00\n**Incident End:** 29 September 2021 05:40\n**Duration:** 7 days, 5 hours, 40 minutes\n**Affected Services and Features:**\n- Google Kubernetes Engine (GKE)\n- Google Compute Engine (GCE)\n- Google Cloud Build (GCB)\n**Regions/Zones:** Global\n**Description:**\nGoogle Kubernetes Engine, Google Compute Engine, and Google Cloud build experienced connection failures in Docker workloads to Google Cloud Load Balancers (GCLB) and destinations hosted behind content distribution networks (CDN’s) with a specific network configuration, such as debian.org and github.com.\nFrom preliminary analysis, the root cause of the issue was a rollout to the network virtualization stack, which inadvertently broke mechanisms to tolerate Maximum Transmission Unit (MTU) mismatches. This change, in conjunction with with a mismatch for the MTU between nested Docker containers and Virtual Machines (VM’s), in combination with how the GCP networking subsystem handles large packets, resulted in packet drops leading to timeouts and connection errors for Docker workloads.\n**Customer Impact:**\nGoogle Kubernetes Engine: Increased timeout and “Connection Failed” errors when connecting to GCLB or destinations hosted behind content distribution networks (CDN’s) with a specific network configuration.\nGoogle Compute Engine: Increased timeout and “Connection Failed” errors when connecting to GCLB or destinations hosted behind content distribution networks (CDN’s) with a specific network configuration for Docker workloads running on GCE.\nGoogle Cloud Build: Increased build failures for builds which fetch sources from repositories hosted behind content distribution networks (CDN’s) with a specific network configuration.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Cloud Build","id":"fw8GzBdZdqy4THau7e1y"}],"uri":"incidents/QSirAFiyN5yMeeE6GNxq","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"E18Caoo5X1m6dTa1PVr1","number":"9319634779909953108","begin":"2021-09-22T21:10:00+00:00","created":"2021-09-23T20:19:00+00:00","end":"2021-09-23T07:30:00+00:00","modified":"2021-09-29T17:55:58+00:00","external_desc":"We experienced an issue with Cloud Console where customers observed warning notifications related to payment method, beginning at Wednesday, 2021-09-22 14:10 US/Pacific.","updates":[{"created":"2021-09-29T17:55:58+00:00","modified":"2021-09-29T17:55:58+00:00","when":"2021-09-29T17:55:58+00:00","text":"INCIDENT REPORT (Combined GCP / WS)\nSummary\nOn 22 September 2021 14:10 US/Pacific, Google Cloud, Workspace, and Geo (Maps) customers in the United States (U.S.) incorrectly received alert notifications on the Google Cloud Console user interface and an email regarding an issue with processing automatic payments. The issue persisted for 11 hours, 20 minutes.\nAll customers in the United States who use credit card billing were inadvertently notified via email “Your payment information could not be processed. Visit the payment overview page to make sure your payment information is up to date and to pay any outstanding charges.” Customers also received a notification alert in the Google Cloud Console to update their primary payment method. After customers attempted to update their payment information in the admin console, they received a message that the payment could not be processed.\nWe sincerely apologize for the inconvenience caused by the incorrect notification to our customers. Our engineering team has deployed a fix for this and has ensured that account settings are restored. We are taking additional steps to prevent similar problems in the future, as outlined below.\nRoot Cause\nThe trigger for this service disruption was an update to the payment configuration intended to support virtual credit cards (one-time credit card numbers from Visa and MasterCard). This update unintentionally caused a situation in which no credit card payments were able to be processed in the U.S. region and resulted in payment failure notifications being sent out to affected customers. Google's standard change control policy is to roll out all production updates progressively; however, in this specific case, errors in the rollout process meant that the change rolled out much more rapidly. Addressing this is a key part of our corrective actions.\nRemediation and Prevention\nGoogle engineers were alerted to the issue on 22 September 2021 15:28 US/Pacific and immediately started an investigation. Engineers identified an unexpected increase of payment request failures and initiated a rollback of the configuration change at 17:42, which resolved the issue causing the incorrect email notifications as well as the Cloud Console alert notifications.\nOnce it was confirmed that affected customers were no longer prompted with the notification alert and impact had ended, engineers began mitigation work to update account state back to the proper payment settings to prevent a recurrence.\nTo prevent recurrence and reduce the impact of similar events, we are taking the following actions:\nImproving our payments infrastructure monitoring and alerting configuration with “early indicator” signals for faster detection and notification of widespread payment failures.\nSpeeding up payment system mitigation procedures by automating the root causing of problems, and increasing the rate at which recovery steps can be processed.\nEnsuring that the class of change which resulted in this incident is pushed progressively, with time to detect adverse impact at small scope.\nDetailed Description of Impact\n#### Google Cloud Console - Google Cloud Platform and Geo Maps\nU.S. customer accounts received an alert notification in their admin account warning that they needed to verify their payment method. When attempting to update the preferred credit card method, the user interface was unresponsive.\nCustomers received an email warning message informing them they needed to verify their payment method. They also might have received an email warning that their project was going to be suspended. After the issue was resolved, account alert notifications disappeared, and the account state was set back to the proper payments setting. Some customers may have observed restrictions and were unable to create new resources. These restrictions were corrected when the account was returned to good standing.\nThe period of impact was between 14:10 and 00:30.\n#### Google Admin Console - Google Workspace\nU.S. customer accounts received an alert notification in their admin account warning that they needed to verify their payment method. When attempting to update the preferred credit card method, the user interface was unresponsive.\nCustomers received an email warning they needed to verify their payment method. When they attempted to update the credit card they are currently using, they were unable to do so. After the issue was resolved, account alert notifications disappeared, and the account state was set back to the proper payments setting.\nThe period of impact was between 14:10 and 01:30.\nAdditional Information for Customers\nThe impacted geographical scope used in the preliminary incident statement was incorrect. While impact initially was suspected to be global, our engineers confirmed impact only affected U.S. customers with credit cards stored for automatic payments. The corrected details for impacted regions / zones are contained in the above report.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-23T20:23:13+00:00","modified":"2021-09-24T22:18:11+00:00","when":"2021-09-23T20:23:13+00:00","text":"We are posting this retroactively to provide details of a recent service incident.\nWe apologize for the inconvenience this issue may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 22 September 2021 14:10\n**Incident End:** 23 September 2021 00:30\n**Duration:** 10 hours, 20 minutes\n**Affected Services and Features:**\n- Google Cloud Console: Cloud console user interface received a warning notification for US customers with a self-serve billing account.\n**Regions/Zones:** US\n**Description:**\nGoogle Cloud customers received a warning message on the Google Cloud console user interface and an email regarding an issue with processing their account payment. Impacted customers were redirected to update payment information. Attempts to update the primary payment method were unsuccessful, and the issues persisted. Initial investigation of this issue revealed the impact was triggered after an update to the payment configuration was released during the impact time frame.\n**Customer Impact:**\nGoogle Cloud customers received a warning message on the Google Cloud console user interface and an email: “Your payment information could not be processed. Visit the payment overview page to make sure your payment information is up to date and to pay any outstanding charges.”\n- After customers clicked on the “Fix It” option, a pop-up message was displayed, prompting them to update their primary payment method.\n- Customers were unable to fix the account after updating the primary payment method and the warning message reoccurred post the customer action.\nAfter resolution, the account warning message disappeared, and the account state was set back to the proper Payments setting.\n**Additional details:**\nOur engineering team has mitigated the customer impact after complete rollback of the changes that triggered the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-23T20:19:01+00:00","modified":"2021-09-23T20:19:01+00:00","when":"2021-09-23T20:19:01+00:00","text":"**Summary:** Global: Cloud console user interface received a warning notification for customers with a self-serve billing account.\n**Description:** We experienced an issue with Cloud Console where customers observed warning notifications related to payment method, beginning at Wednesday, 2021-09-22 14:10 US/Pacific.\n**Diagnosis:** Customers may receive \"Payment failed\" notifications. Error Message: “Your payment information could not be processed. Visit the payment overview page to make sure your payment information is up to date and to pay any outstanding charges.”","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-29T17:55:58+00:00","modified":"2021-09-29T17:55:58+00:00","when":"2021-09-29T17:55:58+00:00","text":"INCIDENT REPORT (Combined GCP / WS)\nSummary\nOn 22 September 2021 14:10 US/Pacific, Google Cloud, Workspace, and Geo (Maps) customers in the United States (U.S.) incorrectly received alert notifications on the Google Cloud Console user interface and an email regarding an issue with processing automatic payments. The issue persisted for 11 hours, 20 minutes.\nAll customers in the United States who use credit card billing were inadvertently notified via email “Your payment information could not be processed. Visit the payment overview page to make sure your payment information is up to date and to pay any outstanding charges.” Customers also received a notification alert in the Google Cloud Console to update their primary payment method. After customers attempted to update their payment information in the admin console, they received a message that the payment could not be processed.\nWe sincerely apologize for the inconvenience caused by the incorrect notification to our customers. Our engineering team has deployed a fix for this and has ensured that account settings are restored. We are taking additional steps to prevent similar problems in the future, as outlined below.\nRoot Cause\nThe trigger for this service disruption was an update to the payment configuration intended to support virtual credit cards (one-time credit card numbers from Visa and MasterCard). This update unintentionally caused a situation in which no credit card payments were able to be processed in the U.S. region and resulted in payment failure notifications being sent out to affected customers. Google's standard change control policy is to roll out all production updates progressively; however, in this specific case, errors in the rollout process meant that the change rolled out much more rapidly. Addressing this is a key part of our corrective actions.\nRemediation and Prevention\nGoogle engineers were alerted to the issue on 22 September 2021 15:28 US/Pacific and immediately started an investigation. Engineers identified an unexpected increase of payment request failures and initiated a rollback of the configuration change at 17:42, which resolved the issue causing the incorrect email notifications as well as the Cloud Console alert notifications.\nOnce it was confirmed that affected customers were no longer prompted with the notification alert and impact had ended, engineers began mitigation work to update account state back to the proper payment settings to prevent a recurrence.\nTo prevent recurrence and reduce the impact of similar events, we are taking the following actions:\nImproving our payments infrastructure monitoring and alerting configuration with “early indicator” signals for faster detection and notification of widespread payment failures.\nSpeeding up payment system mitigation procedures by automating the root causing of problems, and increasing the rate at which recovery steps can be processed.\nEnsuring that the class of change which resulted in this incident is pushed progressively, with time to detect adverse impact at small scope.\nDetailed Description of Impact\n#### Google Cloud Console - Google Cloud Platform and Geo Maps\nU.S. customer accounts received an alert notification in their admin account warning that they needed to verify their payment method. When attempting to update the preferred credit card method, the user interface was unresponsive.\nCustomers received an email warning message informing them they needed to verify their payment method. They also might have received an email warning that their project was going to be suspended. After the issue was resolved, account alert notifications disappeared, and the account state was set back to the proper payments setting. Some customers may have observed restrictions and were unable to create new resources. These restrictions were corrected when the account was returned to good standing.\nThe period of impact was between 14:10 and 00:30.\n#### Google Admin Console - Google Workspace\nU.S. customer accounts received an alert notification in their admin account warning that they needed to verify their payment method. When attempting to update the preferred credit card method, the user interface was unresponsive.\nCustomers received an email warning they needed to verify their payment method. When they attempted to update the credit card they are currently using, they were unable to do so. After the issue was resolved, account alert notifications disappeared, and the account state was set back to the proper payments setting.\nThe period of impact was between 14:10 and 01:30.\nAdditional Information for Customers\nThe impacted geographical scope used in the preliminary incident statement was incorrect. While impact initially was suspected to be global, our engineers confirmed impact only affected U.S. customers with credit cards stored for automatic payments. The corrected details for impacted regions / zones are contained in the above report.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"Wdsr1n5vyDvCt78qEifm","service_name":"Google Cloud Console","affected_products":[{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"}],"uri":"incidents/E18Caoo5X1m6dTa1PVr1","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"AQ2XT5utr5zovdxoE64s","number":"1346994744969028346","begin":"2021-09-22T10:30:00+00:00","created":"2021-09-22T14:43:18+00:00","end":"2021-09-23T18:49:00+00:00","modified":"2021-09-24T17:25:07+00:00","external_desc":"Google Engineers identified an issue with Cloud Build Github Pull Request Triggers.","updates":[{"created":"2021-09-24T17:25:03+00:00","modified":"2021-09-24T17:25:03+00:00","when":"2021-09-24T17:25:03+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 22 September 2021 03:30\n**Incident End:** 23 September 2021 11:49\n**Duration:** 1 day, 8 hours, 19 minutes\n**Affected Services and Features:**\n- Google Cloud Platform - Cloud Build service triggers failed\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Build service, used to run builds on Google Cloud Platform (GCP), experienced integration issues with GitHub. In this case, the Pull Request Triggers – with comment control enabled – failed for some users with the ‘writer’ role. From preliminary analysis, the issue was caused by an unexpected response received from the GitHub collaborator API call used to validate user permissions.\n**Customer Impact:**\n- GCP builds failed to initiate when GitHub Pull Requests were created by users granted ‘write’ permissions on a repository via team permissions.\n- “Needs /gcbrun from a collaborator” error message was displayed after Triggers failed.\n**Additional details:**\n- The issue was mitigated after our engineering team rolled out a mitigation to unblock comment controlled triggers that are invoked by users with the ‘writer’ role.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-23T21:47:08+00:00","modified":"2021-09-23T21:47:09+00:00","when":"2021-09-23T21:47:08+00:00","text":"The issue with Cloud Build has been resolved for all affected users as of Thursday, 2021-09-23 14:45 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-23T16:38:52+00:00","modified":"2021-09-23T16:38:53+00:00","when":"2021-09-23T16:38:52+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: Mitigation work is still underway by our engineering team.\nThe mitigation is expected to complete by Thursday, 2021-09-23 14:45 US/Pacific.}\nWe will provide more information by Thursday, 2021-09-23 15:00 US/Pacific.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-23T16:09:24+00:00","modified":"2021-09-23T16:09:24+00:00","when":"2021-09-23T16:09:24+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: Mitigation work is still currently underway by our engineering team.\nWe will provide more information by Thursday, 2021-09-23 10:00 US/Pacific.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-23T15:30:04+00:00","modified":"2021-09-23T15:30:06+00:00","when":"2021-09-23T15:30:04+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Thursday, 2021-09-23 09:00 US/Pacific.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-23T15:11:52+00:00","modified":"2021-09-23T15:11:53+00:00","when":"2021-09-23T15:11:52+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Thursday, 2021-09-23 08:30 US/Pacific.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-23T03:53:40+00:00","modified":"2021-09-23T03:53:40+00:00","when":"2021-09-23T03:53:40+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Thursday, 2021-09-23 07:00 US/Pacific.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-23T01:04:20+00:00","modified":"2021-09-23T01:04:20+00:00","when":"2021-09-23T01:04:20+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: Mitigation work is currently underway by our engineering team.\nWe will provide more information by Wednesday, 2021-09-22 21:00 US/Pacific.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T22:46:44+00:00","modified":"2021-09-22T22:46:49+00:00","when":"2021-09-22T22:46:44+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers.\nDescription: We are continuing to make progress with partners on investigating this issue. Options for mitigation are being explored as well.\nWe will provide an update by Wednesday, 2021-09-22 18:00 US/Pacific with latest details.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T20:45:54+00:00","modified":"2021-09-22T20:45:54+00:00","when":"2021-09-22T20:45:54+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers\nDescription: We are continuing to work with our partner to investigate the issue, and exploring other options for mitigation.\nWe will provide an update by Wednesday, 2021-09-22 16:00 US/Pacific with current details.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T18:40:07+00:00","modified":"2021-09-22T18:40:07+00:00","when":"2021-09-22T18:40:07+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers\nDescription: We are continuing to work with our partner to investigate the issue.\nWe will provide an update by Wednesday, 2021-09-22 14:00 US/Pacific with current details.\nDiagnosis: Users with write access to a repository via a team are no longer considered collaborators in the GitHub API. This affects customers with GitHub Pull Request comment control triggers as certain users will no longer be able to trigger builds.\nWorkaround: To workaround this, the users should be given the writer role to a repository directly.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T17:14:19+00:00","modified":"2021-09-22T17:14:19+00:00","when":"2021-09-22T17:14:19+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers\nDescription: We have attempted a rollback of the current release in an effort to mitigate the issue, but have not seen improvements in the situation. We are continuing to work with our partner to investigate the issue.\nWe will provide an update by Wednesday, 2021-09-22 12:00 US/Pacific with current details.\nDiagnosis: GitHub Pull Request Triggers with comment control enabled are not firing for customers with the \"writer\" role.\nWorkaround: A workaround is to use the \"collaborator\" role.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T16:28:45+00:00","modified":"2021-09-22T16:28:51+00:00","when":"2021-09-22T16:28:45+00:00","text":"Summary: Google Engineers are investigating an issue with Cloud Build Github Pull Request Triggers\nDescription: We are experiencing an issue with Cloud Build related to Github Pull Request Triggers.\nOur engineering team continues to investigate this issue to identify a mitigation. We are also reaching out to our partner to assist with the investigation.\nWe will provide an update by Wednesday, 2021-09-22 10:30 US/Pacific with current details.\nDiagnosis: GitHub Pull Request Triggers with comment control enabled are not firing for customers with the \"writer\" role.\nWorkaround: A workaround is to use the \"collaborator\" role.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T15:51:07+00:00","modified":"2021-09-22T15:51:07+00:00","when":"2021-09-22T15:51:07+00:00","text":"Summary: Google Engineers are investigating an Issue with Cloud Build\nDescription: We are experiencing an issue with Cloud Build related to Github Pull Request Triggers.\nOur engineering team continues to investigate this issue to identify a mitigation.\nWe will provide an update by Wednesday, 2021-09-22 09:35 US/Pacific with current details.\nDiagnosis: GitHub Pull Request Triggers with comment control enabled are not firing for customers with the \"writer\" role\nWorkaround: A workaround is to use the \"collaborator\" role.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T15:06:20+00:00","modified":"2021-09-22T15:06:26+00:00","when":"2021-09-22T15:06:20+00:00","text":"Summary: Google Engineers are investigating an Issue with Cloud Build\nDescription: We are experiencing an issue with Cloud Build related to Github Pull Request Triggers\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-09-22 09:15 US/Pacific with current details.\nDiagnosis: GitHub Pull Request Triggers with comment control enabled are not firing for customers with the \"writer\" role\nWorkaround: A workaround is to use the \"collaborator\" role.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-22T14:43:09+00:00","modified":"2021-09-22T14:43:18+00:00","when":"2021-09-22T14:43:09+00:00","text":"Summary: Google Engineers are investigating an Issue with Cloud Build\nDescription: We are experiencing an issue with Cloud Build related to Github Pull Request Triggers\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-09-22 09:15 US/Pacific with current details.\nDiagnosis: There is a regression for Github Pull Request Triggers with comment control enabled. Customers with the \"writer\" role may not be able to trigger builds\nWorkaround: A workaround is to use the \"collaborator\" role.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-24T17:25:03+00:00","modified":"2021-09-24T17:25:03+00:00","when":"2021-09-24T17:25:03+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 22 September 2021 03:30\n**Incident End:** 23 September 2021 11:49\n**Duration:** 1 day, 8 hours, 19 minutes\n**Affected Services and Features:**\n- Google Cloud Platform - Cloud Build service triggers failed\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Build service, used to run builds on Google Cloud Platform (GCP), experienced integration issues with GitHub. In this case, the Pull Request Triggers – with comment control enabled – failed for some users with the ‘writer’ role. From preliminary analysis, the issue was caused by an unexpected response received from the GitHub collaborator API call used to validate user permissions.\n**Customer Impact:**\n- GCP builds failed to initiate when GitHub Pull Requests were created by users granted ‘write’ permissions on a repository via team permissions.\n- “Needs /gcbrun from a collaborator” error message was displayed after Triggers failed.\n**Additional details:**\n- The issue was mitigated after our engineering team rolled out a mitigation to unblock comment controlled triggers that are invoked by users with the ‘writer’ role.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Cloud Developer Tools","id":"BGJQ6jbGK4kUuBTQFZ1G"},{"title":"Cloud Build","id":"fw8GzBdZdqy4THau7e1y"}],"uri":"incidents/AQ2XT5utr5zovdxoE64s","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"rjF86FbooET3FDpMV9w1","number":"1694098087281257421","begin":"2021-09-17T15:00:00+00:00","created":"2021-09-17T14:39:52+00:00","end":"2021-09-17T18:25:00+00:00","modified":"2021-09-20T23:33:53+00:00","external_desc":"Increased VM failure rates in a subset of Google Cloud zones","updates":[{"created":"2021-09-20T23:33:50+00:00","modified":"2021-09-20T23:33:50+00:00","when":"2021-09-20T23:33:50+00:00","text":"**Incident Start:** 17 September 2021 08:00\n**Incident End:** 17 September 2021 11:25\n**Duration:** 3 hours, 25 minutes\n**Affected Services and Features:**\nGoogle Compute Engine\n**Regions/Zones:** us-central1-f\n**Description:**\nGoogle Compute Engine reported elevated instance failures. From preliminary analysis an issue with a node software rollout was initially suspected, but subsequently ruled out. Due to the potential impact, we proactively notified customers on the Cloud Status Dashboard. However, further analysis concluded the error rates were negligible and not a cause for concern.\n**Customer Impact:**\nAfter analysis, it was determined this particular incident did not have any customer impact.\n**Additional details:**\nWe are continuing to enhance our detection mechanisms to avoid false positives.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-17T21:20:13+00:00","modified":"2021-09-17T21:20:13+00:00","when":"2021-09-17T21:20:13+00:00","text":"The issue with Google Compute Engine is believed to be affecting a very small number of customers and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-17T19:00:08+00:00","modified":"2021-09-17T19:00:09+00:00","when":"2021-09-17T19:00:08+00:00","text":"Summary: Increased VM failure rates in a subset of Google Cloud zones\nDescription: Mitigation work is still underway with our engineering team.\nWe do not have an ETA for mitigation at this point.\nAs a workaround, customers can use alternative zones.\nWe will provide more information by Friday, 2021-09-17 14:15 US/Pacific.\nDiagnosis: Customers may be experiencing higher VM failure rates.\nWorkaround: Use alternative zones.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-17T17:18:19+00:00","modified":"2021-09-17T17:18:19+00:00","when":"2021-09-17T17:18:19+00:00","text":"Summary: Increased VM failure rates in a subset of Google Cloud zones\nDescription: Mitigation work is still underway with our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-09-17 12:15 US/Pacific.\nDiagnosis: Customers may be experiencing higher VM failure rates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-17T15:52:25+00:00","modified":"2021-09-17T15:52:26+00:00","when":"2021-09-17T15:52:25+00:00","text":"Summary: Increased VM failure rates in a subset of Google Cloud zones\nDescription: Mitigation work is still underway with our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-09-17 10:15 US/Pacific.\nDiagnosis: Customers may be experiencing higher VM failure rates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-17T15:06:50+00:00","modified":"2021-09-17T15:06:52+00:00","when":"2021-09-17T15:06:50+00:00","text":"Summary: Increased VM failure rates in a subset of Google Cloud zones\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-09-17 09:00 US/Pacific.\nDiagnosis: Customers may be experiencing higher VM failure rates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-17T14:39:44+00:00","modified":"2021-09-17T14:39:53+00:00","when":"2021-09-17T14:39:44+00:00","text":"Summary: Increased VM failure rates in a subset of Google Cloud zones\nDescription: We are experiencing an intermittent issue with Google Compute Engine in the following zones:\nasia-east1-c\nasia-east2-c\nasia-northeast1-c\nasia-northeast2-c\nasia-northeast3-c\nasia-south1-c\nasia-south2-c\nasia-southeast1-c\nasia-southeast2-c\naustralia-southeast1-c\naustralia-southeast2-c\neurope-central2-c\neurope-north1-c\neurope-west1-c\neurope-west2-c\neurope-west3-c\neurope-west4-c\neurope-west6-c\nnorthamerica-northeast1-c\nnorthamerica-northeast2-c\nsouthamerica-east1-c\nus-central1-c\nus-central1-f\nus-east1-c\nus-east4-c\nus-west1-c\nus-west2-a\nus-west2-b\nus-west2-c\nus-west3-c\nus-west4-c\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-09-17 08:10 US/Pacific with current details.\nDiagnosis: Customers may be experiencing higher VM failure rates.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-20T23:33:50+00:00","modified":"2021-09-20T23:33:50+00:00","when":"2021-09-20T23:33:50+00:00","text":"**Incident Start:** 17 September 2021 08:00\n**Incident End:** 17 September 2021 11:25\n**Duration:** 3 hours, 25 minutes\n**Affected Services and Features:**\nGoogle Compute Engine\n**Regions/Zones:** us-central1-f\n**Description:**\nGoogle Compute Engine reported elevated instance failures. From preliminary analysis an issue with a node software rollout was initially suspected, but subsequently ruled out. Due to the potential impact, we proactively notified customers on the Cloud Status Dashboard. However, further analysis concluded the error rates were negligible and not a cause for concern.\n**Customer Impact:**\nAfter analysis, it was determined this particular incident did not have any customer impact.\n**Additional details:**\nWe are continuing to enhance our detection mechanisms to avoid false positives.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"L3ggmi3Jy4xJmgodFA9K","service_name":"Google Compute Engine","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"}],"uri":"incidents/rjF86FbooET3FDpMV9w1","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"nkUtGfx3WQcAB1iLEtcd","number":"17175204310165775620","begin":"2021-09-13T17:20:00+00:00","created":"2021-09-17T20:10:07+00:00","end":"2021-09-18T08:27:00+00:00","modified":"2021-09-20T19:27:09+00:00","external_desc":"Global: Cloud Networking connectivity issues for customers using packet mirroring.","updates":[{"created":"2021-09-20T19:25:32+00:00","modified":"2021-09-20T19:25:32+00:00","when":"2021-09-20T19:25:32+00:00","text":"We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 13 September 2021 10:20\n**Incident End:** 18 September 2021 01:27\n**Duration:** 4 days, 15 hours, 7 minutes\n**Affected Services and Features:**\n- Google Compute Engine – Access to Google Cloud services such as Google Cloud Storage, Cloud Bigtable, Cloud Spanner, and Google Container Registry were affected for Virtual Machine (VMs) with packet mirroring enabled.\n- Google Kubernetes Engine – Clusters with mirroring enabled had issues accessing Google endpoints including Google Container Registry and Google Cloud storage buckets.\n**Regions/Zones:** Global\n**Description:**\nGoogle Compute Engine (GCE) VM instances and Google Kubernetes Engine (GKE) instances were unable to access Google Cloud services if the packet mirroring feature was enabled.\nFrom preliminary analysis, the root cause was identified as an issue in the packet mirroring functionality that resulted in packet drops for VM to Google Cloud Services endpoint connections.\n**Customer Impact:**\n* GCE VMs with packet mirroring enabled were unable to access Google services. * Affected VMs were unable to retrieve Google Container Registry (GCR) images. * Affected VMs experienced timeouts when accessing Google Services.\n* GKE clusters with packet mirroring enabled were unable to access Google services.\n**Additional details:**\n* As a workaround, customers were informed to disable packet mirroring (if enabled) in order to reduce the impact on affected projects.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-18T09:47:54+00:00","modified":"2021-09-18T10:01:57+00:00","when":"2021-09-18T09:47:54+00:00","text":"The issue with Cloud Networking has been mitigated for all affected projects and there is no customer impact as of Saturday, 2021-09-18 01:29 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-18T07:07:07+00:00","modified":"2021-09-18T07:07:07+00:00","when":"2021-09-18T07:07:07+00:00","text":"Summary: Global: Cloud Networking connectivity issues for customers using packet mirroring.\nDescription: Engineering team is continuing to monitor the mitigation steps currently being rolled out and expect it to complete by Saturday, 2021-09-18 03:00 US/Pacific.\nCustomers can disable packet mirroring as a workaround.\nWe will provide more information by Saturday, 2021-09-18 03:00 US/Pacific.\nDiagnosis: Customers might experience connection failures or delays for connections from GCE VMs to Google endpoints including gcr.io and Google Cloud Storage buckets.\nWorkaround: Disable packet mirroring.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-18T05:43:16+00:00","modified":"2021-09-18T05:43:22+00:00","when":"2021-09-18T05:43:16+00:00","text":"Summary: Global: Cloud Networking connectivity issues for customers using packet mirroring.\nDescription: Engineering team is continuing to monitor the mitigation steps currently being rolled out and expect it to complete by Saturday, 2021-09-18 01:00 US/Pacific.\nCustomers can disable packet mirroring as a workaround.\nWe will provide more information by Saturday, 2021-09-18 01:00 US/Pacific.\nDiagnosis: Customers might experience connection failures or delays for connections from GCE VMs to Google endpoints including gcr.io and Google Cloud Storage buckets.\nWorkaround: Disable packet mirroring.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-18T02:22:07+00:00","modified":"2021-09-18T02:22:12+00:00","when":"2021-09-18T02:22:07+00:00","text":"Summary: Global: Cloud Networking connectivity issues for customers using packet mirroring.\nDescription: Mitigation work is underway by our engineering team. The mitigation is expected to complete by Friday, 2021-09-17 23:00 US/Pacific.\nCustomers can disable packet mirroring as a workaround.\nWe will provide more information by Friday, 2021-09-17 23:00 US/Pacific.\nDiagnosis: Customers might experience connection failures or delays for connections from GCE VMs to Google endpoints including gcr.io and Google Cloud Storage buckets.\nWorkaround: Disable packet mirroring.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-17T20:10:06+00:00","modified":"2021-09-17T20:10:07+00:00","when":"2021-09-17T20:10:06+00:00","text":"Summary: Global: Cloud Networking connectivity issues for customers using packet mirroring.\nDescription: Mitigation work is underway by our engineering team. The mitigation is expected to complete by Friday, 2021-09-17 10:00 US/Pacific.\nCustomers can disable packet mirroring as a workaround.\nWe will provide more information by Friday, 2021-09-17 23:00 US/Pacific.\nDiagnosis: Customers might experience connection failures or delays for connections from GCE VMs to Google endpoints including gcr.io and Google Cloud Storage buckets.\nWorkaround: Disable packet mirroring.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-20T19:25:32+00:00","modified":"2021-09-20T19:25:32+00:00","when":"2021-09-20T19:25:32+00:00","text":"We apologize for the inconvenience this service outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 13 September 2021 10:20\n**Incident End:** 18 September 2021 01:27\n**Duration:** 4 days, 15 hours, 7 minutes\n**Affected Services and Features:**\n- Google Compute Engine – Access to Google Cloud services such as Google Cloud Storage, Cloud Bigtable, Cloud Spanner, and Google Container Registry were affected for Virtual Machine (VMs) with packet mirroring enabled.\n- Google Kubernetes Engine – Clusters with mirroring enabled had issues accessing Google endpoints including Google Container Registry and Google Cloud storage buckets.\n**Regions/Zones:** Global\n**Description:**\nGoogle Compute Engine (GCE) VM instances and Google Kubernetes Engine (GKE) instances were unable to access Google Cloud services if the packet mirroring feature was enabled.\nFrom preliminary analysis, the root cause was identified as an issue in the packet mirroring functionality that resulted in packet drops for VM to Google Cloud Services endpoint connections.\n**Customer Impact:**\n* GCE VMs with packet mirroring enabled were unable to access Google services. * Affected VMs were unable to retrieve Google Container Registry (GCR) images. * Affected VMs experienced timeouts when accessing Google Services.\n* GKE clusters with packet mirroring enabled were unable to access Google services.\n**Additional details:**\n* As a workaround, customers were informed to disable packet mirroring (if enabled) in order to reduce the impact on affected projects.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/nkUtGfx3WQcAB1iLEtcd","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"SDkKgEgDZJCrvutAz3bn","number":"3108142056827409484","begin":"2021-09-09T19:06:03+00:00","created":"2021-09-09T19:06:04+00:00","end":"2021-09-09T19:54:38+00:00","modified":"2021-09-09T22:25:23+00:00","external_desc":"europe-central2, europe-west3: Elevated error rates for GKE control plane","updates":[{"created":"2021-09-09T22:25:23+00:00","modified":"2021-09-09T22:25:24+00:00","when":"2021-09-09T22:25:23+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 09 September 2021 10:46\n**Incident End:** 09 September 2021 12:44\n**Duration:** 1 hour, 58 minutes\n**Affected Services and Features:**\n- Google Kubernetes Engine (GKE) - Cluster operations\n**Regions/Zones:** europe-west3 , europe-central2\n**Description:**\nGoogle Kubernetes Engine (GKE) experienced elevated error rates intermittently in europe-west3 and europe-central2 for a duration of 1 hour, 58 minutes. From preliminary analysis, the issue was caused by an ongoing rollout in the backend authentication service. The backend authentication service was rolled back, ending impact at 12:44.\n**Customer Impact:**\n- Less than 0.5% of cluster operations that require an authentication token experienced elevated error rates.\n- Elevated error rates on GKE master pods that required authorization to change GCE resources.\n**Additional details:**\nA retry of the failed operation would have completed successfully.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-09T19:54:37+00:00","modified":"2021-09-09T19:54:38+00:00","when":"2021-09-09T19:54:37+00:00","text":"The issue with Google Kubernetes Engine has been resolved for all affected projects as of Thursday, 2021-09-09 12:53 US/Pacific.\nThere is no further impact to our customers.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-09T19:48:23+00:00","modified":"2021-09-09T19:48:23+00:00","when":"2021-09-09T19:48:23+00:00","text":"Summary: europe-central2, europe-west3: Elevated error rates for GKE control plane\nDescription: Our engineering team is currently working on the mitigation of the issue.\nThe affected locations have reduced and updated for the incident post.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2021-09-09 14:00 US/Pacific.\nDiagnosis: Customers may experience issue with cluster availability and errors with CreateToken. Service account may intermittently succeed or fail.\nWorkaround: Customers who experience failure can retry.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-09T19:29:36+00:00","modified":"2021-09-09T19:29:36+00:00","when":"2021-09-09T19:29:36+00:00","text":"Summary: europe-central2, europe-west3, europe-west4, us-east4, asia-northeast1: Elevated error rates for GKE control plane\nDescription: We are experiencing an issue with Google Kubernetes Engine.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2021-09-09 14:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may experience issue with cluster availability and errors with CreateToken.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-09T19:18:18+00:00","modified":"2021-09-09T19:18:18+00:00","when":"2021-09-09T19:18:18+00:00","text":"Summary: europe-central2, europe-west3, europe-west4, us-east4, asia-northeast1: Elevated error rates for GKE control plane\nDescription: We are investigating a potential issue with Google Kubernetes Engine.\nWe will provide more information by Thursday, 2021-09-09 12:35 US/Pacific.\nDiagnosis: We are currently investigating the impact of this incident.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-09T19:06:04+00:00","modified":"2021-09-09T19:06:05+00:00","when":"2021-09-09T19:06:04+00:00","text":"Summary: europe-west3, europe-west4, us-east4, asia-northeast1: Elevated error rates for GKE control plane\nDescription: We are investigating a potential issue with Google Kubernetes Engine.\nWe will provide more information by Thursday, 2021-09-09 12:35 US/Pacific.\nDiagnosis: We are currently investigating the impact of this incident.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-09T22:25:23+00:00","modified":"2021-09-09T22:25:24+00:00","when":"2021-09-09T22:25:23+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 09 September 2021 10:46\n**Incident End:** 09 September 2021 12:44\n**Duration:** 1 hour, 58 minutes\n**Affected Services and Features:**\n- Google Kubernetes Engine (GKE) - Cluster operations\n**Regions/Zones:** europe-west3 , europe-central2\n**Description:**\nGoogle Kubernetes Engine (GKE) experienced elevated error rates intermittently in europe-west3 and europe-central2 for a duration of 1 hour, 58 minutes. From preliminary analysis, the issue was caused by an ongoing rollout in the backend authentication service. The backend authentication service was rolled back, ending impact at 12:44.\n**Customer Impact:**\n- Less than 0.5% of cluster operations that require an authentication token experienced elevated error rates.\n- Elevated error rates on GKE master pods that required authorization to change GCE resources.\n**Additional details:**\nA retry of the failed operation would have completed successfully.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LCSbT57h59oR4W98NHuz","service_name":"Google Kubernetes Engine","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/SDkKgEgDZJCrvutAz3bn","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"mRTeQ94fZRLdx2Q7NLLX","number":"6101118185948634658","begin":"2021-09-07T22:04:00+00:00","created":"2021-09-07T22:57:42+00:00","end":"2021-09-07T22:45:00+00:00","modified":"2021-09-08T17:26:25+00:00","external_desc":"Regions: (Asia, Western US) \u003c-\u003e (Eastern US, Europe). Packet loss on inter region traffic.","updates":[{"created":"2021-09-08T17:25:57+00:00","modified":"2021-09-08T17:25:57+00:00","when":"2021-09-08T17:25:57+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 07 September 2021 15:04\n**Incident End:** 07 September 2021 15:45\n**Duration:** 41 minutes\n**Affected Services and Features:**\n- Google Cloud Networking - Cloud VPC, Cloud Interconnect.\n- Ingress and Egress traffic of any Google Cloud services to and from the affected regions.\n**Regions/Zones:**\nThe traffic between multiple regions (inter-region) was affected, while the traffic within each region (intra-region) remained unaffected. The regions asia-east, asia-northeast, asia-southeast, australia-southeast, us-west, and us-central had trouble connecting to the regions of us-east, northamerica-northeast, europe-central, europe-north, and europe-west, and vice-versa.\n**Description:**\nGoogle Cloud Networking experienced elevated packet loss for traffic between multiple regions for a duration of 41 minutes. Preliminary analysis indicates that during routine and typically non-disruptive router maintenance, a software issue occurred which unintentionally impacted packet forwarding.\n**Customer Impact:**\n- Google Cloud Networking - elevated packet loss\n- Other services using Cloud Networking - elevated latencies or other networking related delays.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-07T23:16:21+00:00","modified":"2021-09-07T23:16:21+00:00","when":"2021-09-07T23:16:21+00:00","text":"The issue with Cloud Networking has been resolved for all affected projects as of Tuesday, 2021-09-07 15:45 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-07T23:07:09+00:00","modified":"2021-09-07T23:07:14+00:00","when":"2021-09-07T23:07:09+00:00","text":"Summary: Regions: (Asia, Western US) \u003c-\u003e (Eastern US, Europe). Packet loss on inter region traffic. Mitigation completed.\nDescription: We believe the issue with Cloud Networking is mitigated starting 15:45 US/Pacific . Our engineering team continues to work towards full resolution of the issue.\nWe will provide an update by Tuesday, 2021-09-07 16:37 US/Pacific with current details.\nDiagnosis: Customer will see packet loss on traffic between regions in US and Asia\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-07T23:05:27+00:00","modified":"2021-09-07T23:05:28+00:00","when":"2021-09-07T23:05:27+00:00","text":"Summary: Regions: (Asia, Western US) \u003c-\u003e (Eastern US, Europe). Packet loss on inter region traffic. Mitigation completed.\nDescription: We believe the issue with Cloud Networking is mitigated starting 15:45. Our engineering team continues to work towards full resolution of the issue.\nWe will provide an update by Tuesday, 2021-09-07 16:37 US/Pacific with current details.\nDiagnosis: Customer will see packet loss on traffic between regions in US and Asia\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-07T22:57:36+00:00","modified":"2021-09-07T22:57:42+00:00","when":"2021-09-07T22:57:36+00:00","text":"Summary: Regions: US, Asia: Packet loss reported on GCE traffic between US and Asia\nDescription: We are experiencing an issue with Cloud Networking beginning at Tuesday, 2021-09-07 15:04 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-09-07 16:28 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customer will see packet loss on traffic between regions in US and Asia\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-08T17:25:57+00:00","modified":"2021-09-08T17:25:57+00:00","when":"2021-09-08T17:25:57+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 07 September 2021 15:04\n**Incident End:** 07 September 2021 15:45\n**Duration:** 41 minutes\n**Affected Services and Features:**\n- Google Cloud Networking - Cloud VPC, Cloud Interconnect.\n- Ingress and Egress traffic of any Google Cloud services to and from the affected regions.\n**Regions/Zones:**\nThe traffic between multiple regions (inter-region) was affected, while the traffic within each region (intra-region) remained unaffected. The regions asia-east, asia-northeast, asia-southeast, australia-southeast, us-west, and us-central had trouble connecting to the regions of us-east, northamerica-northeast, europe-central, europe-north, and europe-west, and vice-versa.\n**Description:**\nGoogle Cloud Networking experienced elevated packet loss for traffic between multiple regions for a duration of 41 minutes. Preliminary analysis indicates that during routine and typically non-disruptive router maintenance, a software issue occurred which unintentionally impacted packet forwarding.\n**Customer Impact:**\n- Google Cloud Networking - elevated packet loss\n- Other services using Cloud Networking - elevated latencies or other networking related delays.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Virtual Private Cloud (VPC)","id":"BSGtCUnz6ZmyajsjgTKv"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/mRTeQ94fZRLdx2Q7NLLX","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"16SSwVXrYSLjy8fEMvyZ","number":"1280580001127136058","begin":"2021-09-07T18:19:00+00:00","created":"2021-09-08T19:17:06+00:00","end":"2021-09-08T18:00:00+00:00","modified":"2021-09-09T21:09:57+00:00","external_desc":"us-central1: Cloud Functions experiencing increased latency and pending queue aborted error request rate.","updates":[{"created":"2021-09-09T21:09:57+00:00","modified":"2021-09-09T21:09:57+00:00","when":"2021-09-09T21:09:57+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 07 September 2021 11:19\n**Incident End:** 08 September 2021 11:00\n**Duration:** 23 hours, 41 minutes\n**Affected Services and Features:**\n- Google App Engine - Standard\n- Google Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Google Cloud Functions experienced elevated latency on requests in us-central1 on a small number of projects for a duration of 23 hours, 41 minutes. From preliminary analysis, the root cause of the issue is due to a sudden increase in load within the region.\n**Customer Impact:**\n- Affected customers would have experienced elevated serving latency on apps and functions.\n- This may have manifested with http errors and corresponding cloud logging entries such as: “Request was aborted after waiting too long to attempt to service your request.”\n**Additional Details:**\n- Impact was mitigated by applying targeted throttling.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-08T19:28:29+00:00","modified":"2021-09-08T19:28:29+00:00","when":"2021-09-08T19:28:29+00:00","text":"The issue with Google Cloud Functions has been resolved for all affected projects as of Wednesday, 2021-09-08 12:27 US/Pacific.\nPlease follow the link to status dashboard post with more details: https://status.cloud.google.com/incidents/uaRinwS8pu2yyjdYbDaM\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-08T19:16:59+00:00","modified":"2021-09-08T19:17:06+00:00","when":"2021-09-08T19:16:59+00:00","text":"Summary: us-central1: Cloud Functions experiencing increased latency and pending queue aborted error request rate.\nDescription: Engineering team investigation has identified increased latency and an increase in pushbacks to requests (increase in pending queue aborted error request rate) for some customers.\nOur engineering team continues to work on the mitigation to reduce the pushback errors and reduce the latency observed.\nFor further updates please follow this link: https://status.cloud.google.com/incidents/uaRinwS8pu2yyjdYbDaM\nDiagnosis: Customer may observe increased latency and heightened pending queue aborted error request rate.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-09T21:09:57+00:00","modified":"2021-09-09T21:09:57+00:00","when":"2021-09-09T21:09:57+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 07 September 2021 11:19\n**Incident End:** 08 September 2021 11:00\n**Duration:** 23 hours, 41 minutes\n**Affected Services and Features:**\n- Google App Engine - Standard\n- Google Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Google Cloud Functions experienced elevated latency on requests in us-central1 on a small number of projects for a duration of 23 hours, 41 minutes. From preliminary analysis, the root cause of the issue is due to a sudden increase in load within the region.\n**Customer Impact:**\n- Affected customers would have experienced elevated serving latency on apps and functions.\n- This may have manifested with http errors and corresponding cloud logging entries such as: “Request was aborted after waiting too long to attempt to service your request.”\n**Additional Details:**\n- Impact was mitigated by applying targeted throttling.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"}],"uri":"incidents/16SSwVXrYSLjy8fEMvyZ","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"uaRinwS8pu2yyjdYbDaM","number":"13800817493833147491","begin":"2021-09-07T18:19:00+00:00","created":"2021-09-08T18:41:04+00:00","end":"2021-09-08T18:00:00+00:00","modified":"2021-09-09T21:09:56+00:00","external_desc":"us-central1: Google App Engine Standard experiencing increased latency and pending queue aborted error request rate.","updates":[{"created":"2021-09-09T21:09:56+00:00","modified":"2021-09-09T21:09:56+00:00","when":"2021-09-09T21:09:56+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 07 September 2021 11:19\n**Incident End:** 08 September 2021 11:00\n**Duration:** 23 hours, 41 minutes\n**Affected Services and Features:**\n- Google App Engine - Standard\n- Google Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Google Cloud Functions experienced elevated latency on requests in us-central1 on a small number of projects for a duration of 23 hours, 41 minutes. From preliminary analysis, the root cause of the issue is due to a sudden increase in load within the region.\n**Customer Impact:**\n- Affected customers would have experienced elevated serving latency on apps and functions.\n- This may have manifested with http errors and corresponding cloud logging entries such as: “Request was aborted after waiting too long to attempt to service your request.”\n**Additional Details:**\n- Impact was mitigated by applying targeted throttling.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-08T19:25:36+00:00","modified":"2021-09-08T19:25:41+00:00","when":"2021-09-08T19:25:36+00:00","text":"The issue with Google App Engine and Cloud Functions has been resolved for all affected projects as of Wednesday, 2021-09-08 12:24 US/Pacific.\nThere is no further impact to our customers.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-08T18:40:57+00:00","modified":"2021-09-08T18:41:05+00:00","when":"2021-09-08T18:40:57+00:00","text":"Summary: us-central1: Google App Engine Standard and Cloud Functions experiencing increased latency and pending queue aborted error request rate.\nDescription: Further investigation into the increased latency issue has confirmed that we are also observing an increase in pushbacks to requests (increase in pending queue aborted error request rate) for some customers.\nOur engineering team continues to work on the mitigation to reduce the pushback errors and reduce the latency observed.\nWe will provide more information by Wednesday, 2021-09-08 12:30 US/Pacific.\nDiagnosis: Customer may observe increased latency and heightened pending queue aborted error request rate.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-09T21:09:56+00:00","modified":"2021-09-09T21:09:56+00:00","when":"2021-09-09T21:09:56+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 07 September 2021 11:19\n**Incident End:** 08 September 2021 11:00\n**Duration:** 23 hours, 41 minutes\n**Affected Services and Features:**\n- Google App Engine - Standard\n- Google Cloud Functions\n**Regions/Zones:** us-central1\n**Description:**\nGoogle App Engine and Google Cloud Functions experienced elevated latency on requests in us-central1 on a small number of projects for a duration of 23 hours, 41 minutes. From preliminary analysis, the root cause of the issue is due to a sudden increase in load within the region.\n**Customer Impact:**\n- Affected customers would have experienced elevated serving latency on apps and functions.\n- This may have manifested with http errors and corresponding cloud logging entries such as: “Request was aborted after waiting too long to attempt to service your request.”\n**Additional Details:**\n- Impact was mitigated by applying targeted throttling.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"}],"uri":"incidents/uaRinwS8pu2yyjdYbDaM","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"WzDzgmTjQfAKPJha6Rbb","number":"3409436363697907976","begin":"2021-09-03T12:40:00+00:00","created":"2021-09-03T13:39:38+00:00","end":"2021-09-03T13:20:00+00:00","modified":"2021-09-03T19:27:11+00:00","external_desc":"europe-west2-c: Cloud networking is currently experiencing issues.","updates":[{"created":"2021-09-03T19:27:00+00:00","modified":"2021-09-03T19:27:00+00:00","when":"2021-09-03T19:27:00+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 03 September 2021 05:40\n**Incident End:** 03 September 2021 06:20\n**Duration:** 40 minutes\n**Affected Services and Features:**\n- Cloud Networking\n- Compute Engine\n- Persistent Disk\n- Kubernetes Engine\n- App Engine\n- Cloud Run\n- Cloud Bigtable\n- Cloud Storage\n- Cloud Filestore\n- Cloud MemoryStore\n- Cloud Monitoring\n- Cloud VPN\n**Regions/Zones:** europe-west2-c\n**Description:**\nGoogle Cloud Networking experienced elevated packet loss and latency in europe-west2-c for a duration of 40 minutes. Additional Cloud services that use Cloud Networking experienced similar symptoms. From preliminary analysis, the root cause of the issue was a sudden redirection of traffic from a datacenter that was experiencing an issue.\n**Customer Impact:**\n- Affected customers may have experienced elevated packet loss and latency as their traffic was temporarily redirected.\n- Google Compute Engine instances experienced failed connectivity and packet loss on ingress, egress, and inter-zonal traffic. Autoscaling was also impacted and may have failed to make decisions.\n- Persistent Disk experienced elevated I/O latency on regional and zonal disks\n- Google Kubernetes Engine clusters experienced 30% of zonal and 5% of regional clusters reporting as unhealthy.\n- Google App Engine and Cloud Run served elevated 500 errors within europe-west2.\n- Cloud Bigtable experienced an elevated error rate and latency on operations within europe-west2.\n- Cloud SQL experienced issues connecting to instances in europe-west2. Affected instances may have lost telemetry data, and HA instances failed over if the primary was in the affected zone.\n- Google Cloud Storage experienced elevated read errors of up to 2.3% in europe-west2.\n- Cloud Filestore instances within europe-west2-c may have been unreachable.\n- Cloud MemoryStore instances within europe-west2 may have been unreachable.\n- Cloud Monitoring experienced delayed or missing metrics from resources in the zone.\n- Cloud VPN experienced a short duration where tunnels went down for a short time before being reestablished.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-03T14:17:31+00:00","modified":"2021-09-03T14:17:32+00:00","when":"2021-09-03T14:17:31+00:00","text":"The issue with Cloud Networking has been resolved for all affected users as of Friday, 2021-09-03 06:20 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-03T14:06:16+00:00","modified":"2021-09-03T14:06:16+00:00","when":"2021-09-03T14:06:16+00:00","text":"Summary: europe-west2-c: Cloud networking is currently experiencing issues.\nDescription: On further investigation we have confirmed the issue is affecting a single zone europe-west2-c.\nOur engineering team is currently investigating the issue and confirming the current impact to our users.\nWe will provide an update by Friday, 2021-09-03 07:30 US/Pacific with current details.\nDiagnosis: Customer may high latency and packet loss.\nWorkaround: Other Zone and region can be utilized for workaround.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-03T14:02:27+00:00","modified":"2021-09-03T14:02:28+00:00","when":"2021-09-03T14:02:27+00:00","text":"Summary: europe-west2: Cloud networking is currently experiencing issues.\nDescription: We are experiencing an issue with Cloud Networking , beginning at Friday, 2021-09-03 0535 US/Pacific\nOur engineering team is currently investigating the issue and confirming the current impact to our users.\nWe will provide an update by Friday, 2021-09-03 07:30 US/Pacific with current details.\nDiagnosis: Customer may high latency and packet loss.\nWorkaround: Other Zone and region can be utilized for workaround.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-03T13:39:36+00:00","modified":"2021-09-03T13:39:39+00:00","when":"2021-09-03T13:39:36+00:00","text":"Summary: Cloud networking issues being investigated impacting some user in eu-west2c\nDescription: We are experiencing an issue with Cloud Networking , beginning at Friday, 2021-09-03 0535 US/Pacific\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-09-03 06:59 US/Pacific with current details.\nDiagnosis: Customer may high latency and packet loss\nWorkaround: Other Zone and region are unaffected","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-03T19:27:00+00:00","modified":"2021-09-03T19:27:00+00:00","when":"2021-09-03T19:27:00+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 03 September 2021 05:40\n**Incident End:** 03 September 2021 06:20\n**Duration:** 40 minutes\n**Affected Services and Features:**\n- Cloud Networking\n- Compute Engine\n- Persistent Disk\n- Kubernetes Engine\n- App Engine\n- Cloud Run\n- Cloud Bigtable\n- Cloud Storage\n- Cloud Filestore\n- Cloud MemoryStore\n- Cloud Monitoring\n- Cloud VPN\n**Regions/Zones:** europe-west2-c\n**Description:**\nGoogle Cloud Networking experienced elevated packet loss and latency in europe-west2-c for a duration of 40 minutes. Additional Cloud services that use Cloud Networking experienced similar symptoms. From preliminary analysis, the root cause of the issue was a sudden redirection of traffic from a datacenter that was experiencing an issue.\n**Customer Impact:**\n- Affected customers may have experienced elevated packet loss and latency as their traffic was temporarily redirected.\n- Google Compute Engine instances experienced failed connectivity and packet loss on ingress, egress, and inter-zonal traffic. Autoscaling was also impacted and may have failed to make decisions.\n- Persistent Disk experienced elevated I/O latency on regional and zonal disks\n- Google Kubernetes Engine clusters experienced 30% of zonal and 5% of regional clusters reporting as unhealthy.\n- Google App Engine and Cloud Run served elevated 500 errors within europe-west2.\n- Cloud Bigtable experienced an elevated error rate and latency on operations within europe-west2.\n- Cloud SQL experienced issues connecting to instances in europe-west2. Affected instances may have lost telemetry data, and HA instances failed over if the primary was in the affected zone.\n- Google Cloud Storage experienced elevated read errors of up to 2.3% in europe-west2.\n- Cloud Filestore instances within europe-west2-c may have been unreachable.\n- Cloud MemoryStore instances within europe-west2 may have been unreachable.\n- Cloud Monitoring experienced delayed or missing metrics from resources in the zone.\n- Cloud VPN experienced a short duration where tunnels went down for a short time before being reestablished.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Google App Engine","id":"kchyUtnkMHJWaAva8aYc"},{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"},{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"},{"title":"Cloud Memorystore","id":"LGPLu3M5pcUAKU1z6eP3"},{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/WzDzgmTjQfAKPJha6Rbb","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"9dSqbyLnq9ALkVnD84eF","number":"1544920841042535714","begin":"2021-09-02T09:00:00+00:00","created":"2021-09-02T15:25:22+00:00","end":"2021-09-02T19:00:00+00:00","modified":"2021-09-03T16:57:50+00:00","external_desc":"Google Cloud Storage UI is experiencing some issues","updates":[{"created":"2021-09-03T16:57:38+00:00","modified":"2021-09-03T16:57:38+00:00","when":"2021-09-03T16:57:38+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 02 Sep 2021 02:00\n**Incident End:** 02 Sep 2021 12:00\n**Duration:** 10 hours\n**Affected Services and Features:**\n- Google Cloud Storage - Cloud Console UI\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Storage (GCS) experienced up to an 85% failure rate globally when creating buckets from the GCS Console UI or other service UI’s which create buckets programmatically. From preliminary analysis, the increased failure rate was due to a GCS API rollout.\n**Customer Impact:**\n- Customers experienced errors including “RPO is not supported” when creating new GCS buckets using the Cloud Console UI.\n**Additional details:**\n- Customer workarounds included using the gcloud CLI (gsutil), or the API (such as via client libraries).\n- Retrying failed requests may have also succeeded\n- The GCS API rollout has been rolled back which fully restored the service.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-02T15:37:32+00:00","modified":"2021-09-02T15:37:38+00:00","when":"2021-09-02T15:37:32+00:00","text":"The issue with Google Cloud Storage is believed to be affecting only the user interface and has a workaround to use gsutil on CLI. Our Engineering Team is currently working on the resolution.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-02T15:25:19+00:00","modified":"2021-09-02T15:25:23+00:00","when":"2021-09-02T15:25:19+00:00","text":"Summary: Google Cloud Storage UI is experiencing some issues\nDescription: We are experiencing an issue with Google Cloud Storage beginning at Thursday, 2021-09-02 07:45:57 PDT US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2021-09-02 08:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Users attempting to create gcs buckets in pantheon are getting errors\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-03T16:57:38+00:00","modified":"2021-09-03T16:57:38+00:00","when":"2021-09-03T16:57:38+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 02 Sep 2021 02:00\n**Incident End:** 02 Sep 2021 12:00\n**Duration:** 10 hours\n**Affected Services and Features:**\n- Google Cloud Storage - Cloud Console UI\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Storage (GCS) experienced up to an 85% failure rate globally when creating buckets from the GCS Console UI or other service UI’s which create buckets programmatically. From preliminary analysis, the increased failure rate was due to a GCS API rollout.\n**Customer Impact:**\n- Customers experienced errors including “RPO is not supported” when creating new GCS buckets using the Cloud Console UI.\n**Additional details:**\n- Customer workarounds included using the gcloud CLI (gsutil), or the API (such as via client libraries).\n- Retrying failed requests may have also succeeded\n- The GCS API rollout has been rolled back which fully restored the service.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"},{"title":"Google Cloud Console","id":"Wdsr1n5vyDvCt78qEifm"}],"uri":"incidents/9dSqbyLnq9ALkVnD84eF","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"gwKjX9Lukav15SaFPbBF","number":"1393196405563089999","begin":"2021-09-01T02:35:00+00:00","created":"2021-09-02T16:22:29+00:00","end":"2021-09-03T03:55:00+00:00","modified":"2021-09-07T21:39:46+00:00","external_desc":"us-central1, europe-west1, us-west1, asia-east1: Issue with Local SSDs on Google Compute Engine.","updates":[{"created":"2021-09-07T21:39:46+00:00","modified":"2021-09-07T21:39:46+00:00","when":"2021-09-07T21:39:46+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 31 August 2021 19:35\n**Incident End:** 2 September 2021 20:55\n**Duration:** 2 days, 1 hours, 20 minutes\n**Affected Services and Features:**\n- Google Compute Engine - Local SSD\n**Regions/Zones:**\n- us-central1-{a|b|f}\n- europe-west1-b\n- us-west1-a\n- asia-east1-b\n**Description:**\nGoogle Compute Engine (GCE) experienced local solid-state drive (SSD) unavailability for new instances in select zones intermittently for a duration of 2 days, 1 hours, and 20 minutes. From preliminary analysis, the root cause is due to latent issues within the GCE capacity management system.\n**Customer Impact:**\n- Unable to add Local SSDs to existing/new instances.\n**Additional details:**\n- The asia-southeast1-b zone was not impacted despite previous communication that suggested it was.\n- Existing instances, or instances that rebooted, were not impacted.\n- Low priority maintenance events on host machines were paused to prevent further impact to customer instances.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-03T16:04:17+00:00","modified":"2021-09-03T16:04:17+00:00","when":"2021-09-03T16:04:17+00:00","text":"The issue with VM maintenance on Google Compute Engine has been resolved for all affected users as of Friday, 2021-09-03 08:31 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-03T08:43:44+00:00","modified":"2021-09-03T08:43:44+00:00","when":"2021-09-03T08:43:44+00:00","text":"Summary: us-central1, europe-west1, us-west1, asia-east1, asia-southeast1: Issue with VM maintenance on Google Compute Engine.\nDescription: Mitigation work is still underway by our engineering team. The engineering team has confirmed that pausing the VM maintenance has mitigated current customer impact.\nZones recovered: us-west1, asia-east1, and asia-southeast1\nZones recovering: us-central1, europe-west1\nSuspected Root cause: An internal maintenance operation incorrectly removed too many machines from the service.\nWe will provide more information by Friday, 2021-09-03 13:30 US/Pacific.\nDiagnosis: Customers may see on VM termination/maintenance, replacement VMs cannot acquire Local SSD in order to initialize and therefore VMs will not be able to restart.\nWorkaround: No workaround available at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-02T20:34:06+00:00","modified":"2021-09-02T20:34:06+00:00","when":"2021-09-02T20:34:06+00:00","text":"Summary: us-central1, europe-west1, us-west1, asia-east1, asia-southeast1: Issue with VM maintenance on Google Compute Engine.\nDescription: Mitigation work is currently under progress. Engineering has confirmed that pausing the VM maintenance has mitigated current customer impact.\nCustomers may observe loss of local SSD on VM termination till the full resolution is deployed.\nWe will provide more information by Friday, 2021-09-03 13:30 US/Pacific.\nDiagnosis: Customers may see on VM termination/maintenance, replacement VMs cannot acquire Local SSD in order to initialize and therefore VMs will not be able to restart.\nWorkaround: No workaround available at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-02T19:03:24+00:00","modified":"2021-09-02T19:03:24+00:00","when":"2021-09-02T19:03:24+00:00","text":"Summary: us-central1, europe-west1, us-west1, asia-east1, asia-southeast1: Issue with VM maintenance on Google Compute Engine.\nDescription: Mitigation work is currently under progress. Engineering team continues to pause the regular VM maintenance operations for SSD machines to mitigate the customer impact.\nWe do not have an ETA for completion of mitigation activities at this point.\nWe will provide more information by Thursday, 2021-09-02 13:30 US/Pacific.\nDiagnosis: Customers may see on VM termination/maintenance, replacement VMs cannot acquire Local SSD in order to initialize and therefore VMs will not be able to restart.\nWorkaround: No workaround available at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-02T18:00:11+00:00","modified":"2021-09-02T18:00:11+00:00","when":"2021-09-02T18:00:11+00:00","text":"Summary: us-central1, europe-west1, us-west1, asia-east1, asia-southeast1: Issue with VM maintenance on Google Compute Engine.\nDescription: Engineering team has implemented a pause on regular VM maintenance operations for SSD machines to mitigate the customer impact.\nWe do not have an ETA for completion of mitigation activities at this point.\nWe will provide more information by Thursday, 2021-09-02 12:00 US/Pacific.\nDiagnosis: Customers may see on VM termination/maintenance, replacement VMs cannot acquire Local SSD in order to initialize and therefore VMs will not be able to restart.\nWorkaround: No workaround available at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-02T16:57:57+00:00","modified":"2021-09-02T16:57:57+00:00","when":"2021-09-02T16:57:57+00:00","text":"Summary: us-central1, europe-west1, us-west1, asia-east1, asia-southeast1: Issue with VM maintenance on Google Compute Engine.\nDescription: Engineering team has started mitigation efforts to reduce customer impact.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2021-09-02 11:00 US/Pacific.\nDiagnosis: Customers may see on VM termination/maintenance, replacement VMs cannot acquire Local SSD in order to initialize and therefore VMs will not be able to restart.\nWorkaround: No workaround available at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-02T16:22:27+00:00","modified":"2021-09-02T16:22:30+00:00","when":"2021-09-02T16:22:27+00:00","text":"Summary: us-central1, europe-west1, us-west1, asia-east1, asia-southeast1: Issue with VM maintenance on Google Compute Engine.\nDescription: Some customers are experiencing an issue performing VM maintenance activities on Google Compute Engine.\nOur engineering team is currently investigating the issue.\nWe will provide an update by Thursday, 2021-09-02 10:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may see on VM termination/maintenance, replacement VMs cannot acquire Local SSD in order to initialize and therefore VMs will not be able to restart.\nWorkaround: No workaround available at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-07T21:39:46+00:00","modified":"2021-09-07T21:39:46+00:00","when":"2021-09-07T21:39:46+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 31 August 2021 19:35\n**Incident End:** 2 September 2021 20:55\n**Duration:** 2 days, 1 hours, 20 minutes\n**Affected Services and Features:**\n- Google Compute Engine - Local SSD\n**Regions/Zones:**\n- us-central1-{a|b|f}\n- europe-west1-b\n- us-west1-a\n- asia-east1-b\n**Description:**\nGoogle Compute Engine (GCE) experienced local solid-state drive (SSD) unavailability for new instances in select zones intermittently for a duration of 2 days, 1 hours, and 20 minutes. From preliminary analysis, the root cause is due to latent issues within the GCE capacity management system.\n**Customer Impact:**\n- Unable to add Local SSDs to existing/new instances.\n**Additional details:**\n- The asia-southeast1-b zone was not impacted despite previous communication that suggested it was.\n- Existing instances, or instances that rebooted, were not impacted.\n- Low priority maintenance events on host machines were paused to prevent further impact to customer instances.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"L3ggmi3Jy4xJmgodFA9K","service_name":"Google Compute Engine","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"}],"uri":"incidents/gwKjX9Lukav15SaFPbBF","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"5yL8cbrpS3ssbYjRZQJv","number":"17020659156612080007","begin":"2021-08-31T18:32:00+00:00","created":"2021-08-31T19:58:35+00:00","end":"2021-08-31T21:09:00+00:00","modified":"2021-09-01T20:47:59+00:00","external_desc":"Global: Cloud Pubsub experiencing elevated latencies and failures.","updates":[{"created":"2021-09-01T20:44:36+00:00","modified":"2021-09-01T20:44:36+00:00","when":"2021-09-01T20:44:36+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 31 August 2021 11:32\n**Incident End:** 31 August 2021 14:09\n**Duration:** 2 hours, 37 minutes\n**Affected Services and Features:**\n- Google Cloud Pubsub - AdminOperation CreateSubscription, Backlog Stats Lookup (how many messages are in the queue to be received and acknowledged, how old is the oldest unacknowledged message)\n- Google Cloud Functions - Deployments\n- Google Cloud Dataflow - Jobs and Watermarks\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Pubsub experienced problems creating new subscriptions and viewing backlog statistics for a period of 2 hours and 37 minutes. The Pubsub issue also caused issues with Google Cloud Functions deployments and affected related Google Cloud Dataflow streaming jobs and watermarks. From preliminary analysis, the root cause of the issue was an unhealthy task in our Pubsub change notification system that created a feedback loop with increased errors on backlog stats which scaled up more tasks and put more load on our Pubsub change notification system. Google Cloud Dataflow Streaming jobs that interact with PubSub were stalled due to the dependency of those jobs on Backlog Stats which are required to calculate the watermark.\n**Customer Impact:**\n- Google Cloud Pubsub - Users experienced latencies creating subscriptions and viewing their backlog status data such as how many messages they have in the queue to be received and acknowledged and how old is the oldest unacknowledged message)\n- Google Cloud Functions - Users experienced issues with deployments.\n- Google Cloud Dataflow - Streaming jobs that interact with PubSub stalled and experienced lagging watermarks.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-31T21:17:17+00:00","modified":"2021-08-31T21:17:17+00:00","when":"2021-08-31T21:17:17+00:00","text":"The issue with Cloud Pub/Sub, Cloud Functions, and Dataflow has been resolved for all affected projects as of Tuesday, 2021-08-31 14:09 US/Pacific, 13:02 US/Pacific, and 13:16 US/Pacific respectively.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-31T21:01:39+00:00","modified":"2021-08-31T21:01:39+00:00","when":"2021-08-31T21:01:39+00:00","text":"Summary: Global: Cloud Pubsub experiencing elevated latencies and failures.\nDescription: The issue with Cloud Functions and Dataflow are now resolved and mitigation work is currently underway by our engineering team for Cloud Pub/Sub.\nThe mitigation is expected to complete by Tuesday, 2021-08-31 15:30 US/Pacific.\nCloud Pub/Sub:\n* Impact is ongoing\n* Customers may experience latencies creating subscriptions.\n* Customers are unavailable to lookup their backlog data.\nCloud Functions:\n* Impact resolved and customers should no longer experience errors with deployments\nDataflow:\n* Impact resolved and customers should no longer experience issues with jobs.\nIf you are still experiencing issues with Cloud Functions or Dataflow, please open a support case so we can work with you to resolve those issues.\nWe will provide more information by Tuesday, 2021-08-31 15:30 US/Pacific.\nDiagnosis: Customers could experience elevated latencies and failures for Cloud Pub/Sub\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-31T20:27:10+00:00","modified":"2021-08-31T20:27:10+00:00","when":"2021-08-31T20:27:10+00:00","text":"Summary: Global: Cloud Pubsub, Cloud Functions and Cloud Dataflow experiencing elevated latencies and failures.\nDescription: The issue with Cloud Functions and Dataflow are now resolved and mitigation work is currently underway by our engineering team for Cloud Pub/Sub.\nWe do not have an ETA for Cloud Pub/Sub mitigation at this point.\nCloud Pub/Sub:\n* Impact is ongoing\n* Customers may experience latencies creating subscriptions.\n* Customers are unavailable to lookup their backlog data.\nCloud Functions:\n* Impact resolved and customers should no longer experience errors with deployments\nDataflow:\n* Impact resolved and customers should no longer experience issues with jobs.\nIf you are still experiencing issues with Cloud Functions or Dataflow, please open a support case so we can work with you to resolve those issues.\nWe will provide more information by Tuesday, 2021-08-31 14:30 US/Pacific.\nDiagnosis: Customers could experience elevated latencies and failures for Cloud Pub/Sub\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-31T19:58:35+00:00","modified":"2021-08-31T19:58:35+00:00","when":"2021-08-31T19:58:35+00:00","text":"Summary: Global: Cloud Pubsub, Cloud Functions and Cloud Dataflow experiencing elevated latencies and failures.\nDescription: We are experiencing an issue with Cloud Pub/Sub, Cloud Functions and Cloud Dataflow beginning at Tuesday, 2021-08-31 11:30 US/Pacific.\nCloud Pub/Sub:\n* Customers may experience latencies creating subscriptions.\n* Customers are unavailable to lookup their backlog data.\nCloud Functions:\n* Users are experiencing errors with deployment\nDataflow:\n* Customers may experience lagging jobs/watermarks\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-08-31 13:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers could experience elevated latencies and failures for Cloud Pub/Sub, Cloud Functions and Dataflow jobs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-01T20:44:36+00:00","modified":"2021-09-01T20:44:36+00:00","when":"2021-09-01T20:44:36+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 31 August 2021 11:32\n**Incident End:** 31 August 2021 14:09\n**Duration:** 2 hours, 37 minutes\n**Affected Services and Features:**\n- Google Cloud Pubsub - AdminOperation CreateSubscription, Backlog Stats Lookup (how many messages are in the queue to be received and acknowledged, how old is the oldest unacknowledged message)\n- Google Cloud Functions - Deployments\n- Google Cloud Dataflow - Jobs and Watermarks\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Pubsub experienced problems creating new subscriptions and viewing backlog statistics for a period of 2 hours and 37 minutes. The Pubsub issue also caused issues with Google Cloud Functions deployments and affected related Google Cloud Dataflow streaming jobs and watermarks. From preliminary analysis, the root cause of the issue was an unhealthy task in our Pubsub change notification system that created a feedback loop with increased errors on backlog stats which scaled up more tasks and put more load on our Pubsub change notification system. Google Cloud Dataflow Streaming jobs that interact with PubSub were stalled due to the dependency of those jobs on Backlog Stats which are required to calculate the watermark.\n**Customer Impact:**\n- Google Cloud Pubsub - Users experienced latencies creating subscriptions and viewing their backlog status data such as how many messages they have in the queue to be received and acknowledged and how old is the oldest unacknowledged message)\n- Google Cloud Functions - Users experienced issues with deployments.\n- Google Cloud Dataflow - Streaming jobs that interact with PubSub stalled and experienced lagging watermarks.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"},{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"}],"uri":"incidents/5yL8cbrpS3ssbYjRZQJv","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"sBT6tcw8bxRArf6Jw19o","number":"2852238876231482449","begin":"2021-08-31T18:30:00+00:00","created":"2021-09-01T00:16:58+00:00","end":"2021-08-31T20:16:00+00:00","modified":"2021-09-01T00:21:25+00:00","external_desc":"Global: Dataflow Streaming jobs that interact with PubSub are experiencing failures.","updates":[{"created":"2021-09-01T00:18:30+00:00","modified":"2021-09-01T00:18:30+00:00","when":"2021-09-01T00:18:30+00:00","text":"Global: Dataflow Streaming jobs that interact with PubSub experienced failures.\nThe issue with Cloud Dataflow has been resolved for all affected users as of Tuesday, 2021-08-31 13:22 US/Pacific.\nFor more information, please follow: https://status.cloud.google.com/incidents/5yL8cbrpS3ssbYjRZQJv\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-01T00:16:59+00:00","modified":"2021-09-01T00:16:59+00:00","when":"2021-09-01T00:16:59+00:00","text":"Summary: Global: Dataflow Streaming jobs that interact with PubSub are experiencing failures.\nDescription: We are experiencing an issue with Cloud Dataflow beginning at Tuesday, 2021-08-31 11:30 US/Pacific. Our engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/5yL8cbrpS3ssbYjRZQJv where we will provide the next update by Tuesday, 2021-08-31 13:30 US/Pacific.\nDiagnosis: Customers will experience lagging jobs/watermarks and failure in subscription creation.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-01T00:18:30+00:00","modified":"2021-09-01T00:18:30+00:00","when":"2021-09-01T00:18:30+00:00","text":"Global: Dataflow Streaming jobs that interact with PubSub experienced failures.\nThe issue with Cloud Dataflow has been resolved for all affected users as of Tuesday, 2021-08-31 13:22 US/Pacific.\nFor more information, please follow: https://status.cloud.google.com/incidents/5yL8cbrpS3ssbYjRZQJv\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"T9bFoXPqG8w8g1YbWTKY","service_name":"Google Cloud Dataflow","affected_products":[{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"}],"uri":"incidents/sBT6tcw8bxRArf6Jw19o","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"XnB6ncPMYd3Tei7XcCE1","number":"4559585505212123721","begin":"2021-08-26T21:25:00+00:00","created":"2021-08-26T21:25:44+00:00","end":"2021-08-26T21:53:00+00:00","modified":"2022-03-28T20:48:42+00:00","external_desc":"Cloud Logging experiencing issue returning results for \"tail\" commands globally","updates":[{"created":"2021-08-26T21:53:29+00:00","modified":"2021-08-26T21:53:29+00:00","when":"2021-08-26T21:53:29+00:00","text":"The issue with Cloud Logging `gcloud alpha logging tail` command and any direct calls to the Tailing API is believed to be affecting a very small number of projects and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-26T21:25:42+00:00","modified":"2021-08-26T21:25:44+00:00","when":"2021-08-26T21:25:42+00:00","text":"Summary: Cloud Logging experiencing issue returning results for \"tail\" commands globally\nDescription: We are experiencing an issue with Cloud Logging.\nCustomers may not be able to look at live logs by running \"tail\" commands\nOur engineering team continues to investigate the issue.\nWe will provide an update by Thursday, 2021-08-26 14:55 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may not be able to look at live logs by running \"tail\" commands\nWorkaround: Normal log queries work fine and users can still use streaming in the UI.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-26T21:53:29+00:00","modified":"2021-08-26T21:53:29+00:00","when":"2021-08-26T21:53:29+00:00","text":"The issue with Cloud Logging `gcloud alpha logging tail` command and any direct calls to the Tailing API is believed to be affecting a very small number of projects and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/XnB6ncPMYd3Tei7XcCE1","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"A27EJTBQ6anaCdeNX6zp","number":"4326509736320173557","begin":"2021-08-26T13:49:00+00:00","created":"2021-08-26T13:49:30+00:00","end":"2021-08-27T01:06:48+00:00","modified":"2021-09-02T21:21:06+00:00","external_desc":"GKE service is unaffected. Mitigation for issue with Anthos On-Prem admin workstation access, is underway. See further details below.","updates":[{"created":"2021-08-27T01:07:06+00:00","modified":"2021-09-02T21:21:06+00:00","when":"2021-08-27T01:07:06+00:00","text":"GKE service is unaffected. Mitigation for issue with Anthos On-Prem admin workstation access, is underway.\nFix: - Release 1.8.2 with the fix delivered.\nETA for the hotfix releases: - Hotfix for 1.7.3 is expected to be released by Friday (9/03).\nFor workaround procedures, please refer to the previous notification down below.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here. We thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-26T21:15:54+00:00","modified":"2021-08-26T21:15:54+00:00","when":"2021-08-26T21:15:54+00:00","text":"Summary: Anthos On-Prem customers unable to login to admin workstation (starting 2021-08-25 for customers using Anthos OP v1.7.2)\nDescription: Mitigation work is currently underway by our engineering team.\nETA for the releases: - Release 1.8.2 with the fix is expected to be next Monday (8/29). - And hotfix for 1.7.3 is expected to be next Thursday (9/02).\nWe will provide an update on Thursday 2021-09-02 by 17:00 US/Pacific.\nDiagnosis: Customers are unable to login to their admin workstation in both their prod and non-prod environments.\nWorkaround: Mitigation steps for the admin workstation:\nUse a temporary VM to perform the following steps. If you don’t have a VM, you can create an admin workstation at 1.7.1-gke.4 as the temporary VM.\n1)Ensure the VM and the problematic 1.7.2-gke.2 admin workstation are in power off state.\n2)Attach the boot disk of the problematic admin workstation to the VM. The boot disk is the one with the label Hard disk 1.\n3)Mount the boot disk inside the VM. sudo mkdir -p /mnt/boot-disk sudo mount /dev/sdc1 /mnt/boot-disk, assuming the boot disk is identified as dev/sdc1.\n4)Set the ubuntu user expiration date to unlimited. sudo chroot /mnt/boot-disk chage -M 99999 ubuntu\n5)Shutdown the temporary VM.\n6)Power on the admin workstation. You should be able to SSH to the admin workstation as usual.\n7)[Clean up] Delete the temporary VM.\nNote that this fix will break the CIS benchmark rule[1] 5.4.1.1 Ensure password expiration is 365 days or less.\n[1]https://cloud.google.com/anthos/clusters/docs/on-prem/1.7/concepts/cis-ubuntu-benchmark\nMitigation steps for non admin workstation nodes (including cluster nodes and Seesaw VM):\nLogin onto each node (or use SSH) to run the following command on the node sudo chroot /mnt/boot-disk chage -M 99999 ubuntu\nNote that if the non admin workstation nodes are not SSHable due to the error, you will have to use the mitigation step for admin workstation.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-26T14:12:47+00:00","modified":"2021-08-26T14:12:54+00:00","when":"2021-08-26T14:12:47+00:00","text":"Summary: Anthos On-Prem customers unable to login to admin workstation (starting 2021-08-25 for customers using Anthos OP v1.7.2)\nDescription: Mitigation work is currently underway by our engineering team.\nETA for the releases: - Release 1.8.2 with the fix is aiming next Monday (8/29). - And hotfix for 1.7.3 is also aiming next week.\nWe will provide more information by Thursday, 2021-08-26 17:00 US/Pacific.\nDiagnosis: Customers are unable to login to their admin workstation in both their prod and non-prod environments.\nWorkaround: Mitigation steps for the admin workstation:\nUse a temporary VM to perform the following steps. If you don’t have a VM, you can create an admin workstation at 1.7.1-gke.4 as the temporary VM.\n1)Ensure the VM and the problematic 1.7.2-gke.2 admin workstation are in power off state.\n2)Attach the boot disk of the problematic admin workstation to the VM. The boot disk is the one with the label Hard disk 1.\n3)Mount the boot disk inside the VM. sudo mkdir -p /mnt/boot-disk sudo mount /dev/sdc1 /mnt/boot-disk, assuming the boot disk is identified as dev/sdc1.\n4)Set the ubuntu user expiration date to unlimited. sudo chroot /mnt/boot-disk chage -M 99999 ubuntu\n5)Shutdown the temporary VM.\n6)Power on the admin workstation. You should be able to SSH to the admin workstation as usual.\n7)[Clean up] Delete the temporary VM.\nNote that this fix will break the CIS benchmark rule[1] 5.4.1.1 Ensure password expiration is 365 days or less.\n[1]https://cloud.google.com/anthos/clusters/docs/on-prem/1.7/concepts/cis-ubuntu-benchmark\nMitigation steps for non admin workstation nodes (including cluster nodes and Seesaw VM):\nLogin onto each node (or use SSH) to run the following command on the node sudo chroot /mnt/boot-disk chage -M 99999 ubuntu\nNote that if the non admin workstation nodes are not SSHable due to the error, you will have to use the mitigation step for admin workstation.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-26T13:49:22+00:00","modified":"2021-08-26T13:51:17+00:00","when":"2021-08-26T13:49:22+00:00","text":"Summary: Anthos On-Prem customers unable to login to admin workstation (starting 2021-08-25 for customers using Anthos OP v1.7.2)\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Thursday, 2021-08-26 07:25 US/Pacific.\nDiagnosis: Customers are unable to login to their admin workstation in both their prod and non-prod environments.\nWorkaround: Mitigation steps for the admin workstation:\nUse a temporary VM to perform the following steps. If you don’t have a VM, you can create an admin workstation at 1.7.1-gke.4 as the temporary VM.\n1)Ensure the VM and the problematic 1.7.2-gke.2 admin workstation are in power off state.\n2)Attach the boot disk of the problematic admin workstation to the VM. The boot disk is the one with the label Hard disk 1.\n3)Mount the boot disk inside the VM. sudo mkdir -p /mnt/boot-disk sudo mount /dev/sdc1 /mnt/boot-disk, assuming the boot disk is identified as dev/sdc1.\n4)Set the ubuntu user expiration date to unlimited. sudo chroot /mnt/boot-disk chage -M 99999 ubuntu\n5)Shutdown the temporary VM.\n6)Power on the admin workstation. You should be able to SSH to the admin workstation as usual.\n7)[Clean up] Delete the temporary VM.\nNote that this fix will break the CIS benchmark rule[1] 5.4.1.1 Ensure password expiration is 365 days or less.\n[1]https://cloud.google.com/anthos/clusters/docs/on-prem/1.7/concepts/cis-ubuntu-benchmark\nMitigation steps for non admin workstation nodes (including cluster nodes and Seesaw VM):\nLogin onto each node (or use SSH) to run the following command on the node sudo chroot /mnt/boot-disk chage -M 99999 ubuntu\nNote that if the non admin workstation nodes are not SSHable due to the error, you will have to use the mitigation step for admin workstation.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-27T01:07:06+00:00","modified":"2021-09-02T21:21:06+00:00","when":"2021-08-27T01:07:06+00:00","text":"GKE service is unaffected. Mitigation for issue with Anthos On-Prem admin workstation access, is underway.\nFix: - Release 1.8.2 with the fix delivered.\nETA for the hotfix releases: - Hotfix for 1.7.3 is expected to be released by Friday (9/03).\nFor workaround procedures, please refer to the previous notification down below.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here. We thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LCSbT57h59oR4W98NHuz","service_name":"Google Kubernetes Engine","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/A27EJTBQ6anaCdeNX6zp","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"vcRkm91DVuWNGWMw9w84","number":"1183468118352922339","begin":"2021-08-25T08:37:00+00:00","created":"2021-08-25T10:39:11+00:00","end":"2021-08-25T11:14:00+00:00","modified":"2021-08-25T18:20:08+00:00","external_desc":"Google Cloud Dataflow elevated errors starting new or querying existing dataflow jobs in us-west1, asia-east1, asia-northeast1, and europe-west1.","updates":[{"created":"2021-08-25T18:18:50+00:00","modified":"2021-08-25T18:18:50+00:00","when":"2021-08-25T18:18:50+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 25 August 2021 01:37\n**Incident End:** 25 August 2021 04:14\n**Duration:** 2 hours, 37 minutes\n**Affected Services and Features:**\n- Google Cloud Dataflow\n- Dataproc Metastore\n**Regions/Zones:** us-west1, asia-east1, asia-northeast1, europe-west1\n**Description:**\nGoogle Cloud Dataflow experienced elevated errors starting new or querying existing dataflow jobs in us-west1, asia-east1, asia-northeast1, and europe-west1 for a duration of 2 hours and 37 minutes. From preliminary analysis, the root cause of the issue was a misconfiguration triggered by a rollout.\n**Customer Impact:**\n- 500 errors when launching new dataflow jobs.\n- 500 errors querying existing dataflow jobs.\n- The majority of the impact for customers was in us-west1, with 33% of job creation and query traffic reporting errors.\n- Dataproc Metastore uses underlying Dataflow jobs for some features, and thus experienced elevated errors of up to 100% on the following API’s in us-west1 from 01:27 to 03:53; Restore, Import (from SQL, Avro), and Export (to Avro).\n- There was a slight re-occurance in europe-west1 between 06:06 and 08:02 with a peak error rate of 3.5%.\n- Existing jobs continued to progress without issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-25T11:43:02+00:00","modified":"2021-08-25T15:18:15+00:00","when":"2021-08-25T11:43:02+00:00","text":"The issue with Google Cloud Dataflow has been resolved for all affected projects as of Wednesday, 2021-08-25 04:42 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-25T11:30:24+00:00","modified":"2021-08-25T15:22:33+00:00","when":"2021-08-25T11:30:24+00:00","text":"Summary: Dataflow job querying and creation impacted in us-west1\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Wednesday, 2021-08-25 06:00 US/Pacific.\nWe will provide more information by Wednesday, 2021-08-25 05:30 US/Pacific.\nDiagnosis: We believe starting a Dataflow job or querying existing Dataflow jobs may fail for the customers. However, existing Dataflow jobs should continue to progress as usual.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-25T10:39:11+00:00","modified":"2021-08-25T15:22:26+00:00","when":"2021-08-25T10:39:11+00:00","text":"Summary: Dataflow job querying and creation impacted in us-west1\nDescription: We are experiencing an issue with Google Cloud Dataflow in us-west1 beginning at Wednesday, 2021-08-25 01:37 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-08-25 04:30 US/Pacific with current details. We apologize to all who are affected by the disruption.\nDiagnosis: We believe starting a Dataflow job or querying existing Dataflow jobs may fail for the customers. However, existing Dataflow jobs should continue to progress as usual.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-25T18:18:50+00:00","modified":"2021-08-25T18:18:50+00:00","when":"2021-08-25T18:18:50+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 25 August 2021 01:37\n**Incident End:** 25 August 2021 04:14\n**Duration:** 2 hours, 37 minutes\n**Affected Services and Features:**\n- Google Cloud Dataflow\n- Dataproc Metastore\n**Regions/Zones:** us-west1, asia-east1, asia-northeast1, europe-west1\n**Description:**\nGoogle Cloud Dataflow experienced elevated errors starting new or querying existing dataflow jobs in us-west1, asia-east1, asia-northeast1, and europe-west1 for a duration of 2 hours and 37 minutes. From preliminary analysis, the root cause of the issue was a misconfiguration triggered by a rollout.\n**Customer Impact:**\n- 500 errors when launching new dataflow jobs.\n- 500 errors querying existing dataflow jobs.\n- The majority of the impact for customers was in us-west1, with 33% of job creation and query traffic reporting errors.\n- Dataproc Metastore uses underlying Dataflow jobs for some features, and thus experienced elevated errors of up to 100% on the following API’s in us-west1 from 01:27 to 03:53; Restore, Import (from SQL, Avro), and Export (to Avro).\n- There was a slight re-occurance in europe-west1 between 06:06 and 08:02 with a peak error rate of 3.5%.\n- Existing jobs continued to progress without issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"},{"title":"Dataproc Metastore","id":"PXZh68NPz9auRyo4tVfy"}],"uri":"incidents/vcRkm91DVuWNGWMw9w84","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"TSaW8RCnLss2ob5fM2t5","number":"16348501793535204061","begin":"2021-08-24T04:12:02+00:00","created":"2021-08-24T05:20:16+00:00","end":"2021-08-24T05:31:46+00:00","modified":"2021-08-24T05:31:46+00:00","external_desc":"We are experiencing a connectivity issue affecting Google Cloud Storage in australia-southeast2","updates":[{"created":"2021-08-24T05:31:45+00:00","modified":"2021-08-24T05:31:46+00:00","when":"2021-08-24T05:31:45+00:00","text":"The issue with Google Cloud Storage has been resolved for all affected users as of Monday, 2021-08-23 22:27 US/Pacific. However most users should have seen the impact mitigated as of approximately 21:20 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T05:20:16+00:00","modified":"2021-08-24T05:20:16+00:00","when":"2021-08-24T05:20:16+00:00","text":"Summary: We are experiencing a connectivity issue affecting Google Cloud Storage in australia-southeast2\nDescription: We are experiencing an issue impacting Google Cloud Storage in australia-southeast2 begining at 2021-08-23 19:52 US/Pacific. We believe this issue is linked to https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G\nOur engineering team continues to investigate the issue.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T05:31:45+00:00","modified":"2021-08-24T05:31:46+00:00","when":"2021-08-24T05:31:45+00:00","text":"The issue with Google Cloud Storage has been resolved for all affected users as of Monday, 2021-08-23 22:27 US/Pacific. However most users should have seen the impact mitigated as of approximately 21:20 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"UwaYoXQ5bHYHG6EdiPB8","service_name":"Google Cloud Storage","affected_products":[{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"}],"uri":"incidents/TSaW8RCnLss2ob5fM2t5","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"ZzoChnVNAqbWBKC4wcAP","number":"14147611926594694208","begin":"2021-08-24T03:24:37+00:00","created":"2021-08-24T04:04:02+00:00","end":"2021-08-24T04:40:28+00:00","modified":"2021-08-24T04:40:28+00:00","external_desc":"We are experiencing a connectivity issue affecting Cloud Bigtable in australia-southeast2","updates":[{"created":"2021-08-24T04:40:28+00:00","modified":"2021-08-24T04:40:28+00:00","when":"2021-08-24T04:40:28+00:00","text":"The issue with Cloud Bigtable has been resolved for all affected users as of Monday, 2021-08-23 21:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:04:02+00:00","modified":"2021-08-24T04:04:02+00:00","when":"2021-08-24T04:04:02+00:00","text":"Summary: We are experiencing a connectivity issue affecting Cloud Bigtable in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T04:40:28+00:00","modified":"2021-08-24T04:40:28+00:00","when":"2021-08-24T04:40:28+00:00","text":"The issue with Cloud Bigtable has been resolved for all affected users as of Monday, 2021-08-23 21:36 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LfZSuE3xdQU46YMFV5fy","service_name":"Google Cloud Bigtable","affected_products":[{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"}],"uri":"incidents/ZzoChnVNAqbWBKC4wcAP","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"FWZV7AXUH99PsLc9Eywr","number":"8604705649045314008","begin":"2021-08-24T03:21:31+00:00","created":"2021-08-24T04:02:17+00:00","end":"2021-08-24T04:48:46+00:00","modified":"2021-08-24T04:48:46+00:00","external_desc":"We are experiencing a connectivity issue affecting Cloud Memorystore in australia-southeast2","updates":[{"created":"2021-08-24T04:48:45+00:00","modified":"2021-08-24T04:48:45+00:00","when":"2021-08-24T04:48:45+00:00","text":"The issue with Cloud Memorystore has been resolved for all affected users as of approximately Monday, 2021-08-23 20:03 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:02:12+00:00","modified":"2021-08-24T04:02:18+00:00","when":"2021-08-24T04:02:12+00:00","text":"Summary: We are experiencing a connectivity issue affecting Cloud Memorystore in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T04:48:45+00:00","modified":"2021-08-24T04:48:45+00:00","when":"2021-08-24T04:48:45+00:00","text":"The issue with Cloud Memorystore has been resolved for all affected users as of approximately Monday, 2021-08-23 20:03 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LGPLu3M5pcUAKU1z6eP3","service_name":"Cloud Memorystore","affected_products":[{"title":"Cloud Memorystore","id":"LGPLu3M5pcUAKU1z6eP3"}],"uri":"incidents/FWZV7AXUH99PsLc9Eywr","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"LccZqKpFkxnBijMvQFU3","number":"7840853179390995303","begin":"2021-08-24T03:20:24+00:00","created":"2021-08-24T04:08:22+00:00","end":"2021-08-24T05:41:36+00:00","modified":"2021-08-24T05:41:36+00:00","external_desc":"We are experiencing a connectivity issue affecting Cloud Pub/Sub in australia-southeast2","updates":[{"created":"2021-08-24T05:41:36+00:00","modified":"2021-08-24T05:41:36+00:00","when":"2021-08-24T05:41:36+00:00","text":"The issue with Cloud Pub/Sub has been resolved for all affected projects as of Monday, 2021-08-23 20:38 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:08:22+00:00","modified":"2021-08-24T04:08:22+00:00","when":"2021-08-24T04:08:22+00:00","text":"Summary: We are experiencing a connectivity issue affecting Cloud Pub/Sub in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T05:41:36+00:00","modified":"2021-08-24T05:41:36+00:00","when":"2021-08-24T05:41:36+00:00","text":"The issue with Cloud Pub/Sub has been resolved for all affected projects as of Monday, 2021-08-23 20:38 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"dFjdLh2v6zuES6t9ADCB","service_name":"Google Cloud Pub/Sub","affected_products":[{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"}],"uri":"incidents/LccZqKpFkxnBijMvQFU3","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"pYnCSa67rJ3Q5FrXxPHg","number":"7105084436743396399","begin":"2021-08-24T03:19:54+00:00","created":"2021-08-24T04:18:23+00:00","end":"2021-08-24T04:37:49+00:00","modified":"2021-08-24T04:37:49+00:00","external_desc":"We are experiencing a connectivity issue affecting Cloud access policy in australia-southeast2","updates":[{"created":"2021-08-24T04:37:49+00:00","modified":"2021-08-24T04:37:49+00:00","when":"2021-08-24T04:37:49+00:00","text":"The issue with Cloud Identity \u0026 Security has been resolved for all affected users as of Monday, 2021-08-23 20:10 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:18:23+00:00","modified":"2021-08-24T04:18:24+00:00","when":"2021-08-24T04:18:23+00:00","text":"Summary: We are experiencing a connectivity issue affecting Cloud access policy in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T04:37:49+00:00","modified":"2021-08-24T04:37:49+00:00","when":"2021-08-24T04:37:49+00:00","text":"The issue with Cloud Identity \u0026 Security has been resolved for all affected users as of Monday, 2021-08-23 20:10 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"adnGEDEt9zWzs8uF1oKA","service_name":"Identity and Access Management","affected_products":[{"title":"Identity and Access Management","id":"adnGEDEt9zWzs8uF1oKA"}],"uri":"incidents/pYnCSa67rJ3Q5FrXxPHg","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"1LczkFcPKVVPUEw6XBf7","number":"4310336585790370745","begin":"2021-08-24T03:18:00+00:00","created":"2021-08-24T04:18:41+00:00","end":"2021-08-24T04:49:00+00:00","modified":"2022-03-28T19:41:11+00:00","external_desc":"We are experiencing a connectivity issue affecting Persistent Disk in australia-southeast2","updates":[{"created":"2021-08-24T04:49:38+00:00","modified":"2021-08-24T04:49:38+00:00","when":"2021-08-24T04:49:38+00:00","text":"The issue with Google Compute Engine Persistent Disk has been resolved for all affected users as of Monday, 2021-08-23 20:13 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:18:41+00:00","modified":"2021-08-24T04:18:42+00:00","when":"2021-08-24T04:18:41+00:00","text":"Summary: We are experiencing a connectivity issue affecting Persistent Disk in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T04:49:38+00:00","modified":"2021-08-24T04:49:38+00:00","when":"2021-08-24T04:49:38+00:00","text":"The issue with Google Compute Engine Persistent Disk has been resolved for all affected users as of Monday, 2021-08-23 20:13 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"}],"uri":"incidents/1LczkFcPKVVPUEw6XBf7","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"8aPfN89nkeWJVui93StH","number":"10408151242071435888","begin":"2021-08-24T03:15:41+00:00","created":"2021-08-24T04:06:17+00:00","end":"2021-08-24T04:28:15+00:00","modified":"2021-08-24T04:28:15+00:00","external_desc":"We are experiencing a connectivity issue affecting Google Kubernetes Engine in australia-southeast2","updates":[{"created":"2021-08-24T04:28:15+00:00","modified":"2021-08-24T04:28:15+00:00","when":"2021-08-24T04:28:15+00:00","text":"The issue with Google Kubernetes Engine has been resolved for all affected users as of Monday, 2021-08-23 20:41 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:06:16+00:00","modified":"2021-08-24T04:06:17+00:00","when":"2021-08-24T04:06:16+00:00","text":"Summary: We are experiencing a connectivity issue affecting Google Kubernetes Engine in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T04:28:15+00:00","modified":"2021-08-24T04:28:15+00:00","when":"2021-08-24T04:28:15+00:00","text":"The issue with Google Kubernetes Engine has been resolved for all affected users as of Monday, 2021-08-23 20:41 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LCSbT57h59oR4W98NHuz","service_name":"Google Kubernetes Engine","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/8aPfN89nkeWJVui93StH","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"RPBQJ6t6qkJGkKG21Quv","number":"5727678322536895542","begin":"2021-08-24T03:14:16+00:00","created":"2021-08-24T04:09:37+00:00","end":"2021-08-24T05:43:01+00:00","modified":"2021-08-24T05:43:01+00:00","external_desc":"We are experiencing a connectivity issue affecting Cloud Dataproc in australia-southeast2","updates":[{"created":"2021-08-24T05:43:01+00:00","modified":"2021-08-24T05:43:01+00:00","when":"2021-08-24T05:43:01+00:00","text":"The issue with Cloud Dataproc has been resolved for all affected projects as of Monday, 2021-08-23 21:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:09:37+00:00","modified":"2021-08-24T04:09:37+00:00","when":"2021-08-24T04:09:37+00:00","text":"Summary: We are experiencing a connectivity issue affecting Cloud Dataproc in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T05:43:01+00:00","modified":"2021-08-24T05:43:01+00:00","when":"2021-08-24T05:43:01+00:00","text":"The issue with Cloud Dataproc has been resolved for all affected projects as of Monday, 2021-08-23 21:50 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"yjXrEg3Yvy26BauMwr69","service_name":"Google Cloud Dataproc","affected_products":[{"title":"Google Cloud Dataproc","id":"yjXrEg3Yvy26BauMwr69"}],"uri":"incidents/RPBQJ6t6qkJGkKG21Quv","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"NJJp8jkNzFStMyiJZWWD","number":"16011761144916808084","begin":"2021-08-24T03:14:00+00:00","created":"2021-08-24T04:21:20+00:00","end":"2021-08-24T03:41:13+00:00","modified":"2021-08-24T05:48:13+00:00","external_desc":"The issue with Cloud Networking has been resolved for all affected users as of Monday, 2021-08-23 20:41:13 PDT.\nWe thank you for your patience while we worked on resolving the issue.","updates":[{"created":"2021-08-24T04:21:20+00:00","modified":"2021-08-24T04:21:20+00:00","when":"2021-08-24T04:21:20+00:00","text":"Description: We are experiencing an issue with Cloud Networking at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T04:21:20+00:00","modified":"2021-08-24T04:21:20+00:00","when":"2021-08-24T04:21:20+00:00","text":"Description: We are experiencing an issue with Cloud Networking at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"VNJxzcH58QmTt5H6pnT6","service_name":"Google Cloud Networking","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"}],"uri":"incidents/NJJp8jkNzFStMyiJZWWD","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"wNRZfELPApTbYeBrVTRy","number":"17955589576369634627","begin":"2021-08-24T03:14:00+00:00","created":"2021-08-24T04:11:11+00:00","end":"2021-08-24T05:35:00+00:00","modified":"2022-03-28T20:34:24+00:00","external_desc":"We are experiencing a connectivity issue affecting Cloud SQL in australia-southeast2","updates":[{"created":"2021-08-24T05:35:31+00:00","modified":"2021-08-24T05:35:31+00:00","when":"2021-08-24T05:35:31+00:00","text":"The issue with Cloud SQL has been resolved for all affected projects as of Monday, 2021-08-23 21:52 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:11:11+00:00","modified":"2021-08-24T04:11:11+00:00","when":"2021-08-24T04:11:11+00:00","text":"Summary: We are experiencing a connectivity issue affecting Cloud SQL in australia-southeast2\nDescription: We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T05:35:31+00:00","modified":"2021-08-24T05:35:31+00:00","when":"2021-08-24T05:35:31+00:00","text":"The issue with Cloud SQL has been resolved for all affected projects as of Monday, 2021-08-23 21:52 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"}],"uri":"incidents/wNRZfELPApTbYeBrVTRy","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"9FtLfPznLEsy2pwmzfYp","number":"695108566094988677","begin":"2021-08-24T03:14:00+00:00","created":"2021-08-24T04:23:18+00:00","end":"2021-08-24T04:21:00+00:00","modified":"2021-08-24T05:48:33+00:00","external_desc":"Connectivity issues affecting BigQuery in australia-southeast2.","updates":[{"created":"2021-08-24T05:14:27+00:00","modified":"2021-08-24T05:14:28+00:00","when":"2021-08-24T05:14:27+00:00","text":"The issue with BigQuery has been resolved for all affected users as of Monday, 2021-08-23 21:21 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:23:17+00:00","modified":"2021-08-24T04:23:18+00:00","when":"2021-08-24T04:23:17+00:00","text":"Description: We are experiencing an issue with BigQuery at australia-southeast2\nOur engineering team continues to investigate the issue.\nFor regular status updates, please follow: https://status.cloud.google.com/incidents/8DhiwfKvD987f5tJrj1G, no further updates will be provided here.\nWe apologise to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T05:14:27+00:00","modified":"2021-08-24T05:14:28+00:00","when":"2021-08-24T05:14:27+00:00","text":"The issue with BigQuery has been resolved for all affected users as of Monday, 2021-08-23 21:21 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"9CcrhHUcFevXPSVaSxkf","service_name":"Google BigQuery","affected_products":[{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"}],"uri":"incidents/9FtLfPznLEsy2pwmzfYp","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"8DhiwfKvD987f5tJrj1G","number":"13912371513886524933","begin":"2021-08-24T02:50:00+00:00","created":"2021-08-24T03:35:48+00:00","end":"2021-08-24T04:20:00+00:00","modified":"2021-08-24T23:09:25+00:00","external_desc":"Google Cloud infrastructure components faced issues in australia-southeast2 starting 2021-08-23 19:50 PDT. Most of the services are fully restored by 2021-08-23 21:20 PDT.","updates":[{"created":"2021-08-24T22:59:30+00:00","modified":"2021-08-24T22:59:30+00:00","when":"2021-08-24T22:59:30+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 23 August 2021 19:50\n**Incident End:** 23 August 2021 21:20\n**Duration:** 1 hours, 30 minutes\n**Affected Services and Features:**\n- Cloud Networking\n- Cloud L7 and L4 Load balancers\n- Cloud Interconnect\n- Cloud NAT\n- Cloud VPN\n- Google Compute Engine\n- Google Kubernetes Engine\n- Cloud Persistent Disk\n- Cloud Cloud Storage\n- Cloud FileStore\n- Cloud Dataproc\n- Cloud Run\n- Cloud SQL\n- Cloud Spanner\n- Cloud Pub/Sub\n- Cloud Dataflow\n- Cloud Dataproc\n- Cloud Firestore\n- Cloud Bigtable\n- Cloud Logging\n- Cloud Monitoring\n- Cloud BigQuery\n- Cloud IAM\n**Regions/Zones:** australia-southeast2\n**Description:**\nGoogle Cloud Networking experienced intermittent connectivity issues with Google Cloud Services in australia-southeast2 for 1 hour and 30 minutes. The underlying Cloud Networking impact ended at 20:41, however, some Cloud Services took longer to recover - delaying the all clear until 21:20. Any service that uses Cloud Networking may have seen impact. We have included available details of service specific impact below; however, this may not be a comprehensive accounting of all downstream networking impact. From preliminary analysis, the root cause of the issue was transient voltage at the feeder to the network equipment, causing the equipment to reboot. In order to mitigate the issue, traffic within the australia-southeast2 region was redirected temporarily.\n**Customer Impact:**\n- Cloud Networking: Public IP traffic connectivity failed from 19:51 to 20:41.\n- Cloud L7 Load Balancers: Partial dataplane query loss and control plane operational delay for External load balancers from 19:50 - 20:12, and Internal load balancers from 19:50 - 20:18.\n- Cloud L4 Load Balancing Inbound public IP traffic was dropped from 19:51 to 20:41.\n- Cloud Interconnect: Up to 100% packet loss between 19:50 and 20:21.\n- Cloud NAT experienced control plane failures from 19:51 to 20:00.\n- Cloud VPN HA dropped up to 83% of traffic between 19:51 and 20:21, while Legacy VPN dropped ~100% of traffic between 19:51 and 20:41.\n- Cloud SQL: 100% error rate from 19:49 to 20:01.\n- Cloud Storage: 100% error rate through 21:20.\n- Cloud Functions:\n- Cloud Run: 100% error rate through 21:19.\n- Cloud Bigtable: 100% error rate from 19:49 to 20:01 and increased latency from 20:01 to 20:13.\n- Cloud Logging: Logs written within the region may have failed to be ingested from 20:07 to 21:18.\n- Cloud Monitoring: Customers may have experienced falsely firing alerts, missed alerts, missing metrics and failed writes from 19:50 to 20:15.\n- Google Compute Engine: Operations to create or modify instances failed from 19:52 to 20:12. Connectivity from instances to other GCP services may be affected until 20:26. Existing instances may have lost network connectivity. Additionally autoscaling had delays or errors collecting input data which may have impacted autoscaling decisions.\n- Google Kubernetes Engine: Control plane operations on regional clusters failed between 19:50 and 20:04. Increased latency from 20:05 to 20:41. 100% of requests to container.googleapis.com failed\n- Persistent Disk: Up to 100% device unavailability between 19:51 and 20:13.\n- Cloud Filestore: Up to 100% error rate from 19:50 to 20:03.\n- Cloud IAM: ~80% error rate from 19:52 to 20:10.\n- Cloud Spanner: 100% error rate between 19:53 and 20:09.\n- Cloud Pub/Sub: Increased error rate and latency of up to 95% between 19:50 and 20:12.\n- Cloud Dataflow: Increased errors starting jobs and making progress on existing jobs between 19:50 and 20:12.\n- Cloud Dataproc: New cluster creation failed from 20:09 until 21:20\n- Cloud Firestore: Control Plane saw ~90% error rates from 19:50 to 20:03. Data plane so no significant impact.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T05:27:39+00:00","modified":"2021-08-24T05:34:21+00:00","when":"2021-08-24T05:27:39+00:00","text":"Google Cloud infrastructure components faced issues in australia-southeast2. Most of the services are now fully restored. The impact is believed to have started at 2021-08-23 19:50 PST.\nProducts impacted and current status:\n- Cloud FileStore - Service restored. 21:07 PST\n- Cloud Networking - Service restored. 20:41 PST\n- Cloud SQL - Customers might still see errors.\n- Cloud VPN - Service fully restored 20:41 PST\n- Cloud GKE - Service fully restored 20:41 PST\n- Cloud Storage - Service fully restored 22:27 PST\n- Cloud Dataproc - Service fully restored. 21:50:36 PST\n- Cloud Run - Service fully restored\n- Cloud Spanner - Service fully restored\n- Cloud Pub/Sub - Service restored. 20:38 PST\n- Cloud Dataflow - Services fully restored at 20:12 PST\n- Cloud Bigtable - Service restored. 21:36 PST\n- Cloud Memorystore - Service restored. 20:03 PST\n- Cloud Logging - Services fully restored - 21:18 PST\n- Cloud BigQuery - Services fully restored - 21:21 PST.\n- Cloud Identity \u0026 Security(Cloud Access Policy) - Service fully restored.\n- Cloud Load balancers- Fully restored- 21:47 PST\n- Cloud Persistent Disk - Service fully restored\nWe apologise for the service disruption caused by this issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-24T04:53:47+00:00","modified":"2021-08-24T04:59:37+00:00","when":"2021-08-24T04:53:47+00:00","text":"We are experiencing an issue with Google Cloud infrastructure components at australia-southeast2. We have started seeing recovery with our services.\nProducts impacted and current status:\n- Cloud FileStore - Customers might see API errors.\n- Cloud Networking - Service restored. 20:41 PST\n- Cloud VPN\n- Cloud GKE - Service fully restored.\n- Cloud Storage - Service fully restored.\n- Cloud Dataproc\n- Cloud Run - Service fully restored\n- Cloud Spanner - Service fully restored\n- Cloud Pub/Sub\n- Cloud Dataflow\n- Cloud Bigtable - Service restored. 21:36 PST\n- Cloud Memorystore - Service restored. 20:03 PST\n- Cloud Logging\n- Cloud BigQuery\n- Cloud Identity \u0026 Security(Cloud Access Policy) - Service fully restored.\n- Cloud load balancers- Fully restored\n- Persistent Disk - Service fully restored\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-08-23 22:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis:\nCustomers impacted by this issue may see connectivity issues on Google Cloud Services.\nWorkaround:\nNone at this time…","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-24T03:35:42+00:00","modified":"2021-08-24T04:52:47+00:00","when":"2021-08-24T03:35:42+00:00","text":"We are experiencing an issue with Google Cloud infrastructure components in australia-southeast2 starting 2021-08-23 19:50 PT.\nProducts impacted:\n- Google Cloud Load Balancers\n- Google Compute Engine - Customers might see failure for creating new VMs.\n- Cloud FileStore - Customers might see API errors.\n- Cloud VPN\n- Cloud GKE\n- Cloud PD\n- Cloud Storage\n- Cloud Dataproc\n- Cloud Run\n- Cloud Spanner\n- Cloud Pub/Sub\n- Cloud Dataflow\n- Cloud Bigtable\nOur engineering team continues to investigate the issue.\nWe will provide an update by Monday, 2021-08-23 21:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis:\nCustomers impacted by this issue may see connectivity issues on Google Cloud Services.\nWorkaround:\nNone at this time","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-24T22:59:30+00:00","modified":"2021-08-24T22:59:30+00:00","when":"2021-08-24T22:59:30+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.\n(All Times US/Pacific)\n**Incident Start:** 23 August 2021 19:50\n**Incident End:** 23 August 2021 21:20\n**Duration:** 1 hours, 30 minutes\n**Affected Services and Features:**\n- Cloud Networking\n- Cloud L7 and L4 Load balancers\n- Cloud Interconnect\n- Cloud NAT\n- Cloud VPN\n- Google Compute Engine\n- Google Kubernetes Engine\n- Cloud Persistent Disk\n- Cloud Cloud Storage\n- Cloud FileStore\n- Cloud Dataproc\n- Cloud Run\n- Cloud SQL\n- Cloud Spanner\n- Cloud Pub/Sub\n- Cloud Dataflow\n- Cloud Dataproc\n- Cloud Firestore\n- Cloud Bigtable\n- Cloud Logging\n- Cloud Monitoring\n- Cloud BigQuery\n- Cloud IAM\n**Regions/Zones:** australia-southeast2\n**Description:**\nGoogle Cloud Networking experienced intermittent connectivity issues with Google Cloud Services in australia-southeast2 for 1 hour and 30 minutes. The underlying Cloud Networking impact ended at 20:41, however, some Cloud Services took longer to recover - delaying the all clear until 21:20. Any service that uses Cloud Networking may have seen impact. We have included available details of service specific impact below; however, this may not be a comprehensive accounting of all downstream networking impact. From preliminary analysis, the root cause of the issue was transient voltage at the feeder to the network equipment, causing the equipment to reboot. In order to mitigate the issue, traffic within the australia-southeast2 region was redirected temporarily.\n**Customer Impact:**\n- Cloud Networking: Public IP traffic connectivity failed from 19:51 to 20:41.\n- Cloud L7 Load Balancers: Partial dataplane query loss and control plane operational delay for External load balancers from 19:50 - 20:12, and Internal load balancers from 19:50 - 20:18.\n- Cloud L4 Load Balancing Inbound public IP traffic was dropped from 19:51 to 20:41.\n- Cloud Interconnect: Up to 100% packet loss between 19:50 and 20:21.\n- Cloud NAT experienced control plane failures from 19:51 to 20:00.\n- Cloud VPN HA dropped up to 83% of traffic between 19:51 and 20:21, while Legacy VPN dropped ~100% of traffic between 19:51 and 20:41.\n- Cloud SQL: 100% error rate from 19:49 to 20:01.\n- Cloud Storage: 100% error rate through 21:20.\n- Cloud Functions:\n- Cloud Run: 100% error rate through 21:19.\n- Cloud Bigtable: 100% error rate from 19:49 to 20:01 and increased latency from 20:01 to 20:13.\n- Cloud Logging: Logs written within the region may have failed to be ingested from 20:07 to 21:18.\n- Cloud Monitoring: Customers may have experienced falsely firing alerts, missed alerts, missing metrics and failed writes from 19:50 to 20:15.\n- Google Compute Engine: Operations to create or modify instances failed from 19:52 to 20:12. Connectivity from instances to other GCP services may be affected until 20:26. Existing instances may have lost network connectivity. Additionally autoscaling had delays or errors collecting input data which may have impacted autoscaling decisions.\n- Google Kubernetes Engine: Control plane operations on regional clusters failed between 19:50 and 20:04. Increased latency from 20:05 to 20:41. 100% of requests to container.googleapis.com failed\n- Persistent Disk: Up to 100% device unavailability between 19:51 and 20:13.\n- Cloud Filestore: Up to 100% error rate from 19:50 to 20:03.\n- Cloud IAM: ~80% error rate from 19:52 to 20:10.\n- Cloud Spanner: 100% error rate between 19:53 and 20:09.\n- Cloud Pub/Sub: Increased error rate and latency of up to 95% between 19:50 and 20:12.\n- Cloud Dataflow: Increased errors starting jobs and making progress on existing jobs between 19:50 and 20:12.\n- Cloud Dataproc: New cluster creation failed from 20:09 until 21:20\n- Cloud Firestore: Control Plane saw ~90% error rates from 19:50 to 20:03. Data plane so no significant impact.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Infrastructure Components","id":"uoypgc4GWUyzAKRHPjEv"},{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud NAT","id":"hCNpnTQHkUCCGxJy35Yq"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"},{"title":"Persistent Disk","id":"SzESm2Ux129pjDGKWD68"},{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"},{"title":"Cloud Filestore","id":"jog4nyYkquiLeSK5s26q"},{"title":"Google Cloud Dataproc","id":"yjXrEg3Yvy26BauMwr69"},{"title":"Cloud Run","id":"9D7d2iNBQWN24zc1VamE"},{"title":"Google Cloud SQL","id":"hV87iK5DcEXKgWU2kDri"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"},{"title":"Cloud Firestore","id":"CETSkT92V21G6A1x28me"},{"title":"Google Cloud Bigtable","id":"LfZSuE3xdQU46YMFV5fy"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"},{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"},{"title":"Google BigQuery","id":"9CcrhHUcFevXPSVaSxkf"},{"title":"Identity and Access Management","id":"adnGEDEt9zWzs8uF1oKA"},{"title":"Google Cloud Functions","id":"oW4vJ7VNqyxTWNzSHopX"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/8DhiwfKvD987f5tJrj1G","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"QEcZsLuW9bqYZggLZXgS","number":"7343907517039605904","begin":"2021-08-19T03:40:00+00:00","created":"2021-08-19T09:02:38+00:00","end":"2021-08-19T08:40:00+00:00","modified":"2021-08-19T09:27:54+00:00","external_desc":"We experienced performance issues with Cloud Dataflow between 2021-08-18 20:40 Pacific Time to 2021-08-19 01:40 Pacific Time.\nSome streaming jobs may have seen higher than usual watermark. Multiple regions affected eg: europe-west1, us-west2\nThe issue is fully resolved and service is restored.\nWe apologize to all who are affected by the disruption.","updates":[{"created":"2021-08-19T09:02:38+00:00","modified":"2021-08-19T09:02:39+00:00","when":"2021-08-19T09:02:38+00:00","text":"Performance degradation of Dataflow streaming jobs in multiple regions.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-19T09:02:38+00:00","modified":"2021-08-19T09:02:39+00:00","when":"2021-08-19T09:02:38+00:00","text":"Performance degradation of Dataflow streaming jobs in multiple regions.","status":"SERVICE_DISRUPTION","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"T9bFoXPqG8w8g1YbWTKY","service_name":"Google Cloud Dataflow","affected_products":[{"title":"Google Cloud Dataflow","id":"T9bFoXPqG8w8g1YbWTKY"}],"uri":"incidents/QEcZsLuW9bqYZggLZXgS","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"CWYZQhBtU4sf2cYJ8AAf","number":"6700504986737780589","begin":"2021-08-18T18:15:00+00:00","created":"2021-08-18T20:42:34+00:00","end":"2021-08-18T22:06:00+00:00","modified":"2021-08-19T19:05:10+00:00","external_desc":"The issue with Cloud Logging has been resolved for all affected projects.","updates":[{"created":"2021-08-19T19:03:54+00:00","modified":"2021-08-19T19:05:10+00:00","when":"2021-08-19T19:03:54+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 18 August 2021 11:15\n**Incident End:** 18 August 2021 15:06\n**Duration:** 3 hours, 51 minutes\n**Affected Services and Features:** Cloud Logging\n**Regions/Zones:** Global\n**Description:**\nCloud Logging experienced increased read errors globally for 3 hours, 51 minutes. From preliminary analysis, the root cause of the issue was due to a large increase in work from an internal service which overwhelmed the available capacity on a backend database. Log ingestion was not impacted.\n**Customer Impact:**\n* Customers experienced elevated read failures when attempting to query logs.\n* Customers were unable to access logs in the Cloud Console due to query timeouts.\n**Additional details:**\nThe issue has been mitigated and the problematic service has been identified and canceled. Query operations have returned to normal.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-18T22:48:26+00:00","modified":"2021-08-18T22:48:26+00:00","when":"2021-08-18T22:48:26+00:00","text":"The issue with Cloud Logging has been resolved for all affected projects as of Wednesday, 2021-08-18 15:30 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-18T21:51:35+00:00","modified":"2021-08-18T21:51:35+00:00","when":"2021-08-18T21:51:35+00:00","text":"Summary: We are experiencing elevated read error rates with Cloud Logging in multiple regions.\nDescription: Mitigation work is currently underway by our engineering team.\nThe mitigation is expected to complete by Wednesday, 2021-08-18 16:00 US/Pacific.\nWe will provide more information by Wednesday, 2021-08-18 16:00 US/Pacific.\nDiagnosis: Some customers may be unable to view logging in the Cloud Console.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-18T21:38:15+00:00","modified":"2021-08-18T21:38:15+00:00","when":"2021-08-18T21:38:15+00:00","text":"Summary: We are experiencing elevated read error rates with Cloud Logging in multiple regions.\nDescription: We are experiencing elevated read error rates with Cloud Logging in multiple regions beginning at Wednesday, 2021-08-18 11:15 US/Pacific.\nOur engineering team is working on mitigation options.\nWe will provide an update by Wednesday, 2021-08-18 15:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some customers may be unable to view logging in the Cloud Console.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-18T21:14:56+00:00","modified":"2021-08-18T21:14:56+00:00","when":"2021-08-18T21:14:56+00:00","text":"Summary: We are experiencing elevated read error rates with Cloud Logging in multiple regions.\nDescription: We are experiencing elevated read error rates with Cloud Logging in multiple regions beginning at Wednesday, 2021-08-18 11:15 US/Pacific.\nOur engineering team is exploring mitigation options.\nWe will provide an update by Wednesday, 2021-08-18 14:45 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Some customers may be unable to view logging in the Cloud Console.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-18T20:42:33+00:00","modified":"2021-08-18T21:08:25+00:00","when":"2021-08-18T20:42:33+00:00","text":"Summary: We are experiencing elevated read error rates with Cloud Logging in multiple regions\nDescription: We are experiencing elevated read errors with Cloud Logging in multiple regions beginning at Wednesday, 2021-08-18 11:15 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-08-18 14:15 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may be unable to view logging in the Cloud Console.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-19T19:03:54+00:00","modified":"2021-08-19T19:05:10+00:00","when":"2021-08-19T19:03:54+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 18 August 2021 11:15\n**Incident End:** 18 August 2021 15:06\n**Duration:** 3 hours, 51 minutes\n**Affected Services and Features:** Cloud Logging\n**Regions/Zones:** Global\n**Description:**\nCloud Logging experienced increased read errors globally for 3 hours, 51 minutes. From preliminary analysis, the root cause of the issue was due to a large increase in work from an internal service which overwhelmed the available capacity on a backend database. Log ingestion was not impacted.\n**Customer Impact:**\n* Customers experienced elevated read failures when attempting to query logs.\n* Customers were unable to access logs in the Cloud Console due to query timeouts.\n**Additional details:**\nThe issue has been mitigated and the problematic service has been identified and canceled. Query operations have returned to normal.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/CWYZQhBtU4sf2cYJ8AAf","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"9hh2XPBVqqem4DxXZxui","number":"271309249334889945","begin":"2021-08-16T08:19:00+00:00","created":"2021-08-17T15:17:36+00:00","end":"2021-08-17T15:55:00+00:00","modified":"2021-08-17T21:08:01+00:00","external_desc":"Cloud Router creations are failing in southamerica-east1","updates":[{"created":"2021-08-17T21:06:04+00:00","modified":"2021-08-17T21:08:01+00:00","when":"2021-08-17T21:06:04+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 16 August 2021 01:19\n**Incident End:** 17 August 2021 08:55\n**Duration:** 1 day, 7 hours, 36 minutes\n**Affected Services and Features:** Cloud Router\n**Regions/Zones:** southamerica-east1\n**Description:**\nGoogle Cloud Router experienced failures creating or modifying routers in southamerica-east1 for a duration of 1 day, 7 hours and 36 minutes. New Cloud router creations or modifications to existing Cloud Routers would have been impacted. From preliminary analysis, the root cause of the issue is related to a rollout within the virtual routing control plane for the region, which had a configuration mismatch as part of an ongoing migration.\n**Customer Impact:**\n- Customers were unable to create or modify existing Cloud Routers from 16 August 2021 01:19 to 17 August 2021 08:34.\n- Router status requests [1] which rely on the getRouterStatus [2] API may have also returned errors during the incident\n- All existing routers that had completed programming continued to work throughout the entire duration.\n- Dynamic routing functionality [3] using BGP routes continued to work as expected.\n**Additional details:**\n- In order to prevent a recurrence, we are fixing outdated automations used during migrations\n- In order to reduce detection delays, we are improving our control plane probers to ensure we are alerted promptly\n**Reference(s):**\n[1] https://cloud.google.com/network-connectivity/docs/router/how-to/viewing-router-details#viewing-router-status\n[2] https://cloud.google.com/compute/docs/reference/rest/v1/routers/getRouterStatus\n[3] https://cloud.google.com/vpc/docs/routes#dynamic_routes","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-17T17:14:20+00:00","modified":"2021-08-17T17:14:21+00:00","when":"2021-08-17T17:14:20+00:00","text":"The issue with Cloud Router has been resolved for all affected projects as of Tuesday, 2021-08-17 10:00 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-17T16:08:17+00:00","modified":"2021-08-17T16:08:18+00:00","when":"2021-08-17T16:08:17+00:00","text":"Summary: Cloud Router creations are failing in southamerica-east1\nDescription: We believe the issue with Cloud Router is partially resolved. Full resolution is expected to complete by Tuesday, 2021-08-17 10:15 US/Pacific. We will provide an update by Tuesday, 2021-08-17 10:15 US/Pacific with current details.\nDiagnosis: New Cloud Router creations should be failing in southamerica-east1. Status request operations may also fail.\nWorkaround: Avoid making changes to existing router within southamerica-east1.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-17T15:17:36+00:00","modified":"2021-08-17T15:34:39+00:00","when":"2021-08-17T15:17:36+00:00","text":"Summary: Cloud Router creations are failing in southamerica-east1\nDescription: We are experiencing an issue with Cloud Router pertaining to creations within southamerica-east1. Our engineering team continues to investigate the issue. We will provide an update by Tuesday, 2021-08-17 09:10 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: New Cloud Router creations should be failing in southamerica-east1. Status request operations may also fail\nWorkaround: Avoid making changes to existing router within southamerica-east1.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-17T21:06:04+00:00","modified":"2021-08-17T21:08:01+00:00","when":"2021-08-17T21:06:04+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 16 August 2021 01:19\n**Incident End:** 17 August 2021 08:55\n**Duration:** 1 day, 7 hours, 36 minutes\n**Affected Services and Features:** Cloud Router\n**Regions/Zones:** southamerica-east1\n**Description:**\nGoogle Cloud Router experienced failures creating or modifying routers in southamerica-east1 for a duration of 1 day, 7 hours and 36 minutes. New Cloud router creations or modifications to existing Cloud Routers would have been impacted. From preliminary analysis, the root cause of the issue is related to a rollout within the virtual routing control plane for the region, which had a configuration mismatch as part of an ongoing migration.\n**Customer Impact:**\n- Customers were unable to create or modify existing Cloud Routers from 16 August 2021 01:19 to 17 August 2021 08:34.\n- Router status requests [1] which rely on the getRouterStatus [2] API may have also returned errors during the incident\n- All existing routers that had completed programming continued to work throughout the entire duration.\n- Dynamic routing functionality [3] using BGP routes continued to work as expected.\n**Additional details:**\n- In order to prevent a recurrence, we are fixing outdated automations used during migrations\n- In order to reduce detection delays, we are improving our control plane probers to ensure we are alerted promptly\n**Reference(s):**\n[1] https://cloud.google.com/network-connectivity/docs/router/how-to/viewing-router-details#viewing-router-status\n[2] https://cloud.google.com/compute/docs/reference/rest/v1/routers/getRouterStatus\n[3] https://cloud.google.com/vpc/docs/routes#dynamic_routes","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/9hh2XPBVqqem4DxXZxui","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"yC6jA5p6Taoij6pPgSSW","number":"12084164814980114469","begin":"2021-08-06T21:25:00+00:00","created":"2021-08-06T22:11:39+00:00","end":"2021-08-07T03:53:00+00:00","modified":"2021-08-09T16:02:47+00:00","external_desc":"Elevated error rate across all monitoring API endpoints globally","updates":[{"created":"2021-08-09T16:02:22+00:00","modified":"2021-08-09T16:02:22+00:00","when":"2021-08-09T16:02:22+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 06 August 2021 14:25\n**Incident End:** 06 August 2021 20:53\n**Duration:** 6 hours, 28 minutes\n**Affected Services and Features:**\nGoogle Cloud Monitoring\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Monitoring experienced increased latency and error rates for monitoring endpoints globally for 6 hours, 28 minutes. From preliminary analysis, the root cause of the issue is an overload of a monitoring API dependency that serves metric and monitored resource descriptors.\n**Customer Impact:**\nRequests against the monitoring API would have seen increased timeouts, errors, and latency.\nCloud Monitoring dashboards would have failed to load due to timeout.\n**Additional details:**\nThis service disruption was mitigated by increasing the resources available to the affected dependency, and we are confident that there will not be a recurrence.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-07T04:32:21+00:00","modified":"2021-08-07T04:32:21+00:00","when":"2021-08-07T04:32:21+00:00","text":"The issue with Cloud Monitoring has been resolved for all affected users as of Friday, 2021-08-06 20:53 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-07T00:51:30+00:00","modified":"2021-08-07T00:51:30+00:00","when":"2021-08-07T00:51:30+00:00","text":"Summary: Elevated error rate across all monitoring API endpoints globally\nDescription: Mitigation work is still underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-08-06 21:29 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-08-06T23:56:27+00:00","modified":"2021-08-06T23:56:28+00:00","when":"2021-08-06T23:56:27+00:00","text":"Summary: Elevated error rate across all monitoring API endpoints globally\nDescription: Mitigation work is still underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-08-06 17:59 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-08-06T22:58:23+00:00","modified":"2021-08-06T22:58:23+00:00","when":"2021-08-06T22:58:23+00:00","text":"Summary: Elevated error rate across all monitoring API endpoints globally\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-08-06 16:59 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-08-06T22:25:19+00:00","modified":"2021-08-06T22:25:25+00:00","when":"2021-08-06T22:25:19+00:00","text":"Summary: Elevated error rate across all monitoring API endpoints globally\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-08-06 15:59 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-08-06T22:11:39+00:00","modified":"2021-08-06T22:11:40+00:00","when":"2021-08-06T22:11:39+00:00","text":"Summary: Elevated error rate across all monitoring API endpoints globally\nDescription: We are experiencing an issue with Cloud Monitoring beginning at Friday, 2021-08-06 14:25 US/Pacific US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide the next update by Friday, 2021-08-06 15:40 US/Pacific US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]}],"most_recent_update":{"created":"2021-08-09T16:02:22+00:00","modified":"2021-08-09T16:02:22+00:00","when":"2021-08-09T16:02:22+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 06 August 2021 14:25\n**Incident End:** 06 August 2021 20:53\n**Duration:** 6 hours, 28 minutes\n**Affected Services and Features:**\nGoogle Cloud Monitoring\n**Regions/Zones:** Global\n**Description:**\nGoogle Cloud Monitoring experienced increased latency and error rates for monitoring endpoints globally for 6 hours, 28 minutes. From preliminary analysis, the root cause of the issue is an overload of a monitoring API dependency that serves metric and monitored resource descriptors.\n**Customer Impact:**\nRequests against the monitoring API would have seen increased timeouts, errors, and latency.\nCloud Monitoring dashboards would have failed to load due to timeout.\n**Additional details:**\nThis service disruption was mitigated by increasing the resources available to the affected dependency, and we are confident that there will not be a recurrence.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Monitoring","id":"3zaaDb7antc73BM1UAVT"}],"uri":"incidents/yC6jA5p6Taoij6pPgSSW","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"f8NTn1ZFA4vXTd3CHQgw","number":"11183604505216208942","begin":"2021-08-06T09:38:00+00:00","created":"2021-08-06T09:41:46+00:00","end":"2021-08-06T10:38:00+00:00","modified":"2022-05-26T20:06:39+00:00","external_desc":"Significant packet loss on Cloud Interconnect in europe-west2 for some customers.","updates":[{"created":"2021-08-06T10:38:20+00:00","modified":"2021-08-06T10:38:21+00:00","when":"2021-08-06T10:38:20+00:00","text":"This incident with Cloud Networking was initially triggered by our internal monitoring systems.\nThere is a current incident running for this issue please ref:\nhttps://status.cloud.google.com/incidents/oNMgU4ZjJFCiFTfeMyDX","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-06T10:28:59+00:00","modified":"2021-08-06T10:29:00+00:00","when":"2021-08-06T10:28:59+00:00","text":"Summary: Significant packet loss on Cloud Interconnect in europe-west2 for some customers.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-08-06 04:15 US/Pacific.\nDiagnosis: Visible, near 100% packet loss on Cloud Interconnect.\nWorkaround: Fail over to other Cloud regions.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-06T10:02:19+00:00","modified":"2021-08-06T10:02:21+00:00","when":"2021-08-06T10:02:19+00:00","text":"Summary: Significant packet loss on Cloud Interconnect in europe-west2 for some customers.\nDescription: We believe the issue with Cloud Interconnect is partially resolved.\nWe will provide an update by Friday, 2021-08-06 03:45 US/Pacific with current details.\nDiagnosis: Visible, near 100% packet loss on Cloud Interconnect.\nWorkaround: Fail over to other Cloud regions.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-06T09:41:37+00:00","modified":"2021-08-06T09:41:47+00:00","when":"2021-08-06T09:41:37+00:00","text":"Summary: Significant packet loss on Cloud Interconnect in europe-west2 for some customers.\nDescription: We are experiencing an issue with Cloud Interconnect beginning at Friday, 2021-08-06 01:55 US/Pacific.\nWe're seeing significant packet loss on Cloud Interconnect in europe-west2 for some customers.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-08-06 03:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Visible, near 100% packet loss on Cloud Interconnect.\nWorkaround: Fail over to other Cloud regions.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-06T10:38:20+00:00","modified":"2021-08-06T10:38:21+00:00","when":"2021-08-06T10:38:20+00:00","text":"This incident with Cloud Networking was initially triggered by our internal monitoring systems.\nThere is a current incident running for this issue please ref:\nhttps://status.cloud.google.com/incidents/oNMgU4ZjJFCiFTfeMyDX","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/f8NTn1ZFA4vXTd3CHQgw","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"oNMgU4ZjJFCiFTfeMyDX","number":"16825402852599713412","begin":"2021-08-06T08:56:00+00:00","created":"2021-08-06T10:00:16+00:00","end":"2021-08-06T12:41:00+00:00","modified":"2021-08-06T19:03:17+00:00","external_desc":"We've been informed that Cloud Networking, and Load Balancer, are not functioning in europe-west2","updates":[{"created":"2021-08-06T18:56:46+00:00","modified":"2021-08-06T19:03:17+00:00","when":"2021-08-06T18:56:46+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Timeline:**\n06 August 2021 01:56 - 03:23\n06 August 2021 05:27 - 05:41\n**Duration:** 1 hours, 41 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking, Cloud Load Balancing, Compute Engine, Cloud Spanner, Pub/Sub, Cloud Interconnect\n**Regions/Zones:** europe-west2-a\n**Description:**\nGoogle Cloud Networking and Google Cloud Load Balancing (GCLB) experienced elevated packet loss in europe-west2-a. From preliminary analysis, the root cause of the issue is a configuration change on a device that caused a permission issue with an automatic configuration deployment and failed to propagate. This resulted in a corrupted configuration which caused traffic to be dropped.\n**Customer Impact:**\nCustomers experienced elevated packet loss or connection terminations for connections in europe-west2-a.\nA small number of Compute Engine instances failed to start in europe-west2-a.\nCloud Interconnect in europe-west2 observed an increase in packet loss.\nCloud Spanner requests passing through europe-west2-a experienced increased error rates.\nCloud Pub/Sub may have experienced increased error rates when publishing to topics in europe-west2-a.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-06T11:31:04+00:00","modified":"2021-08-06T11:31:08+00:00","when":"2021-08-06T11:31:04+00:00","text":"The issue with Cloud Networking / Load Balancing has been resolved for all affected users as of Friday, 2021-08-06 03:22 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-06T10:59:58+00:00","modified":"2021-08-06T11:00:01+00:00","when":"2021-08-06T10:59:58+00:00","text":"Summary: We've been informed that Cloud Networking, and Load Balancer, are not functioning in europe-west2\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-08-06 04:36 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-06T10:00:13+00:00","modified":"2021-08-06T10:00:17+00:00","when":"2021-08-06T10:00:13+00:00","text":"Summary: We've been informed that Cloud Networking, and Load Balancer, are not functioning in europe-west2\nDescription: We are investigating a potential issue with Cloud Networking and/or Load Balancing in europe-west2.\nWe will provide more information by Friday, 2021-08-06 04:00 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-06T18:56:46+00:00","modified":"2021-08-06T19:03:17+00:00","when":"2021-08-06T18:56:46+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Timeline:**\n06 August 2021 01:56 - 03:23\n06 August 2021 05:27 - 05:41\n**Duration:** 1 hours, 41 minutes\n**Affected Services and Features:**\nGoogle Cloud Networking, Cloud Load Balancing, Compute Engine, Cloud Spanner, Pub/Sub, Cloud Interconnect\n**Regions/Zones:** europe-west2-a\n**Description:**\nGoogle Cloud Networking and Google Cloud Load Balancing (GCLB) experienced elevated packet loss in europe-west2-a. From preliminary analysis, the root cause of the issue is a configuration change on a device that caused a permission issue with an automatic configuration deployment and failed to propagate. This resulted in a corrupted configuration which caused traffic to be dropped.\n**Customer Impact:**\nCustomers experienced elevated packet loss or connection terminations for connections in europe-west2-a.\nA small number of Compute Engine instances failed to start in europe-west2-a.\nCloud Interconnect in europe-west2 observed an increase in packet loss.\nCloud Spanner requests passing through europe-west2-a experienced increased error rates.\nCloud Pub/Sub may have experienced increased error rates when publishing to topics in europe-west2-a.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"},{"title":"Cloud Spanner","id":"EcNGGUgBtBLrtm4mWvqC"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"},{"title":"Hybrid Connectivity","id":"5x6CGnZvSHQZ26KtxpK1"}],"uri":"incidents/oNMgU4ZjJFCiFTfeMyDX","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"LGFBxyLwbh92E47fAzJ5","number":"135405297875892189","begin":"2021-08-01T07:00:00+00:00","created":"2021-08-03T20:28:27+00:00","end":"2021-08-04T23:18:00+00:00","modified":"2021-08-05T17:35:12+00:00","external_desc":"Mutliregional Price for E2 Free Tier core is set incorrectly","updates":[{"created":"2021-08-05T17:34:33+00:00","modified":"2021-08-05T17:34:34+00:00","when":"2021-08-05T17:34:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 01 August 2021 00:00\n**Incident End:** 04 August 2021 16:18\n**Duration:** 3 days, 16 hours, 18 minutes\n**Affected Services and Features:**\nGoogle Compute Engine Billing\n**Regions/Zones:** us-west1, us-east1, us-central1\n**Description:**\nGoogle Compute Engine Billing experienced incorrect charges in us-west1, us-east1, and us-central1 for e2-micro VMs for 3 days, 16 hours, 18 minutes. From preliminary analysis, the root cause was an incorrect discount being applied to E2 core usage for e2-micro VMs due to a mismatch between metered usage and the discount for the E2 SKU.\n**Customer Impact:**\nCustomers may have seen incorrect charges for e2-micro VMs in us-west1, us-east1, and us-central1 regions during the duration of the incident.\n**Additional details:**\nThis issue has been fully resolved, and charges have been corrected for all affected projects. No action is required from customers regarding this issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-05T12:52:04+00:00","modified":"2021-08-05T12:52:05+00:00","when":"2021-08-05T12:52:04+00:00","text":"The issue with Google Compute Engine has been resolved for all affected users as of Thursday, 2021-08-05 05:51 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-08-05T08:35:48+00:00","modified":"2021-08-05T08:35:54+00:00","when":"2021-08-05T08:35:48+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: We believe the issue with Google Compute Engine pricing is partially resolved.\nWe are now confirming if the fix addresses all kinds of corner cases.\nWe will provide an update by Thursday, 2021-08-05 08:30 US/Pacific with current details.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-05T04:59:08+00:00","modified":"2021-08-05T04:59:09+00:00","when":"2021-08-05T04:59:08+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: We believe the issue with Google Compute Engine pricing is partially resolved.\nWe are now confirming if the fix addresses all kinds of corner cases.\nWe will provide an update by Thursday, 2021-08-05 02:40 US/Pacific with current details.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-04T17:51:06+00:00","modified":"2021-08-04T17:51:07+00:00","when":"2021-08-04T17:51:06+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: Mitigation work is still underway by our engineering teamMitigation work is still underway by our engineering team.\nThe full resolution is expected to completed by Wednesday, 2021-08-04 21:00 US/Pacific.\nWe will provide more information by Wednesday, 2021-08-04 22:00 US/Pacific.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-04T17:33:57+00:00","modified":"2021-08-04T17:34:00+00:00","when":"2021-08-04T17:33:57+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: Mitigation work is still underway by our engineering team.\nValidation in the testing environment has been completed.\nThe full mitigation is expected to complete by Wednesday, 2021-08-04 21:00 US/Pacific.\nWe will provide more information by Wednesday, 2021-08-04 13:15 US/Pacific.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-04T16:45:00+00:00","modified":"2021-08-04T16:45:02+00:00","when":"2021-08-04T16:45:00+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: Our engineering team have a mitigation underway.\nWe are finalizing the validation of the fix in our testing environment, we will provide an update on the production rollout as soon as that is complete.\nWe will provide more information by Wednesday, 2021-08-04 12:00 US/Pacific.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-04T11:42:17+00:00","modified":"2021-08-04T11:42:19+00:00","when":"2021-08-04T11:42:17+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: Customers in us-west1 and central1 creating E2-micro VMs are being incorrectly charged. Our engineering team have a mitigation underway.\nThe mitigation is expected to complete by Wednesday, 2021-08-04 end of day US/Pacific.\nWe will provide more information by Wednesday, 2021-08-04 10:00 US/Pacific.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-04T00:31:55+00:00","modified":"2021-08-04T00:31:56+00:00","when":"2021-08-04T00:31:55+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: Customers in us-west1 and central1 creating E2-micro VMs are being incorrectly charged. Our engineering team have a mitigation underway.\nThe mitigation is expected to complete by Wednesday, 2021-08-04 EOD US/Pacific.\nWe will provide more information by Wednesday, 2021-08-04 06:00 US/Pacific.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-03T22:31:53+00:00","modified":"2021-08-03T22:31:56+00:00","when":"2021-08-03T22:31:53+00:00","text":"Summary: Mutliregional Price for E2 Free Tier core is set incorrectly\nDescription: Customers in us-west1 and central1 creating E2-micro VMs are being incorrectly charged. Our engineering team have a mitigation underway.\nMitigation progress is expected by Tuesday, 2021-08-03 17:30 US/Pacific.\nWe will provide more information by Tuesday, 2021-08-03 17:30 US/Pacific.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-03T21:24:47+00:00","modified":"2021-08-03T21:24:47+00:00","when":"2021-08-03T21:24:47+00:00","text":"Summary: Price for E2 Free Tier core is set incorrectly\nDescription: We are experiencing an issue with Google Cloud Billing related to general purpose e2-micro VMs., beginning at Sunday, 2021-08-01 00:00 US/Pacific}.\nCustomers creating E2-micro VMs are being incorrectly charged. Our engineering team have a mitigation underway.\nWe will provide an update by Tuesday, 2021-08-03 15:25 US/Pacific with current details.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-08-03T20:28:26+00:00","modified":"2021-08-03T20:28:27+00:00","when":"2021-08-03T20:28:26+00:00","text":"Summary: Price for E2 Free Tier core is set incorrectly\nDescription: We are experiencing an issue with Google Cloud Billing related to general purpose e2-micro VMs., beginning at Sunday, 2021-08-01 00:00 US/Pacific}.\nCustomers creating E2-micro VMs are being incorrectly charged. Our engineering team have a mitigation underway.\nWe will provide an update by Tuesday, 2021-08-03 14:25 US/Pacific with current details.\nDiagnosis: Customers might see that they're being incorrectly charged for a general purpose e2-micro VMs.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-05T17:34:33+00:00","modified":"2021-08-05T17:34:34+00:00","when":"2021-08-05T17:34:33+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 01 August 2021 00:00\n**Incident End:** 04 August 2021 16:18\n**Duration:** 3 days, 16 hours, 18 minutes\n**Affected Services and Features:**\nGoogle Compute Engine Billing\n**Regions/Zones:** us-west1, us-east1, us-central1\n**Description:**\nGoogle Compute Engine Billing experienced incorrect charges in us-west1, us-east1, and us-central1 for e2-micro VMs for 3 days, 16 hours, 18 minutes. From preliminary analysis, the root cause was an incorrect discount being applied to E2 core usage for e2-micro VMs due to a mismatch between metered usage and the discount for the E2 SKU.\n**Customer Impact:**\nCustomers may have seen incorrect charges for e2-micro VMs in us-west1, us-east1, and us-central1 regions during the duration of the incident.\n**Additional details:**\nThis issue has been fully resolved, and charges have been corrected for all affected projects. No action is required from customers regarding this issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"L3ggmi3Jy4xJmgodFA9K","service_name":"Google Compute Engine","affected_products":[{"title":"Google Compute Engine","id":"L3ggmi3Jy4xJmgodFA9K"}],"uri":"incidents/LGFBxyLwbh92E47fAzJ5","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"ZJoBx7kocBokFxS1ZN5p","number":"7204119448599013942","begin":"2021-07-29T02:24:00+00:00","created":"2021-07-30T15:27:02+00:00","end":"2021-07-31T01:52:00+00:00","modified":"2021-08-02T18:46:57+00:00","external_desc":"Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.","updates":[{"created":"2021-08-02T18:45:48+00:00","modified":"2021-08-02T18:46:57+00:00","when":"2021-08-02T18:45:48+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 28 July 2021 19:24\n**Incident End:** 30 July 2021 18:52\n**Duration:** 1 day, 23 hours and 28 minutes\n**Affected Services and Features:**\nGoogle Cloud Dataproc - Ability to view and access Component Gateway URLs [1]\n**Regions/Zones:**\nasia-east1, asia-east2, asia-northeast1, asia-northeast3, asia-south1, asia-southeast1, asia-southeast2, australia-southeast1, europe-north1, europe-west1, europe-west2, europe-west4, europe-west6, northamerica-northeast1, southamerica-east1, us-central1, us-east1, us-east4, us-west1\n**Description:**\nGoogle Cloud Dataproc experienced elevated 400 errors indicating “Bad Requests” when accessing the component gateway URLs[1] for a duration of around 2 days. From preliminary analysis, the root cause of the issue was a rollout that started on 28 July 2021 at 12:45. The rollout was paused preventing further regions from being affected and a rollback started on 30 July 2021 at 9:18 to mitigate the issue in affected regions . During the incident, a workaround was provided which was to use the SSH SOCKS proxy as described in doc [2] to access the component gateway URLs.\n**Customer Impact:**\nGoogle Cloud Dataproc- Accessing component gateway URLs failed with 400 errors.\n**References:**\n[1] https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls\n[2] https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-31T02:04:20+00:00","modified":"2021-07-31T02:04:20+00:00","when":"2021-07-31T02:04:20+00:00","text":"The issue with Cloud Dataproc has been resolved for all affected projects as of Friday, 2021-07-30 19:03 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-31T01:18:39+00:00","modified":"2021-07-31T01:18:39+00:00","when":"2021-07-31T01:18:39+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: We have observed reduction in the error rate and the issue is currently intermittent.\nEngineering team continues to work on the mitigation. The mitigation is in progress and estimated to complete by Friday, 2021-07-30 19:00 US/Pacific.\nAction: Utilize the SOCKS proxy to access their UIs as a workaround. Please refer to workaround section for more details.\nWe will provide more information by Friday, 2021-07-30 19:00 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: Customers can use the SOCKS proxy to access their UIs while component gateway is not working.\nPlease refer to the link for more details: https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T22:59:37+00:00","modified":"2021-07-30T22:59:37+00:00","when":"2021-07-30T22:59:37+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: We have observed reduction in the error rate and the issue is currently intermittent.\nEngineering team continues to work on the mitigation. The mitigation is estimated to complete by Friday, 2021-07-30 18:00 US/Pacific.\nAction: Utilize the SOCKS proxy to access their UIs as a workaround. Please refer to workaround section for more details.\nWe will provide more information by Friday, 2021-07-30 18:00 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: Customers can use the SOCKS proxy to access their UIs while component gateway is not working.\nPlease refer to the link for more details: https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T22:59:13+00:00","modified":"2021-07-30T22:59:14+00:00","when":"2021-07-30T22:59:13+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: We have observed reduction in the error rate and the issue is currently intermittent.\nEngineering team continues to work on the mitigation. The mitigation is estimated to complete by Friday, 2021-07-30 16:00 US/Pacific.\nAction: Utilize the SOCKS proxy to access their UIs as a workaround. Please refer to workaround section for more details.\nWe will provide more information by Friday, 2021-07-30 18:00 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: Customers can use the SOCKS proxy to access their UIs while component gateway is not working.\nPlease refer to the link for more details: https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T20:17:43+00:00","modified":"2021-07-30T20:17:44+00:00","when":"2021-07-30T20:17:43+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: We have observed reduction in the error rate and the issue is currently intermittent.\nEngineering team continues to work on the mitigation. The mitigation is estimated to complete by Friday, 2021-07-30 16:00 US/Pacific.\nAction: Utilize socks proxy to access their UIs as a workaround. Please refer to workaround section for more details.\nWe will provide more information by Friday, 2021-07-30 16:00 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: Customers can use the socks proxy to access their UIs while component gateway is not working.\nPlease refer to the link for more details: https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T19:27:43+00:00","modified":"2021-07-30T19:27:43+00:00","when":"2021-07-30T19:27:43+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: Mitigation work is still underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nAction: Utilize socks proxy to access their UIs as a workaround. Please refer to workaround section for more details.\nWe will provide more information by Friday, 2021-07-30 13:30 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: Customers can use the socks proxy to access their UIs while component gateway is not working.\nPlease refer to the link for more details: https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T18:07:47+00:00","modified":"2021-07-30T18:07:47+00:00","when":"2021-07-30T18:07:47+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nAction: Utilize socks proxy to access their UIs as a workaround. Please refer to workaround section for more details.\nWe will provide more information by Friday, 2021-07-30 12:30 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: Customers can use the socks proxy to access their UIs while component gateway is not working.\nPlease refer to the link for more details: https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T17:12:20+00:00","modified":"2021-07-30T17:12:23+00:00","when":"2021-07-30T17:12:20+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request). Unable to access clusters details page after cluster creation.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-07-30 11:30 US/Pacific.\nDiagnosis: Unable to access the cluster details page after cluster creation.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T16:47:10+00:00","modified":"2021-07-30T16:47:10+00:00","when":"2021-07-30T16:47:10+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request)\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Friday, 2021-07-30 11:00 US/Pacific.\nDiagnosis: Increased failures with 400 bad request error\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T16:27:42+00:00","modified":"2021-07-30T16:27:48+00:00","when":"2021-07-30T16:27:42+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request)\nDescription: We are experiencing an issue with Cloud Dataproc.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-07-30 10:16 US/Pacific with current details.\nDiagnosis: Increased failures with 400 bad request error\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T15:56:51+00:00","modified":"2021-07-30T15:56:56+00:00","when":"2021-07-30T15:56:51+00:00","text":"Summary: Multi-region: Accessing Component Gateway fails with 400 (Bad Request)\nDescription: We are experiencing an issue with Cloud Dataproc.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-07-30 09:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Increased failures with 400 bad request error\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-30T15:26:54+00:00","modified":"2021-07-30T15:27:03+00:00","when":"2021-07-30T15:26:54+00:00","text":"Summary: Accessing Component Gateway fails with 400 (Bad Request)\nDescription: We are experiencing an issue with Cloud Dataproc.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Friday, 2021-07-30 09:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Increased failures with 400 bad request error\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-08-02T18:45:48+00:00","modified":"2021-08-02T18:46:57+00:00","when":"2021-08-02T18:45:48+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 28 July 2021 19:24\n**Incident End:** 30 July 2021 18:52\n**Duration:** 1 day, 23 hours and 28 minutes\n**Affected Services and Features:**\nGoogle Cloud Dataproc - Ability to view and access Component Gateway URLs [1]\n**Regions/Zones:**\nasia-east1, asia-east2, asia-northeast1, asia-northeast3, asia-south1, asia-southeast1, asia-southeast2, australia-southeast1, europe-north1, europe-west1, europe-west2, europe-west4, europe-west6, northamerica-northeast1, southamerica-east1, us-central1, us-east1, us-east4, us-west1\n**Description:**\nGoogle Cloud Dataproc experienced elevated 400 errors indicating “Bad Requests” when accessing the component gateway URLs[1] for a duration of around 2 days. From preliminary analysis, the root cause of the issue was a rollout that started on 28 July 2021 at 12:45. The rollout was paused preventing further regions from being affected and a rollback started on 30 July 2021 at 9:18 to mitigate the issue in affected regions . During the incident, a workaround was provided which was to use the SSH SOCKS proxy as described in doc [2] to access the component gateway URLs.\n**Customer Impact:**\nGoogle Cloud Dataproc- Accessing component gateway URLs failed with 400 errors.\n**References:**\n[1] https://cloud.google.com/dataproc/docs/concepts/accessing/dataproc-gateways#viewing_and_accessing_component_gateway_urls\n[2] https://cloud.google.com/dataproc/docs/concepts/accessing/cluster-web-interfaces","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"yjXrEg3Yvy26BauMwr69","service_name":"Google Cloud Dataproc","affected_products":[{"title":"Google Cloud Dataproc","id":"yjXrEg3Yvy26BauMwr69"}],"uri":"incidents/ZJoBx7kocBokFxS1ZN5p","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"MfiGCW4E26MPGRnCJ8by","number":"13858523345881857527","begin":"2021-07-27T17:35:27+00:00","created":"2021-07-27T18:00:25+00:00","end":"2021-07-27T23:15:35+00:00","modified":"2021-07-27T23:15:35+00:00","external_desc":"us-central1: GCS is returning stale version of object for bucket","updates":[{"created":"2021-07-27T23:15:28+00:00","modified":"2021-07-27T23:15:34+00:00","when":"2021-07-27T23:15:28+00:00","text":"The issue with Google Cloud Storage is believed to be affecting a very small number of projects and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T22:07:26+00:00","modified":"2021-07-27T22:07:31+00:00","when":"2021-07-27T22:07:26+00:00","text":"Summary: us-central1: GCS is returning stale version of object for bucket\nDescription: We believe the issue with Google Cloud Storage is partially resolved.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-07-27 15:56 US/Pacific with current details.\nDiagnosis: GCS is intermittently serving older deleted versions of objects, or not serving 404 for deleted objects.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T20:26:21+00:00","modified":"2021-07-27T20:26:25+00:00","when":"2021-07-27T20:26:21+00:00","text":"Summary: us-central1: GCS is returning stale version of object for bucket\nDescription: Our engineering team is currently investigating the issue.\nThis issue affects get object requests that hit the us-central1 region and us multi region buckets.\nWe will provide an update by Tuesday, 2021-07-27 15:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: GCS is intermittently serving older deleted versions of objects, or not serving 404 for deleted objects.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T19:37:37+00:00","modified":"2021-07-27T19:37:37+00:00","when":"2021-07-27T19:37:37+00:00","text":"Summary: us-central1: GCS is returning stale version of object for bucket\nDescription: Our engineering team is currently investigating the issue.\nThis issue affects get object requests that hit the us-central1 region and us multi region buckets.\nWe will provide an update by Tuesday, 2021-07-27 13:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: GCS is intermittently serving older deleted versions of objects, or not serving 404 for deleted objects.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T19:01:22+00:00","modified":"2021-07-27T19:01:22+00:00","when":"2021-07-27T19:01:22+00:00","text":"Summary: us-central1: GCS is returning stale version of object for bucket\nDescription: Our engineering team continues to investigate the issue.\nThis issue affects get object requests that hit the us-central1 region and us multi region buckets.\nWe will provide an update by Tuesday, 2021-07-27 13:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: GCS is intermittently serving older deleted versions of objects, or not serving 404 for deleted objects.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T18:00:24+00:00","modified":"2021-07-27T18:00:25+00:00","when":"2021-07-27T18:00:24+00:00","text":"Summary: GCS is returning stale version of object for bucket in us-central1\nDescription: We are experiencing an intermittent issue with Google Cloud Storage.\nThis issue affects us-central1 and multi-region buckets in US where resources are located in us-central1.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-07-27 12:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: GCS is intermittently serving older deleted versions of objects, or not serving 404 for deleted objects.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-27T23:15:28+00:00","modified":"2021-07-27T23:15:34+00:00","when":"2021-07-27T23:15:28+00:00","text":"The issue with Google Cloud Storage is believed to be affecting a very small number of projects and our Engineering Team is working on it.\nIf you have questions or are impacted, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"UwaYoXQ5bHYHG6EdiPB8","service_name":"Google Cloud Storage","affected_products":[{"title":"Google Cloud Storage","id":"UwaYoXQ5bHYHG6EdiPB8"}],"uri":"incidents/MfiGCW4E26MPGRnCJ8by","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"txaxJMs7eNMV5cHwgfRP","number":"1839278878480132730","begin":"2021-07-27T16:02:00+00:00","created":"2021-07-27T18:53:54+00:00","end":"2021-07-27T20:14:00+00:00","modified":"2021-07-29T00:30:56+00:00","external_desc":"Global: Cloud Logging Query timing out","updates":[{"created":"2021-07-28T16:47:26+00:00","modified":"2021-07-29T00:30:56+00:00","when":"2021-07-28T16:47:26+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 27 July 2021 09:02\n**Incident End:** 27 July 2021 13:14\n**Duration:** 4 hours, 12 minutes\n**Affected Services and Features:**\nCloud Logging\n**Regions/Zones:** Global\n**Description:**\nCloud Logging experienced query timeouts globally for a duration of 4 hours, 12 minutes. From preliminary analysis, the root cause of the issue is a configuration change in the backend component used for indexing and cataloging the logs.\n**Customer Impact:**\n* Cloud Logging - Query timeouts\n* Cloud Console - Elevated errors for Activity Stream due to Cloud Logging query timeouts.\n* Kubernetes, Cloud Run- Elevated logging errors due to Cloud Logging query timeouts.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T21:48:09+00:00","modified":"2021-07-27T21:48:15+00:00","when":"2021-07-27T21:48:09+00:00","text":"The issue with Cloud Logging has been resolved for all affected projects as of Tuesday, 2021-07-27 14:46 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T20:59:54+00:00","modified":"2021-07-27T20:59:54+00:00","when":"2021-07-27T20:59:54+00:00","text":"Summary: Global: Cloud Logging Query timing out\nDescription: We believe the issue with Cloud Logging is partially resolved and there is no further impact observed.\nThe ETA for full resolution is end of week July 30th.\nWe will provide an update by Friday, July 30th with current details.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T19:58:38+00:00","modified":"2021-07-27T19:58:38+00:00","when":"2021-07-27T19:58:38+00:00","text":"Summary: Global: Cloud Logging Query timing out\nDescription: We believe the issue with Cloud Logging is partially resolved and there is no further impact observed.\nThe ETA for full resolution is unknown at this time.\nWe will provide an update by Tuesday, 2021-07-27 14:00 US/Pacific with current details.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T19:22:33+00:00","modified":"2021-07-27T19:22:34+00:00","when":"2021-07-27T19:22:33+00:00","text":"Summary: Global: Cloud Logging Query timing out\nDescription: We believe the issue with Cloud Logging is partially resolved and there is no further impact observed.\nWe will provide an update by Tuesday, 2021-07-27 13:00 US/Pacific with current details.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T18:53:54+00:00","modified":"2021-07-27T18:53:55+00:00","when":"2021-07-27T18:53:54+00:00","text":"Summary: Global: Cloud Logging Query timing out\nDescription: We are investigating a potential issue with Cloud Logging.\nWe will provide more information by Tuesday, 2021-07-27 12:25 US/Pacific.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-28T16:47:26+00:00","modified":"2021-07-29T00:30:56+00:00","when":"2021-07-28T16:47:26+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 27 July 2021 09:02\n**Incident End:** 27 July 2021 13:14\n**Duration:** 4 hours, 12 minutes\n**Affected Services and Features:**\nCloud Logging\n**Regions/Zones:** Global\n**Description:**\nCloud Logging experienced query timeouts globally for a duration of 4 hours, 12 minutes. From preliminary analysis, the root cause of the issue is a configuration change in the backend component used for indexing and cataloging the logs.\n**Customer Impact:**\n* Cloud Logging - Query timeouts\n* Cloud Console - Elevated errors for Activity Stream due to Cloud Logging query timeouts.\n* Kubernetes, Cloud Run- Elevated logging errors due to Cloud Logging query timeouts.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Operations","id":"DixAowEQm45KgqXKP5tR"},{"title":"Cloud Logging","id":"PuCJ6W2ovoDhLcyvZ1xa"}],"uri":"incidents/txaxJMs7eNMV5cHwgfRP","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"eJ8bnwiB1kQXo8fet4Zg","number":"2129730558922264261","begin":"2021-07-27T15:29:00+00:00","created":"2021-07-27T15:29:36+00:00","end":"2021-07-27T16:00:00+00:00","modified":"2021-07-28T20:45:04+00:00","external_desc":"Global: Cloud Scheduler Pub/Sub jobs fail with permission denied","updates":[{"created":"2021-07-28T20:45:04+00:00","modified":"2021-07-28T20:45:04+00:00","when":"2021-07-28T20:45:04+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident here: https://status.cloud.google.com/incidents/fEXXEicMtx5SaVZz2Gt7.\nPlease note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T16:00:01+00:00","modified":"2021-07-27T16:05:13+00:00","when":"2021-07-27T16:00:01+00:00","text":"The issue with Cloud Scheduler has been resolved for some projects as of Tuesday, 2021-07-27 08:59 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T15:56:01+00:00","modified":"2021-07-27T15:56:07+00:00","when":"2021-07-27T15:56:01+00:00","text":"Summary: Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2021-07-27 09:37 US/Pacific.\nDiagnosis: Receiving Cloud Scheduler PERMISSION_DENIED\nWorkaround: Add publisher role to Cloud Scheduler service account. The service account has the form service-PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T15:29:34+00:00","modified":"2021-07-27T15:29:37+00:00","when":"2021-07-27T15:29:34+00:00","text":"Summary: Some Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: We are experiencing an issue with Cloud Scheduler\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-07-27 09:00 US/Pacific with current details.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-28T20:45:04+00:00","modified":"2021-07-28T20:45:04+00:00","when":"2021-07-28T20:45:04+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident here: https://status.cloud.google.com/incidents/fEXXEicMtx5SaVZz2Gt7.\nPlease note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"Y9fKAQ6BVTQUYomrNN9A","service_name":"Google Cloud Scheduler","affected_products":[{"title":"Google Cloud Scheduler","id":"Y9fKAQ6BVTQUYomrNN9A"}],"uri":"incidents/eJ8bnwiB1kQXo8fet4Zg","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"FB2c3B9EAS7JA1mHgth1","number":"15430518859596137620","begin":"2021-07-27T12:50:00+00:00","created":"2021-07-27T12:50:45+00:00","end":"2021-07-27T12:54:00+00:00","modified":"2022-05-26T20:07:36+00:00","external_desc":"HTTP load balancing returning error for some user in Europe.","updates":[{"created":"2021-07-27T12:54:50+00:00","modified":"2021-07-27T12:54:57+00:00","when":"2021-07-27T12:54:50+00:00","text":"The issue with Cloud Load Balancing has been resolved for all affected users as of Tuesday, 2021-07-27 05:33 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T12:50:37+00:00","modified":"2021-07-27T12:50:46+00:00","when":"2021-07-27T12:50:37+00:00","text":"Summary: HTTP load balancing returning error for some user in Europe.\nDescription: We are experiencing an issue with Cloud Load Balancing beginning at Tuesday, 2021-07-27 04:31 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-07-27 06:20 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: None at this time.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-27T12:54:50+00:00","modified":"2021-07-27T12:54:57+00:00","when":"2021-07-27T12:54:50+00:00","text":"The issue with Cloud Load Balancing has been resolved for all affected users as of Tuesday, 2021-07-27 05:33 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/FB2c3B9EAS7JA1mHgth1","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"UPG5wxRnLGjqqVFMW7Kq","number":"17548538680718969456","begin":"2021-07-27T11:15:00+00:00","created":"2021-07-27T14:51:10+00:00","end":"2021-07-28T04:05:00+00:00","modified":"2021-07-29T00:32:15+00:00","external_desc":"Issue resolved: HTTP load balancing returning error for some users in Europe.","updates":[{"created":"2021-07-28T15:12:57+00:00","modified":"2021-07-29T00:32:15+00:00","when":"2021-07-28T15:12:57+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Timeline:**\n27 June 2021 04:15 - 05:33\n27 June 2021 06:56 - 07:41\n**Total Duration:**\n2 hours 3 minutes\n**Affected Services and Features:**\nGoogle Cloud Load Balancing\n**Regions/Zones:**\nWestern Europe\n**Description:**\nGoogle Cloud Load Balancing (GCLB) experienced increased TLS handshake errors or connection terminations for HTTPS traffic served from load balancers in Western Europe for a duration of 2 hours, 3 minutes. From preliminary analysis, the root cause of the issue is a configuration change that resulted in an overload of an internal component responsible for handling part of the traffic requests.\n**Customer Impact:**\nIncreased TLS handshake errors or connection terminations for GCLB traffic served from HTTPS load balancers in Western Europe.\n**Additional details:**\nThe issue was fully resolved on 27 June 2021 21:05 US/Pacific, and we are confident that there will not be a recurrence.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-28T04:05:23+00:00","modified":"2021-07-28T04:05:24+00:00","when":"2021-07-28T04:05:23+00:00","text":"The issue with HTTP load balancing has been resolved for all affected users in Europe as of Tuesday, 2021-07-27 19:02 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T16:27:14+00:00","modified":"2021-07-27T16:27:14+00:00","when":"2021-07-27T16:27:14+00:00","text":"Summary: HTTP load balancing returning error for some user in Europe.\nDescription: We believe the issue with Cloud Networking is resolved and there is no further impact.\nEngineering team has mitigated the current impact and expect the full resolution to be rolled out within next 12 hours.\nWe will provide an update by Tuesday, 2021-07-27 21:30 US/Pacific with current details.\nDiagnosis: None at this time\nWorkaround: None at this time","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T15:24:44+00:00","modified":"2021-07-27T15:24:45+00:00","when":"2021-07-27T15:24:44+00:00","text":"Summary: HTTP load balancing returning error for some user in Europe.\nDescription: We believe the issue with Cloud Networking is resolved.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-07-27 15:30 US/Pacific with current details.\nDiagnosis: none at this time\nWorkaround: none at this time","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T15:22:20+00:00","modified":"2021-07-27T15:22:21+00:00","when":"2021-07-27T15:22:20+00:00","text":"Summary: HTTP load balancing returning error for some user in Europe.\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Tuesday, 2021-07-27 09:57 US/Pacific.\nDiagnosis: none at this time\nWorkaround: none at this time","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T14:51:08+00:00","modified":"2021-07-27T14:51:11+00:00","when":"2021-07-27T14:51:08+00:00","text":"Summary: HTTP load balancing returning error for some user in Europe.\nDescription: We are experiencing an intermittent issue with Cloud Networking beginning at Tuesday, 2021-07-27 06:56:46 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Tuesday, 2021-07-27 08:20 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: none at this time\nWorkaround: none at this time","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-28T15:12:57+00:00","modified":"2021-07-29T00:32:15+00:00","when":"2021-07-28T15:12:57+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Timeline:**\n27 June 2021 04:15 - 05:33\n27 June 2021 06:56 - 07:41\n**Total Duration:**\n2 hours 3 minutes\n**Affected Services and Features:**\nGoogle Cloud Load Balancing\n**Regions/Zones:**\nWestern Europe\n**Description:**\nGoogle Cloud Load Balancing (GCLB) experienced increased TLS handshake errors or connection terminations for HTTPS traffic served from load balancers in Western Europe for a duration of 2 hours, 3 minutes. From preliminary analysis, the root cause of the issue is a configuration change that resulted in an overload of an internal component responsible for handling part of the traffic requests.\n**Customer Impact:**\nIncreased TLS handshake errors or connection terminations for GCLB traffic served from HTTPS load balancers in Western Europe.\n**Additional details:**\nThe issue was fully resolved on 27 June 2021 21:05 US/Pacific, and we are confident that there will not be a recurrence.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Networking","id":"VNJxzcH58QmTt5H6pnT6"},{"title":"Cloud Load Balancing","id":"ix7u9beT8ivBdjApTif3"}],"uri":"incidents/UPG5wxRnLGjqqVFMW7Kq","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"vFhgfrfzzrx6zQo69SdQ","number":"10314800147357781421","begin":"2021-07-27T07:00:00+00:00","created":"2021-09-24T03:26:12+00:00","end":"2021-09-24T01:53:00+00:00","modified":"2021-09-27T17:31:28+00:00","external_desc":"Issues with GKE 1.20 (lower than 1.20.9-gke.2100) node pools using Docker as runtime.","updates":[{"created":"2021-09-27T17:29:50+00:00","modified":"2021-09-27T17:29:50+00:00","when":"2021-09-27T17:29:50+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 27 July 2021, GKE clusters began being upgraded from 1.19 to 1.20 in the REGULAR release channel.\n**Incident End:** 23 September 2021, incident is mitigated by pausing automatic upgrades.\n**Duration:** 59 days\n**Affected Services and Features:**\nGoogle Kubernetes Engine (GKE) - Pods on nodes with affected versions will restart when docker restarts. Clusters on the REGULAR release channel were automatically upgraded into versions affected by this issue.\n**Regions/Zones:** Global\n**Description:**\nContainers within GKE cluster node pools using docker are getting restarted in the event of docker restarts. This issue affects the following node versions: - All 1.20 versions below 1.20.9-gke.2100 - All 1.21 versions below 1.21.3-gke.1600\nThe engineering team has halted the rollout from 1.19 to 1.20 in release channels to prevent any new impact to our customers.\n**Customer Impact:**\nGKE cluster pods restart when docker restarts. This issue affects the following node versions: - All 1.20 versions below 1.20.9-gke.2100 - All 1.21 versions below 1.21.3-gke.1600\n**Additional details:**\nCustomer action required: To fix this issue, either use containerd or upgrade nodes to version:\n- 1.20: 1.20.9-gke.2100 or higher\n- 1.21: 1.21.3-gke.1600 or higher\nThe recommended action for clusters on release channels are: - STATIC - Upgrade to 1.20.10-gke.301 or higher - RAPID - N/A - All available versions have the fix - REGULAR - Upgrade to 1.20.10-gke.301 - STABLE - Downgrade affected nodepools to a 1.19 version\nOur engineering team is currently releasing a fixed version for 1.20 for the STABLE release channel. This release is currently scheduled to come out by 8 October, 2021.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-24T19:53:46+00:00","modified":"2021-09-24T19:53:51+00:00","when":"2021-09-24T19:53:46+00:00","text":"GKE clusters running node pools that use docker may experience containers restarting every time docker restarts. Affected node versions:\n* All 1.20 versions below 1.20.9-gke.2100\n* All 1.21 versions below 1.21.3-gke.1600\nTo fix this issue, either use containerd or upgrade nodes to version:\n* 1.20: 1.20.9-gke.2100 or higher\n* 1.21: 1.21.3-gke.1600 or higher\nRecommended fix per release channel:\nSTATIC - Upgrade to 1.20.10-gke.301 or higher\nRAPID - N/A - All available versions have the fix\nREGULAR - Upgrade to 1.20.10-gke.301\nSTABLE - Downgrade affected nodepools to a 1.19 version\nOur engineering team is continuing to work on a release with the fix to the issue in a 1.20 version of the stable channel, which is scheduled to be released before the first week of October. If you have questions, please open a case with the Support Team and we will work with you until this issue is resolved.\nNo further updates will be provided here.\nWe thank you for your patience while we're working on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-09-24T06:48:51+00:00","modified":"2021-09-24T06:48:52+00:00","when":"2021-09-24T06:48:51+00:00","text":"Summary: Issues with GKE node pools upgraded from 1.19 to 1.20 (lower than 1.20.9-gke.2100) using Docker as runtime.\nDescription: We are experiencing an issue with GKE node pools using Docker as a runtime upgraded from 1.19 to 1.20 (lower than 1.20.9-gke.2100). Customers may see containers restarting when docker daemon is restarted.\nProblematic versions:\n* All 1.20.0.* - 1.20.8.*\n* 1.20.9.* below 1.20.9-gke.1600\n**Action required:**\nDo not upgrade from 1.19 -\u003e 1.20.x (lower than 1.20.9-gke.2100)\nCustomers can upgrade to 1.20.9-gke.2100+ or 1.20.10-gke.301 to mitigate the issue.\nOur engineering team continues to work on the fix.\nWe will provide an update by Friday, 2021-09-24 15:00 US/Pacific.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may see GKE Cluster pods restarting when Docker restarts.\nWorkaround: Customers can upgrade to 1.20.9-gke.2100+ or 1.20.10-gke.301 to mitigate the issue.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-24T04:59:58+00:00","modified":"2021-09-24T04:59:58+00:00","when":"2021-09-24T04:59:58+00:00","text":"Summary: Specific GKE Clusters upgraded from 1.19 to 1.20 (lower than 1.20.9-gke.2100) may experience pods restarting when Docker restarts.\nDescription: We are experiencing an issue with GKE clusters upgraded from 1.19 to 1.20(lower than 1.20.9-gke.2100). Clusters may see pods restarting when Dockers are restarted.\nProblematic versions:\n* All 1.20.0.* - 1.20.8.*\n* 1.20.9.* below 1.20.9-gke.1600\n**Action required:**\nDo not upgrade from 1.19 -\u003e 1.20.x (lower than 1.20.9-gke.2100)\nCustomers can upgrade to 1.20.9-gke.2100+ or 1.20.10-gke.301 to mitigate the issue.\nOur engineering team continues to investigate the issue at the back end.\nWe will provide an update by Friday, 2021-09-24 00:00 US/Pacific.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may see GKE Cluster pods restarting when Docker restarts.\nWorkaround: Customers can upgrade to 1.20.9-gke.2100+ or 1.20.10-gke.301 to mitigate the issue.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-24T03:28:30+00:00","modified":"2021-09-24T03:28:30+00:00","when":"2021-09-24T03:28:30+00:00","text":"Summary: Specific GKE Clusters upgraded from 1.19 to 1.20 (lower than 1.20.9-gke.2100) may experience pods restarting when Docker restarts.\nDescription: We are experiencing an issue with GKE clusters upgraded from 1.19 to 1.20(lower than 1.20.9-gke.2100). Clusters may see pods restarting when Dockers are restarted.\nProblematic versions:\n* All 1.20.0.* - 1.20.8.*\n* 1.20.9.* below 1.20.9-gke.1600\n**Action required:**\nDo not upgrade from 1.19 -\u003e 1.20.x (lower than 1.20.9-gke.2100)\nTo mitigate a cluster already upgraded to a problematic version, upgrade master and nodes to 1.20.10-gke.301.\nAvailable versions with fix:\n* 1.20.9-gke.210\n* 1.20.10+\nOur engineering team continues to investigate the issue at the back end.\nWe will provide an update by Thursday, 2021-09-23 22:00 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may see GKE Cluster pods restarting when Docker restarts.\nWorkaround: Do not upgrade from 1.19 -\u003e 1.20.x (lower than 1.20.9-gke.2100)\nTo mitigate a cluster already upgraded, to a problematic version, upgrade master and nodes to 1.20.10-gke.301. Versions above 1.20.9-gke.2100 is not impacted.","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-09-24T03:26:12+00:00","modified":"2021-09-24T03:26:13+00:00","when":"2021-09-24T03:26:12+00:00","text":"Summary: Specific GKE Clusters upgraded from 1.19 to 1.20 (lower than 1.20.9-gke.2100) may experience pods restarting when Docker restarts.\nDescription: We are experiencing an issue with GKE clusters upgraded from 1.19 to 1.20(lower than 1.20.9-gke.2100). Clusters may see pods restarting when Dockers are restarted.\nProblematic versions:\n* All 1.20.0.* - 1.20.8.*\n* 1.20.9.* below 1.20.9-gke.1600\n**Action required:**\nDo not upgrade from 1.19 -\u003e 1.20.x (lower than 1.20.9-gke.2100)\nTo mitigate a cluster already upgraded to a problematic version, upgrade master and nodes to 1.20.10-gke.301.\nAvailable versions with fix:\n* 1.20.9-gke.210\n* 1.20.10+\nOur engineering team continues to investigate the issue at the back end.\nWe will provide an update by Thursday, 2021-09-23 20:30 US/Pacific with current details.\nWe apologize to all who are affected by the disruption.\nDiagnosis: Customers may see GKE Cluster pods restarting when Docker restarts.\nWorkaround: Do not upgrade from 1.19 -\u003e 1.20.x (lower than 1.20.9-gke.2100)\nTo mitigate a cluster already upgraded, to a problematic version, upgrade master and nodes to 1.20.10-gke.301. Versions above 1.20.9-gke.2100 is not impacted.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-09-27T17:29:50+00:00","modified":"2021-09-27T17:29:50+00:00","when":"2021-09-27T17:29:50+00:00","text":"We apologize for the inconvenience this service disruption may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 27 July 2021, GKE clusters began being upgraded from 1.19 to 1.20 in the REGULAR release channel.\n**Incident End:** 23 September 2021, incident is mitigated by pausing automatic upgrades.\n**Duration:** 59 days\n**Affected Services and Features:**\nGoogle Kubernetes Engine (GKE) - Pods on nodes with affected versions will restart when docker restarts. Clusters on the REGULAR release channel were automatically upgraded into versions affected by this issue.\n**Regions/Zones:** Global\n**Description:**\nContainers within GKE cluster node pools using docker are getting restarted in the event of docker restarts. This issue affects the following node versions: - All 1.20 versions below 1.20.9-gke.2100 - All 1.21 versions below 1.21.3-gke.1600\nThe engineering team has halted the rollout from 1.19 to 1.20 in release channels to prevent any new impact to our customers.\n**Customer Impact:**\nGKE cluster pods restart when docker restarts. This issue affects the following node versions: - All 1.20 versions below 1.20.9-gke.2100 - All 1.21 versions below 1.21.3-gke.1600\n**Additional details:**\nCustomer action required: To fix this issue, either use containerd or upgrade nodes to version:\n- 1.20: 1.20.9-gke.2100 or higher\n- 1.21: 1.21.3-gke.1600 or higher\nThe recommended action for clusters on release channels are: - STATIC - Upgrade to 1.20.10-gke.301 or higher - RAPID - N/A - All available versions have the fix - REGULAR - Upgrade to 1.20.10-gke.301 - STABLE - Downgrade affected nodepools to a 1.19 version\nOur engineering team is currently releasing a fixed version for 1.20 for the STABLE release channel. This release is currently scheduled to come out by 8 October, 2021.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"LCSbT57h59oR4W98NHuz","service_name":"Google Kubernetes Engine","affected_products":[{"title":"Google Kubernetes Engine","id":"LCSbT57h59oR4W98NHuz"}],"uri":"incidents/vFhgfrfzzrx6zQo69SdQ","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"fEXXEicMtx5SaVZz2Gt7","number":"8304108178110052490","begin":"2021-07-22T02:22:00+00:00","created":"2021-07-27T16:10:36+00:00","end":"2021-07-27T22:28:00+00:00","modified":"2021-07-29T00:29:58+00:00","external_desc":"Global: Cloud Scheduler Pub/Sub jobs fail with permission denied","updates":[{"created":"2021-07-28T20:32:51+00:00","modified":"2021-07-29T00:29:58+00:00","when":"2021-07-28T20:32:51+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 21 July 2021 19:22\n**Incident End:** 27 July 2021 15:28\n**Duration:** 5 days, 20 hours, 6 minutes\n**Affected Services and Features:**\nGoogle Cloud Scheduler Pub/Sub\n**Regions/Zones:** All Regions\n**Description:**\nGoogle Cloud Scheduler jobs experienced increased errors globally when publishing messages to Pub/Sub topics for a duration of 5 days, 20 hours, 6 minutes. From preliminary analysis, the root cause of the issue is due to a configuration change that updated the service agent used when publishing to Pub/Sub. Projects using the new service agent without the correct permissions resulted in PERMISSION_DENIED errors for tasks that required publishing to Pub/Sub.\n**Customer Impact:**\nAll customers with Cloud Scheduler jobs with a Pub/Sub topic as a target that did not grant the Cloud Scheduler Google-managed service account access to that Pub/Sub topic saw PERMISSION_DENIED errors.\n**Additional details:**\nThe issue was fully resolved on 27 July 2021 at 15:28 US/Pacific after a rollback of the change was completed.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T22:46:11+00:00","modified":"2021-07-27T22:46:11+00:00","when":"2021-07-27T22:46:11+00:00","text":"The issue with Cloud Scheduler has been resolved for all affected projects as of Tuesday, 2021-07-27 15:43 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-27T22:23:54+00:00","modified":"2021-07-27T22:23:55+00:00","when":"2021-07-27T22:23:54+00:00","text":"Summary: Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: We believe the issue with Cloud Scheduler is partially resolved.\nWe do not have an ETA for full resolution at this point.\nWe will provide an update by Tuesday, 2021-07-27 16:01 US/Pacific with current details.\nDiagnosis: Receiving Cloud Scheduler PERMISSION_DENIED\nWorkaround: Add the permission pubsub.topics.publish to Cloud Scheduler service account (service-PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com).","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T20:20:24+00:00","modified":"2021-07-27T20:20:24+00:00","when":"2021-07-27T20:20:24+00:00","text":"Summary: Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: We believe the issue with Cloud Scheduler is partially resolved and there is no further impact observed.\nAction:\n- Customers should utilize the Cloud Services Robot account for authentication.\nWe will provide an update by Tuesday, 2021-07-27 15:30 US/Pacific with current details.\nDiagnosis: Receiving Cloud Scheduler PERMISSION_DENIED\nWorkaround: Add the permission pubsub.topics.publish to Cloud Scheduler service account (service-PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com).","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T19:28:06+00:00","modified":"2021-07-27T19:28:06+00:00","when":"2021-07-27T19:28:06+00:00","text":"Summary: Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: We believe the issue with Cloud Scheduler is partially resolved and there is no further impact observed.\nAction:\n- Customers should utilize the Cloud Services Robot account for authentication.\nWe will provide an update by Tuesday, 2021-07-27 13:30 US/Pacific with current details.\nDiagnosis: Receiving Cloud Scheduler PERMISSION_DENIED\nWorkaround: Add the permission pubsub.topics.publish to Cloud Scheduler service account (service-PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com).","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T18:03:31+00:00","modified":"2021-07-27T18:03:31+00:00","when":"2021-07-27T18:03:31+00:00","text":"Summary: Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: Mitigation work is still underway by our engineering team.\nThe rollback activity is currently 50% complete and ongoing.\nWe will provide more information by Tuesday, 2021-07-27 13:00 US/Pacific.\nDiagnosis: Receiving Cloud Scheduler PERMISSION_DENIED\nWorkaround: Add publisher role to Cloud Scheduler service account. The service account has the form service-PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com","status":"SERVICE_DISRUPTION","affected_locations":[]},{"created":"2021-07-27T16:10:35+00:00","modified":"2021-07-27T16:10:37+00:00","when":"2021-07-27T16:10:35+00:00","text":"Summary: Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\nDescription: This is a continuation of the previous post for incident \"Global: Cloud Scheduler Pub/Sub jobs fail with permission denied\" that was closed as resolved.\nWe have received updates from our engineering team that the Mitigation work is still underway for some regions and are currently waiting for an ETA.\nWe will provide more information by Tuesday, 2021-07-27 11:00 US/Pacific.\nDiagnosis: Receiving Cloud Scheduler PERMISSION_DENIED\nWorkaround: Add publisher role to Cloud Scheduler service account. The service account has the form service-PROJECT_NUMBER@gcp-sa-cloudscheduler.iam.gserviceaccount.com","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-28T20:32:51+00:00","modified":"2021-07-29T00:29:58+00:00","when":"2021-07-28T20:32:51+00:00","text":"We apologize for the inconvenience this service disruption/outage may have caused. We would like to provide some information about this incident below. Please note, this information is based on our best knowledge at the time of posting and is subject to change as our investigation continues. If you have experienced impact outside of what is listed below, please reach out to Google Support by opening a case using https://cloud.google.com/support\n(All Times US/Pacific)\n**Incident Start:** 21 July 2021 19:22\n**Incident End:** 27 July 2021 15:28\n**Duration:** 5 days, 20 hours, 6 minutes\n**Affected Services and Features:**\nGoogle Cloud Scheduler Pub/Sub\n**Regions/Zones:** All Regions\n**Description:**\nGoogle Cloud Scheduler jobs experienced increased errors globally when publishing messages to Pub/Sub topics for a duration of 5 days, 20 hours, 6 minutes. From preliminary analysis, the root cause of the issue is due to a configuration change that updated the service agent used when publishing to Pub/Sub. Projects using the new service agent without the correct permissions resulted in PERMISSION_DENIED errors for tasks that required publishing to Pub/Sub.\n**Customer Impact:**\nAll customers with Cloud Scheduler jobs with a Pub/Sub topic as a target that did not grant the Cloud Scheduler Google-managed service account access to that Pub/Sub topic saw PERMISSION_DENIED errors.\n**Additional details:**\nThe issue was fully resolved on 27 July 2021 at 15:28 US/Pacific after a rollback of the change was completed.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_DISRUPTION","severity":"medium","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Scheduler","id":"Y9fKAQ6BVTQUYomrNN9A"},{"title":"Google Cloud Pub/Sub","id":"dFjdLh2v6zuES6t9ADCB"}],"uri":"incidents/fEXXEicMtx5SaVZz2Gt7","currently_affected_locations":[],"previously_affected_locations":[]},{"id":"WXwFb7tx1XvHyGaxwdYs","number":"14920841997106925105","begin":"2021-07-07T21:25:00+00:00","created":"2021-07-07T22:45:04+00:00","end":"2021-07-07T23:40:00+00:00","modified":"2021-07-16T22:59:00+00:00","external_desc":"Increased latency in North America Regions for Cloud Datastore queries. Previous posts mention Google Cloud Firestore, upon further analysis we believe this is incorrect. We have moved the incident to Cloud Datastore to correctly reflect impact.","updates":[{"created":"2021-07-08T20:29:10+00:00","modified":"2021-07-16T22:59:00+00:00","when":"2021-07-08T20:29:10+00:00","text":"Mini Incident Report\n(All Times US/Pacific)\n**Incident Start:** 07 July 2021 14:25\n**Incident End:** 07 July 2021 16:40\n**Duration:** 2 hours, 15 minutes\n**Affected Services and Features:**\nGoogle Cloud Datastore\n**Regions/Zones:** NAM5 Multi-Region\n**Description:**\nGoogle Cloud Datastore queries experienced increased latency for a period of 2 hours and 15 minutes due to an unexpected spike in traffic.\n**Customer Impact:**\nAffected customers would have experienced increased latency when querying or writing to Cloud Datastore.\n**Additional details:**\nGoogle has addressed future unexpected traffic spikes of this nature by increasing Datastore capacity.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-07T23:58:17+00:00","modified":"2021-07-07T23:58:17+00:00","when":"2021-07-07T23:58:17+00:00","text":"The issue with Cloud Firestore has been resolved for all affected projects as of Wednesday, 2021-07-07 16:40 US/Pacific.\nWe thank you for your patience while we worked on resolving the issue.","status":"AVAILABLE","affected_locations":[]},{"created":"2021-07-07T23:28:51+00:00","modified":"2021-07-07T23:28:51+00:00","when":"2021-07-07T23:28:51+00:00","text":"Summary: Increased latency in North America Regions for Cloud Firestore queries\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2021-07-07 17:30 US/Pacific.\nDiagnosis: Impacted customers may see increased latency when querying or writing to Cloud Firestore.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-07-07T22:52:54+00:00","modified":"2021-07-07T22:52:54+00:00","when":"2021-07-07T22:52:54+00:00","text":"Summary: Increased latency in North America Regions for Cloud Firestore queries\nDescription: Mitigation work is currently underway by our engineering team.\nWe do not have an ETA for mitigation at this point.\nWe will provide more information by Wednesday, 2021-07-07 16:30 US/Pacific.\nDiagnosis: Impacted customers may see increased latency when querying or writing to Cloud Firestore.\nWorkaround: None at this time.","status":"SERVICE_OUTAGE","affected_locations":[]},{"created":"2021-07-07T22:45:03+00:00","modified":"2021-07-07T22:45:04+00:00","when":"2021-07-07T22:45:03+00:00","text":"Summary: Increased latency in North America Regions for Cloud Firestore queries\nDescription: We are experiencing an issue with Cloud Firestore beginning at Wednesday, 2021-07-07 14:25 US/Pacific.\nOur engineering team continues to investigate the issue.\nWe will provide an update by Wednesday, 2021-07-07 16:30 US/Pacific with current details. We apologize to all who are affected by the disruption.\nDiagnosis: Impacted customers may see increased latency when querying or writing to Cloud Firestore.\nWorkaround: None at this time.","status":"SERVICE_DISRUPTION","affected_locations":[]}],"most_recent_update":{"created":"2021-07-08T20:29:10+00:00","modified":"2021-07-16T22:59:00+00:00","when":"2021-07-08T20:29:10+00:00","text":"Mini Incident Report\n(All Times US/Pacific)\n**Incident Start:** 07 July 2021 14:25\n**Incident End:** 07 July 2021 16:40\n**Duration:** 2 hours, 15 minutes\n**Affected Services and Features:**\nGoogle Cloud Datastore\n**Regions/Zones:** NAM5 Multi-Region\n**Description:**\nGoogle Cloud Datastore queries experienced increased latency for a period of 2 hours and 15 minutes due to an unexpected spike in traffic.\n**Customer Impact:**\nAffected customers would have experienced increased latency when querying or writing to Cloud Datastore.\n**Additional details:**\nGoogle has addressed future unexpected traffic spikes of this nature by increasing Datastore capacity.","status":"AVAILABLE","affected_locations":[]},"status_impact":"SERVICE_OUTAGE","severity":"high","service_key":"zall","service_name":"Multiple Products","affected_products":[{"title":"Google Cloud Datastore","id":"MaS3dKoqp1oqkea4qB9U"},{"title":"Cloud Firestore","id":"CETSkT92V21G6A1x28me"}],"uri":"incidents/WXwFb7tx1XvHyGaxwdYs","currently_affected_locations":[],"previously_affected_locations":[]}]